[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Improving your statistical inferences through Monte Carlo simulations",
    "section": "",
    "text": "Introduction\nThis Open Source eBook provides materials for the semester-long Master’s seminar course “Improving your statistical inferences through Monte Carlo simulations” that I deliver at the University of Bern’s Institute of Psychology.\nThis book is split into two parts:",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Improving your statistical inferences through Monte Carlo simulations",
    "section": "",
    "text": "Learning to simulate. Chapters 2-10 teach you the conceptual structure of Monte Carlo simulation studies, and how to implement them in a modular R/tidyverse workflow that prioritizes understandability and code reusability over execution speed.\nLearning from simulations. The remaining chapters use simulations to better understand and use statistical methods, and to understand and guide the behavior of researchers using those methods.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Improving your statistical inferences through Monte Carlo simulations",
    "section": "How to use this book",
    "text": "How to use this book\nThis book is made up of individual Quarto (.qmd) workbooks. Many of the exercises are easiest to complete in your own local copy of these .qmd files.\nWe suggest that you download a .zip of the contents of this book’s code and data from GitHub to run the code locally, complete the exercises, etc.\nYou can also copy and paste the code for any chapter directly from the website. Click the “&lt;/&gt; Code” button on the top right of each page to see the full .qmd file’s code. You can copy and paste this into a .qmd file. However, it’s probably easier to download all the .qmd files and data as mentioned above.\nLearning to code is a practice skill. Almost anyone can become competent in writing reproducible code for data simulation and simulation studies with practice. More than anything else, completing this course requires that you practice in your own time, using not only the examples provided but also ones you create yourself.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#other-learning-resources",
    "href": "index.html#other-learning-resources",
    "title": "Improving your statistical inferences through Monte Carlo simulations",
    "section": "Other learning resources",
    "text": "Other learning resources\nThere are many excellent Open Source resources to learn simulation studies in R and {tidyverse}. Readers are encouraged to seek them out to support the materials already provided in this book. I can particularly recommend the following ones:\n\nMiratrix and Pustejovsky (2026) Designing Monte Carlo Simulations in R. jepusto.github.io/Designing-Simulations-in-R\nStrobl et al. (2024) Simulationsstudien in R: Design und praktische Durchführung. doi: doi.org/10.1007/978-3-662-70561-2 [English translation should be available in late 2026]\n\nThere is also a growing literature on reporting standards, preregistration, and meta-research on simulations such as the risk of Questionable Research Practices in simulation studies. In particularly I recommend:\n\nSiepe et al. (2026) Why, when, and how to (or not to)preregister a simulation study. osf.io/cxjrb_v1\nSiepe et al. (2024) Simulation studies for methodological research in psychology: A standardized template for planning, preregistration, and reporting. Psychological Methods. doi: 10.1037/met0000695\nPawel et al. (2023) Pitfalls and potentials in simulation studies: Questionable research practices in comparative simulation studies allow for spurious claims of superiority of any method. Biometrical Journal. doi: 10.1002/bimj.202200091",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Improving your statistical inferences through Monte Carlo simulations",
    "section": "Contributing",
    "text": "Contributing\nIf you are interested in contributing to or adapting this eBook, all code and data are available on GitHub.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/1_required_skills.html",
    "href": "chapters/1_required_skills.html",
    "title": "1  Required skills",
    "section": "",
    "text": "1.1 Rounding results\nThis book assumes some existing knowledge of data wrangling and visualization in R/tidyverse. Specifically, familiarity with RStudio, reproducible reports written in {quarto} (.qmd) documents, the ‘pipe’ (|&gt; and %&gt;%), the packages {dplyr}, {tidyr}, {forcats}, and {ggplot2}. This chapter provides a refresher and set of self-tests to check your knowledge.\nIf need help with these skills, please see Ian’s other book “Reproducible Data Processing and Visualization in R and tidyverse”.\nIf you’re enrolled in our class but haven’t already taken Ian’s “Reproducible data processing and visualization” class based on that book, or a comparable class, we encourage you to rapidly make your way through it. In previous years, students have taken this simulation course without much familiarity with R when they already have some familiarity with other coding languages such as Python and Matlab. Sometimes, students sign up for this seminar with low confidence in their R abilities. It is entirely possible to succeed in this course without strong existing R skills, but it unavoidably means more self-guided learning and practice for you.\nIt is extremely common to round statistical results before including them in text and tables.\nHowever, did you know that R doesn’t use the rounding method most of us are taught in school where .5 is rounded up to the next integer? Instead it uses “banker’s rounding”, which is better when you round a very large number of numbers, but worse for reporting the results of specific analyses.\nThis is easier to show than explain. The round() function rounds each of the numbers passed to it. What do you expect the output to be?\nCode\nround(c(0.5, \n        1.5, \n        2.5, \n        3.5, \n        4.5, \n        5.5), digits = 0)\nIn most of your R scripts, you should instead use the {roundwork} package’s round_up(), written by Lukas Jung, which produces the round-.5-upwards behavior most of us expect.\nCode\nlibrary(roundwork) \n\nroundwork::round_up(c(0.5, \n                      1.5, \n                      2.5, \n                      3.5, \n                      4.5, \n                      5.5))\n\n\n[1] 1 2 3 4 5 6",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Required skills</span>"
    ]
  },
  {
    "objectID": "chapters/1_required_skills.html#rounding-results",
    "href": "chapters/1_required_skills.html#rounding-results",
    "title": "1  Required skills",
    "section": "",
    "text": "Click to show result\n\n\n\n\n\n\n\nCode\nround(c(0.5, \n        1.5, \n        2.5, \n        3.5, \n        4.5, \n        5.5))\n\n\n[1] 0 2 2 4 4 6\n\n\nWhy is this? Because R’s round() function uses “banker’s rounding, which rounds 5s based on whether the preceding digit is odd or even. This is a good thing in many contexts like accounting, but it’s usually not what we want or expect when rounding specific statistical results for inclusion in a report or manuscript.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Required skills</span>"
    ]
  },
  {
    "objectID": "chapters/1_required_skills.html#data-wrangling-with-dplyr-functions",
    "href": "chapters/1_required_skills.html#data-wrangling-with-dplyr-functions",
    "title": "1  Required skills",
    "section": "1.2 Data wrangling with {dplyr} functions",
    "text": "1.2 Data wrangling with {dplyr} functions\nComplete the following exercises in your local copy of this .qmd file to check your data wrangling/{dplyr} skills.\n\n1.2.1 Calculate mean\n\nUse dplyr::summarize().\nUse the data_intervention data set.\nReturn results in a tibble.\n\n\n\n1.2.2 Calculate SD\n\nUse dplyr::summarize().\nUse the data_intervention data set.\nReturn results in a tibble.\n\n\n\n1.2.3 Calculate mean for each condition\n\nUse dplyr::summarize(), group_by() and the pipe (%&gt;% or |&gt;).\nUse the data_for_ttest data set.\nReturn results in a tibble.\n\n\n\n1.2.4 Calculate mean and SD for each condition\n\nUse dplyr::summarize(), group_by() and the pipe (%&gt;% or |&gt;).\nUse the data_for_ttest data set.\nReturn results in a tibble.\n\n\n\n1.2.5 Calculate mean and SD for each condition rounded to two decimal places\n\nUse dplyr::summarize(), group_by() and the pipe (%&gt;% or |&gt;).\nUse the data_for_ttest data set.\nReturn results in a tibble.\nRound the means and SDs to two decimal places, using the round-half-up method, eg via roundwork::round_up(). Ideally, use mutate_if() or across() to round multiple columns.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Required skills</span>"
    ]
  },
  {
    "objectID": "chapters/1_required_skills.html#tidy-data-and-extracting-estimates",
    "href": "chapters/1_required_skills.html#tidy-data-and-extracting-estimates",
    "title": "1  Required skills",
    "section": "1.3 Tidy data and extracting estimates",
    "text": "1.3 Tidy data and extracting estimates\nLater content in this book relies on you having an understanding of ‘Tidy Data’; the workflow we define and use is built around this concept. Specifically, because most data analysis functions don’t return data in a ‘Tidy’ format, we need to be able to extract their results in Tidy format. Importantly, when learners struggle or make errors when trying to build simulations, it is very often because their workflow is not Tidy.\nCheck your understanding:\nWhat does Tidy Data refer to?\n\n\n\n\n\n\nClick to show answer\n\n\n\n\n\nTidy Data is a set of technical ideas about how data should be structured defined by Hadley Wickham, the main developer of {tidyverse} (Wickham, 2014).\n\n\n\nWhat are the three rules that make a dataset Tidy according to Wickham (2023)?\n\n\n\n\n\n\nClick to show answer\n\n\n\n\n\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value.\n\n\n\n\nIf you need a refresher, see the chapter on tidy data and reshaping in Ian’s other book.\nComplete the following exercises in your local copy of this .qmd file to check your data tidying skills. Most of them involve extracting estimates from objects created by data simulation or statistical modelling functions.\n\n1.3.1 Generate normally distributed data in a tibble\nThe rnorm() samples data from a normally distributed population with a given population mean (\\(\\mu\\)) and population standard deviation (\\(\\sigma\\)).\n\n\nCode\n# set seed for reproduciblity\nset.seed(42)\n\nrnorm(n = 10, \n      mean = 0, \n      sd = 1)\n\n\n [1]  1.37095845 -0.56469817  0.36312841  0.63286260  0.40426832 -0.10612452\n [7]  1.51152200 -0.09465904  2.01842371 -0.06271410\n\n\nMake this tidier by returning this simulated values as the column score in a tibble. Assign the tibble to the object data_control.\nCreate a second object, data_intervention, where the observations are sampled from a population mean (\\(\\mu\\)) of 0.4.\nCreate a new column called condition in each tibble using the appropriate {dplyr} function, setting it to “control” and “intervention” in the respective tibbles.\nCreate a new object, data_rct, from data_control and data_intervention by binding the two tibbles together using the appropriate {dplyr} bind_ function.\n\n\n1.3.2 t-test’s p-value\n\nA Student’s t-test and extract its p value.\nUse the data_for_ttest data set.\nReturn the p value as a column in a tibble.\n\n\n\n1.3.3 Cohen’s d and its 95% Confidence Intervals\n\nCalculate Cohen’s d using effectsize::cohens_d() and extract the Cohen’s d estimate and its 95% CIs.\nUse the data_for_ttest data set.\nReturn the Cohen’s d estimate and its 95% CIs in tidy format tibble as the columns d_estimate, d_ci_lower, d_ci_upper\n\n\n\n1.3.4 Pearson’s r from correlation test\n\nFit a correlation test using cor.test() and extract the correlation estimate.\nUse the data_for_correlation data set.\nReturn results in a tibble.\n\n\n\n1.3.5 p-value from correlation test\n\nFit a correlation test using cor.test() and extract the p value.\nUse the data_for_correlation data set.\nReturn results in a tibble.\n\n\n\n1.3.6 Pearson’s r and its p-value from cor.test()\n\nFit a correlation test using cor.test() and extract the p value and correlation.\nUse the data_for_correlation data set.\nReturn results in a tibble.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Required skills</span>"
    ]
  },
  {
    "objectID": "chapters/2_foundation_concepts.html",
    "href": "chapters/2_foundation_concepts.html",
    "title": "2  Foundation concepts",
    "section": "",
    "text": "2.1 Why do we use simulation?\nImagine you’re throwing yourself a birthday party, and need to buy food and drinks for everyone who is coming. First of all: happy birthday! Second of all: you probably need to figure out how much food and drink to buy in order to make sure everybody has enough to eat and enough to drink. At the same time, you’re not made of money, so you probably don’t want to buy too much, either.\nIn this scenario, you probably will consider some hypothetical possibilities to help you estimate how much food and drink you’ll need. You might think: well, if 20 people come, and they eat an average of half a pizza each, plus 4 bottles of soda each, then we’ll probably need 20 * .5 = 10 pizzas, and 20 * 4 = 80 bottles. Easy!\nBut wait - your best friend is coming, and she always eats at least one and a half full pizzas by herself! Plus maybe some people will have already eaten before they arrive. Some people are also going to another party after yours finishes, so they might not stay that long, so maybe they won’t drink as much as you expect. And maybe fewer than 20 people will show up! Or more! How could we plan for this when there is so much uncertainty?\nUsing simple averages like we did above is easy, but it’s imperfect. So, using your statistics training, you decide to figure out the distribution of food and drink consumption, as well the number of attendees. So you think: OK, I think people will eat pizzas and drink bottles following normal distributions. People will eat an average of 0.5 pizzas, with a standard deviation of 0.1 pizzas. And people will drink 4 bottles, with a standard deviation of 1 bottle. Then you think the same about attendees: on average 20 people will come, with a standard deviation of 2 people. Great!\nExcept…what do you do with this information? These distributions are interesting, and maybe even correct, but how can we translate this into an actionable decision about how much food and drink you should buy for your party?\nWell, what if you threw 100 parties with the means and SDs for food, drink, and attendance above? In such a case, you could figure out some smart way of how much to buy. For example, you could say: I will buy enough food and drink to ensure that, in 95 out of those 100 parties, I would have enough for everyone.\nThis is one of the many, many scenarios that we can use simulation for. When we can make decent guesses about statistical properties of possible scenarios (like how much food and drink people will consume, and how many people will show up at the party) then we can generate possible scenarios many times using these statistical properties, and then make inferences about the things we care about (in this case, how much food and drink to buy). Simulation can extend far beyond simple applications like this - we use it to ensure that production lines in factories run smoothly; to ensure that products are functioning effectively, and XXX.\nYou will learn all about how, when, and why to use simulation in this course. For now, let’s get an answer to our party question. You won’t know what’s happening in the code right now - that’s OK. We will build up your understanding over the following chapters so that you can come back and easily understand this. For now:\nCode\n# set seed\nset.seed(2026)\n\n# Design experiment\nexperiment_parameters &lt;- expand_grid(\n  party_number = 1:100,\n  mean_number_of_attendees = 20,\n  sd_number_of_attendees = 2,\n  mean_pizza_eaten = 0.5,\n  sd_pizza_eaten = 0.1,\n  mean_bottles_drank = 4,\n  sd_bottles_drank = 1\n)\n\n# Generate data function\ngenerate_data &lt;- function(mean_number_of_attendees,\n                          sd_number_of_attendees,\n                          mean_pizza_eaten,\n                          sd_pizza_eaten,\n                          mean_bottles_drank,\n                          sd_bottles_drank) {\n  \n  number_of_attendees &lt;- rnorm(1, mean_number_of_attendees, sd_number_of_attendees)\n  \n  dat &lt;- tibble(\n    attendee = c(1:number_of_attendees),\n    pizzas_eaten = round(rnorm(number_of_attendees, mean_pizza_eaten, sd_pizza_eaten), 1),\n    bottles_drank = round(rnorm(number_of_attendees, mean_bottles_drank, sd_bottles_drank), 0)\n  )\n  \n  dat\n  \n}\n\n\n# analyze data function\nanalyze_data &lt;- function(dat) {\n  \n  tibble(\n    total_pizza_eaten = sum(dat$pizzas_eaten),\n    total_bottles_drank = sum(dat$bottles_drank)\n  )\n\n}\n\n# Run the experiment many times\nsimulation_results &lt;- experiment_parameters |&gt;\n  mutate(\n    generated_data = pmap(\n      .l = list(\n        mean_number_of_attendees,\n        sd_number_of_attendees,\n        mean_pizza_eaten,\n        sd_pizza_eaten,\n        mean_bottles_drank,\n        sd_bottles_drank), \n      .f = generate_data \n      ),\n    results = map(generated_data, analyze_data)\n  ) |&gt;\n  unnest(results)\n\n\n# summarise results\nsimulation_results |&gt;\n  summarise(\n    pizzas_required = quantile(total_pizza_eaten, .95),\n    bottles_required = quantile(total_bottles_drank, .95)\n    )\n\n\n# A tibble: 1 × 2\n  pizzas_required bottles_required\n            &lt;dbl&gt;            &lt;dbl&gt;\n1            11.2             92.0\nTo make sure that 95 out of 100 parties would be covered, we would need 12 pizzas and 93 bottles of beer!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Foundation concepts</span>"
    ]
  },
  {
    "objectID": "chapters/2_foundation_concepts.html#how-are-simulations-different-from-studies",
    "href": "chapters/2_foundation_concepts.html#how-are-simulations-different-from-studies",
    "title": "2  Foundation concepts",
    "section": "2.2 How are simulations different from studies?",
    "text": "2.2 How are simulations different from studies?\nIn the party example above, we didn’t actually throw 100 birthday parties. We pretended to, using a few assumptions about what parties tend to look like (how many people show up, how much they eat and drink). That’s the key difference: - In a study, you collect real data from the world. - In a simulation, you generate fake-but-plausible data from a model of the world.\nIt might sound a bit strange to generate fake data to learn real things about the world. But this is exactly what makes simulation useful: it lets us explore “what would happen if…?” questions without having to pay for 100 parties, recruit 10,000 participants, or wait six months for data collection.\n\n2.2.1 A study learns from the world\nIn an empirical study, the world is the data-generating process. You don’t decide what the distribution of pizza consumption is — you discover it by measuring. Your sample contains surprises, messiness, measurement error, attrition, weird participants, and all the other things that make research both interesting and painful.\nSo the logic of a study is: 1. Collect data from the world. 2. Summarise and analyse that data. 3. Infer something about the process that produced it (and, hopefully, about the wider population).\nEven when we use statistical models in studies, the model is our tool for learning from data we didn’t control.\n\n\n2.2.2 A simulation learns from your assumptions\nIn a simulation, you flip that around: the model is the data-generating process. You decide, in advance, what the distributions look like (or what mechanism produces the data). Then you generate many datasets from that mechanism, and try to understand what happens under those conditions.\nSo the logic of a simulation is: 1. Make experiments that assume things about how data are generated 2. Generate data under these assumptions 3. Analyse this dataset the same way you would analyse real data 4. Repeat this process many times 5. Summarise the results across datasets and experiment conditions\nThis is why simulation is sometimes described as doing statistics “in a sandbox”: you control the world, and you watch what your analysis does inside it.\n\n\n2.2.3 Are simulations just making things up?\nYes! But not randomly. A simulation is only as good as the assumptions you put in. If you assume pizza consumption is normally distributed, you will generate a world where negative pizza consumption is theoretically possible. If you assume attendance varies with a standard deviation of 2, you’re asserting that wildly different attendance (say, 5 people or 60 people) is extremely unlikely.\nAnd this is the point: simulation forces you to think about what processes might generate data.\nA useful way to think about it is: - A study asks: “What is true in the world?” - A simulation asks: “If the world worked like this, what would happen?”\nWe use simulation because many questions we care about are hard to answer directly with studies. For example: - If I run this analysis on data like mine, how often will I get a false positive? - How much sample size do I need for reasonable power in my design (not in a textbook idealisation)? - If I use method A vs method B, which one is more biased under realistic conditions? - If participants drop out in a certain way, how badly does it distort my results? - What is the distribution of a p-value under the null hypothesis?\nYou can’t usually run a real study thousands of times under controlled conditions to find out. But you can simulate it.\n\n\n2.2.4 Simulations don’t replace studies\nSimulation is not a substitute for collecting data. It’s a tool for understanding methods, designing studies, stress-testing assumptions, and building intuition about uncertainty.\nIn the party example, simulation helped you make a decision given uncertainty — but the quality of that decision depended on whether your guessed means and standard deviations were sensible. If your friends are secretly world champion pizza-eaters and eat 3 pizzas each, your simulation will confidently recommend the wrong amount of pizza to buy\nIn short: studies are messy but grounded in reality; simulations are clean but grounded in assumptions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Foundation concepts</span>"
    ]
  },
  {
    "objectID": "chapters/2_foundation_concepts.html#key-components-of-a-monte-carlo-simulation-study",
    "href": "chapters/2_foundation_concepts.html#key-components-of-a-monte-carlo-simulation-study",
    "title": "2  Foundation concepts",
    "section": "2.3 Key components of a Monte Carlo simulation study",
    "text": "2.3 Key components of a Monte Carlo simulation study\nMonte Carlo simulation studies are defined by the following key features:\n\nConstruct an experiment\nGenerate data\nAnalyze data\nRepeat this many times\nSummarize results over the experiment conditions\n\nFuture chapters will spell out in more detail what each one involves, but for the moment it is useful to rote learn them and repeat them to yourself. They serve two purposes. First, they are the best way to think about how a simulation actually works. Second, when we are writing code in this book, we will break it down in terms of each of these steps. Equally importantly for learners, errors or complications when learning to write simulations are, in my experience, very often due to inadequate separation of the steps. E.g., when your code does some data generation in the analysis block, or vice-versa.\nTo very briefly show you these components, we can build the code up in stages to create a very simple simulation answering “what is the distribution of p-values under the null hypothesis?”\n\n2.3.1 Construct an experiment\nWe plan to construct a simulation study experiment where data are sampled from two normally distributed populations with the no difference in means; to test for this difference in means using a Student’s t-test’s p-value; to do these generate-and-analyze steps many times; and to observe the distribution of p-values.\n\n\n2.3.2 Generate data\nDraw data from a normally distributed population using rnorm() and plot it.\n\n\nCode\nrnorm(n = 50, m = 0, sd = 1) |&gt; \n  hist()\n\n\n\n\n\n\n\n\n\nNow, we generate two normally distributed data sets and fit a Student’s t-test.\nCheck your learning: In the code above, the population means in both samples are equivalent. What does a t-test test for? How does this choice of means implement a null population effect?\n\n\n\n\n\n\nClick to show answer\n\n\n\n\n\nt-tests allow us to make inferences about whether two population means differ.\nWhile many of us are in the habit of simply saying the t-test “tests for differences”, you’ll learn in this course that it’s much clearer and more precise to say that they (a) test for differences in means and (b) test for differences at the population level, not the sample. We can trivially say that the sample means are different or not depending on whether they’re numerically identical or not. For example, if we observe the sample means \\(M_{intervention}\\) = 3.11 and \\(M_{control}\\) = 3.14, no statistical test is needed to see that the sample means are different. Whether or not the populations they are drawn from are likely to be different requires an inference test, such as a t-test.\nThe above code implements a null population effect, i.e., where the two populations have no differences in means between them, by using the same value for m in both rnorm() call (i.e., 0). Even though many of the samples that will be drawn from these populations will be numerically different, the populations they are drawn from do not differ in their means.\n\n\n\n\n\n2.3.3 Analyse data\nGenerate two normally distributed data sets and fit a t-test and extract the p-value\n\n\nCode\nt.test(x = rnorm(n = 50, m = 0, sd = 1), \n       y = rnorm(n = 50, m = 0, sd = 1),\n       var.equal = TRUE)\n\n\n\n    Two Sample t-test\n\ndata:  rnorm(n = 50, m = 0, sd = 1) and rnorm(n = 50, m = 0, sd = 1)\nt = -0.63517, df = 98, p-value = 0.5268\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.5292151  0.2725824\nsample estimates:\n  mean of x   mean of y \n-0.09998801  0.02832836 \n\n\n\n\nCode\nt.test(x = rnorm(n = 50, m = 0, sd = 1), \n       y = rnorm(n = 50, m = 0, sd = 1), \n       var.equal = TRUE)$p.value\n\n\n[1] 0.9171144\n\n\n\n\n2.3.4 Repeat this many times and summarize results\nWe repeat this using the replicate() function, and summarize across the iterations by plotting the p-values using hist().\n\n\nCode\nreplicate(n = 100, \n          expr = t.test(x = rnorm(n = 50, m = 0, sd = 1), \n                        y = rnorm(n = 50, m = 0, sd = 1), \n                        var.equal = TRUE)$p.value) |&gt; \n  hist()\n\n\n\n\n\n\n\n\n\nNext we increase the number of iterations, i.e., the number of times we repeat the generate-and-analyze steps.\n\n\nCode\nreplicate(n = 100000, \n          expr = t.test(x = rnorm(n = 50, m = 0, sd = 1), \n                        y = rnorm(n = 50, m = 0, sd = 1), \n                        var.equal = TRUE)$p.value) |&gt; \n  hist()\n\n\n\n\n\n\n\n\n\nCheck your learning: Why is the distribution more uniform when the number of iterations is higher?\n\n\n\n\n\n\nClick to show answer\n\n\n\n\n\nJust like in real-world studies, the number of samples from the population influences how stable the estimates are. The distribution of p-values in the smaple is only pefectly uniform when sample size is infinite. So, as simulated sample size increases, the uniformity of the distribution of p-values will generally increase.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Foundation concepts</span>"
    ]
  },
  {
    "objectID": "chapters/2_foundation_concepts.html#compare-with-distribution-of-p-values-when-population-means-are-not-equal",
    "href": "chapters/2_foundation_concepts.html#compare-with-distribution-of-p-values-when-population-means-are-not-equal",
    "title": "2  Foundation concepts",
    "section": "2.4 Compare with distribution of p-values when population means are not equal",
    "text": "2.4 Compare with distribution of p-values when population means are not equal\n‘Small’, ‘medium’ and ‘large’ differences in population means.\n\n\nCode\nhist(replicate(100000, t.test(rnorm(n = 50, m = 0, sd = 1), rnorm(n = 50, m = 0.2, sd = 1), var.equal = TRUE)$p.value))\n\n\n\n\n\n\n\n\n\nCode\nhist(replicate(100000, t.test(rnorm(n = 50, m = 0, sd = 1), rnorm(n = 50, m = 0.5, sd = 1), var.equal = TRUE)$p.value))\n\n\n\n\n\n\n\n\n\nCode\nhist(replicate(100000, t.test(rnorm(n = 50, m = 0, sd = 1), rnorm(n = 50, m = 0.8, sd = 1), var.equal = TRUE)$p.value))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Foundation concepts</span>"
    ]
  },
  {
    "objectID": "chapters/2_foundation_concepts.html#what-is-the-distribution-of-p-values-under-the-null-hypothesis-newer-plotting",
    "href": "chapters/2_foundation_concepts.html#what-is-the-distribution-of-p-values-under-the-null-hypothesis-newer-plotting",
    "title": "2  Foundation concepts",
    "section": "2.5 What is the distribution of p values under the null hypothesis? [newer plotting]",
    "text": "2.5 What is the distribution of p values under the null hypothesis? [newer plotting]\n\n\nCode\nsim_h0 &lt;- replicate(100000, t.test(rnorm(n = 500, m = 0, sd = 1), rnorm(n = 500, m = 0, sd = 1), var.equal = TRUE)$p.value)\n\npercent_barely_sig_h0 &lt;- round_up(mean(sim_h0 &lt; .05 & sim_h0 &gt; .02)*100, 1)\n\nggplot(data.frame(p = sim_h0), aes(p)) +\n  geom_histogram(binwidth = 0.01, boundary = 0, fill = \"grey\") +\n  scale_x_continuous(limits = c(0, 1), breaks = c(0, .05, .2, .4, .6, .8, 1)) +\n  geom_vline(xintercept = 0.05, linetype = \"dashed\", color = \"purple\") +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nIn the long run of 100,000 studies with large sample sizes (N = 1000) and when the population effect size is zero (Cohen’s d = 0), 2.9% of Student t-test p-values fall between .02 and .05.\nThis corresponds with the alpha value of the test, i.e., the p &lt; .05 inference rule. When the population effect is zero, we will incorrectly conclude that it is non-zero in 5% of the long run of studies.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Foundation concepts</span>"
    ]
  },
  {
    "objectID": "chapters/2_foundation_concepts.html#how-likely-are-barely-significant-p-values",
    "href": "chapters/2_foundation_concepts.html#how-likely-are-barely-significant-p-values",
    "title": "2  Foundation concepts",
    "section": "2.6 How likely are barely significant p-values?",
    "text": "2.6 How likely are barely significant p-values?\n[A second simulation, to show their utility to raise as well as answer questions]\nMany studies in the psychology literature report p-values between .02 and .05 (Masicampo & Lalande, 2012; Hartgerink et al., 2016). We can call these ‘barely significant p-values’.\nThe common threshold for statistical significance, p &lt; .05, treats all p values less than the threshold as equivalent. Regardless of whether your p-value is .048 or .000000000000000000001, the p &lt;. 05 rule says you should conclude that there are detectable differences (e.g., differences in population means from a Student’s t-test).\nBut is there a tipping point around .05 that makes this value particularly useful? Again, we could work through the math of it, but it is equally useful to simulate the long run of p-values and see how many are barely significant (.02 &lt; p &lt; .05). If they rarely occur, why do we use them in the rule?\n\n\nCode\nsim_h1 &lt;- replicate(100000, t.test(rnorm(n = 500, m = 0, sd = 1), rnorm(n = 500, m = 0.8, sd = 1), var.equal = TRUE)$p.value)\n\npercent_barely_sig_h1 &lt;- round_up(mean(sim_h1 &lt; .05 & sim_h1 &gt; .02)*100, 1)\n\nggplot(data.frame(p = sim_h1), aes(p)) +\n  geom_histogram(binwidth = 0.001, boundary = 0, fill = \"grey\") +\n  scale_x_continuous(limits = c(0, 0.05), breaks = breaks_pretty(5)) +\n  geom_vline(xintercept = 0.05, linetype = \"dashed\", color = \"purple\") +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nIn the long run of 100,000 studies with large sample sizes (N = 1000) and when the population effect size is large (Cohen’s d = 0.8), 0% of Student t-test p-values fall between .02 and .05:\nSo, although the common statistical significance cut-off is 5% treats all p-values less than this equally, this quick simulation suggests that - to misquote George Orwell - some p-values are more equal than others.\nBut if barely significant p-values are rarely (or never) observed, why do we use p &lt; .05 as the alpha value? Why not something lower, like .01, since it would decrease the number of false positives in the literature? This has certainly been argued for in the literature (Benjamin et al., 2018). And, perhaps more worryingly, if they’re so statistically rare, why do we see them so often in the literature?\nThe answers to these questions require us to dig deeper into p-values and what affects their distributions using more complicated simulations (e.g., Maassen et al., 2025; Stefan & Schönbrodt, 2023). The take away message, for the moment, is that simulations can help give us pause for thought, help us check our own understanding, clarify our thinking, or answer meaningful questions - sometimes in only a few lines of code.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Foundation concepts</span>"
    ]
  },
  {
    "objectID": "chapters/2_foundation_concepts.html#check-your-learning",
    "href": "chapters/2_foundation_concepts.html#check-your-learning",
    "title": "2  Foundation concepts",
    "section": "2.7 Check your learning",
    "text": "2.7 Check your learning\n\n2.7.0.1 What are the five core components of a Monte Carlo simulation?\n\n\n\n\n\n\nClick to show answer\n\n\n\n\n\n\nConstruct an experiment\nGenerate data\nAnalyze data\nRepeat these many times\nSummarize results over the experiment conditions",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Foundation concepts</span>"
    ]
  },
  {
    "objectID": "chapters/3_writing_functions.html",
    "href": "chapters/3_writing_functions.html",
    "title": "3  Writing functions",
    "section": "",
    "text": "3.1 Overview\nTwo of the key steps in a simulation study (generate data and analyze data) require us to know how to write functions. This R Markdown lesson practices this.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Writing functions</span>"
    ]
  },
  {
    "objectID": "chapters/3_writing_functions.html#what-is-a-function",
    "href": "chapters/3_writing_functions.html#what-is-a-function",
    "title": "3  Writing functions",
    "section": "3.2 What is a function?",
    "text": "3.2 What is a function?\nA function is a piece of code that receives an input, does something to it, and returns an output. That’s it! Of course, that’s also a very vague definition. But let’s look at the most simple example:\n\n\nCode\noutput &lt;- 1 + 1\n\nprint(output)\n\n\n[1] 2\n\n\nThe + operator is a function. It takes some input (one number either side of it), does something to this input (adds the two numbers together), and returns an output (the value of the two numbers added together). We use the &lt;- operator to assign this to an object called output. The assignment operator is also a function, as is print().\nWhen we call functions in R, they tend to take the form that print() does, i.e., a function name followed by its arguments inside round brackets: function_name(argument1, argument2, ...argumentN).\nThe equivalent to + in this style is the sum() function. Let’s see what happens when we run sum(1, 1):\n\n\nCode\noutput &lt;- sum(c(1, 1))\n\nprint(output)\n\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Writing functions</span>"
    ]
  },
  {
    "objectID": "chapters/3_writing_functions.html#writing-functions",
    "href": "chapters/3_writing_functions.html#writing-functions",
    "title": "3  Writing functions",
    "section": "3.3 Writing functions",
    "text": "3.3 Writing functions\nPreviously, you saw a few functions: sum(), round(), mean(), and sqrt(). These are all in-built functions in R, meaning that R “knows” them by default. One other way that R can “know” a function is if we write it ourselves!\nThe way to write a function in R is simple. It takes the following format:\nfunction_name &lt;- function(argument1, argument2, ... argumentN) { [function goes in here] }\nLet’s look at a simple example. We’ll create a new function called sum2(), which is identical to sum(), except it also rounds the output to two decimal places at the end. We’ll also compare the outputs of sum() and sum2():\n\n\nCode\nsum2 &lt;- function(first_number, second_number) {\n  \n  round(first_number + second_number, digits = 2)\n  \n}\n\nsum(2.22222, 3.33333)\n\n\n[1] 5.55555\n\n\nCode\nsum2(2.22222, 3.33333)\n\n\n[1] 5.56\n\n\nCreating functions is very, very important for Monte Carlo simulations. Recall our five steps:\n\nConstruct an experiment\nGenerate data\nAnalyze the data\nDo things many times\nSummarise the results\n\nWe will use functions in all 5 of these steps. And we will basically always write new functions for steps 2 and 3 (and often, step 5).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Writing functions</span>"
    ]
  },
  {
    "objectID": "chapters/3_writing_functions.html#why-functions-are-useful-the-false-positive-rate-of-ai-detection-tools",
    "href": "chapters/3_writing_functions.html#why-functions-are-useful-the-false-positive-rate-of-ai-detection-tools",
    "title": "3  Writing functions",
    "section": "3.4 Why functions are useful: the false positive rate of AI detection tools",
    "text": "3.4 Why functions are useful: the false positive rate of AI detection tools\nImagine a test has has a false positive rate of 5%, like the standard alpha value for a p-value.\nIf you apply this test many times to independent cases without applying familywise error corrections, what is the resulting false positive rate?\nLet’s put this in meaningful terms. Many professors and universities now run essays and assignments through AI detection tools. But students submit many essays and assignments throughout their degree. What is the probability that, assuming you never violate the AI policy, one of your assignments is falsely flagged as using AI in a way that wasn’t allowed?\n\n3.4.1 Math\nThe familywise error rate (\\(\\alpha_{\\text{total}}\\)) for independent tests is the probability of observing at least one false positive (\\(P(V \\geq 1)\\)), which can be reexpressed as the probability of not observing true positives (\\(1 - P(V = 0)\\)). This is the product of the individual probabilities of false positives (\\(1 - \\prod_{i=1}^{n} (1 - \\alpha_i)\\)), which simplifies to:\n\\[\n\\begin{align}\n\\alpha_{\\text{total}} &= P(V \\geq 1) \\\\\n&= 1 - P(V = 0) \\\\\n&= 1 - \\prod_{i=1}^{n} (1 - \\alpha_i) \\\\\n&= 1 - (1 - \\alpha)^n\n\\end{align}\n\\]\nHorrible, disgusting math which most of you want to avoid. But the code to do it is quite simple.\n\n\n3.4.2 Hard coded\nAssuming individual false positives of 5% and 10 total tests.\n\n\nCode\n1 - (1 - 0.05)^10\n\n\n[1] 0.4012631\n\n\n\n\n3.4.3 Using variables\nWhat about other values of alpha and n? We could make them variables at the top of the chunk that are easier to change.\n\n\nCode\n# variables\nalpha &lt;- 0.05\nn &lt;- 10\n\n# code\n1 - (1 - alpha)^n\n\n\n[1] 0.4012631\n\n\n\n\n3.4.4 Written as a function\nDefining our own custom function involves putting the variables as arguments inside the function() call, and putting the code inside its {}.\nWe can then call the function using its name.\n\n\nCode\n# define function\ncalc_aggregate_fpr &lt;- function(n, alpha = 0.05) {\n  1 - (1 - alpha)^n\n}\n\n# usage\ncalc_aggregate_fpr(n = 10)\n\n\n[1] 0.4012631\n\n\nThis also lets us call it an arbitrary number of times, eg for different values of n\n\n\nCode\ncalc_aggregate_fpr(n =  5)\n\n\n[1] 0.2262191\n\n\nCode\ncalc_aggregate_fpr(n = 10)\n\n\n[1] 0.4012631\n\n\nCode\ncalc_aggregate_fpr(n = 15)\n\n\n[1] 0.5367088\n\n\nCode\ncalc_aggregate_fpr(n = 20)\n\n\n[1] 0.6415141\n\n\nCode\ncalc_aggregate_fpr(n = 25)\n\n\n[1] 0.7226104\n\n\n\n\n3.4.5 Use one of {purrr}’s map() functions to apply the custom function to many inputs\nThis previews some skills we’ll learn in a future chapter: calling a function multiple times within a tidy workflow by mapping it on to a set of input columns.\n\n\nCode\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(purrr)\n\nexperiment &lt;- \n  # define many values of n as different rows in a data frame or tibble\n  tibble(n = 1:20) \n\n# view \nexperiment\n\n\n# A tibble: 20 × 1\n       n\n   &lt;int&gt;\n 1     1\n 2     2\n 3     3\n 4     4\n 5     5\n 6     6\n 7     7\n 8     8\n 9     9\n10    10\n11    11\n12    12\n13    13\n14    14\n15    15\n16    16\n17    17\n18    18\n19    19\n20    20\n\n\nCode\nresults &lt;- experiment |&gt;  \n  # use mutate to create a new column, fpr, by calling the custom function for each row, using n as the input\n  # use a _dbl map function as the output is a numeric (double precision floating point) variable\n  mutate(fpr = map_dbl(.x = n, \n                       .f = calc_aggregate_fpr))\n\nresults\n\n\n# A tibble: 20 × 2\n       n    fpr\n   &lt;int&gt;  &lt;dbl&gt;\n 1     1 0.0500\n 2     2 0.0975\n 3     3 0.143 \n 4     4 0.185 \n 5     5 0.226 \n 6     6 0.265 \n 7     7 0.302 \n 8     8 0.337 \n 9     9 0.370 \n10    10 0.401 \n11    11 0.431 \n12    12 0.460 \n13    13 0.487 \n14    14 0.512 \n15    15 0.537 \n16    16 0.560 \n17    17 0.582 \n18    18 0.603 \n19    19 0.623 \n20    20 0.642 \n\n\nBecause we’ve done this in a tidy format in a tibble, we can easily plot the results too.\n\n\nCode\nlibrary(ggplot2)\n\nggplot(results, aes(n, fpr)) +\n  geom_line() +\n  geom_point() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nWhat if we want to vary not only n but also alpha, and see all combinations of them? This previews some skills we’ll learn in a later chapter: defining crossed experiment conditions using expand_grid().\n\n\nCode\nlibrary(tidyr)\n\n# define many values of n and alpha, and then have them 'crossed' to create all permutations of them, using expand_grid()\nexperiment &lt;- \n  expand_grid(n = 1:20,\n              alpha = c(0.01, 0.05, 0.10)) \n\n# view the combinations used in the experiment\nexperiment\n\n\n# A tibble: 60 × 2\n       n alpha\n   &lt;int&gt; &lt;dbl&gt;\n 1     1  0.01\n 2     1  0.05\n 3     1  0.1 \n 4     2  0.01\n 5     2  0.05\n 6     2  0.1 \n 7     3  0.01\n 8     3  0.05\n 9     3  0.1 \n10     4  0.01\n# ℹ 50 more rows\n\n\nCode\nresults &lt;- experiment |&gt;  \n  # use mutate to create a new column, fpr, by calling the custom function for each row\n  # use a map2_ function as there are two inputs, n and alpha\n  # these are passed by location, .x to the first argument, .y to the second\n  mutate(fpr = map2_dbl(.x = n, \n                        .y = alpha,\n                        .f = calc_aggregate_fpr))\n\nresults\n\n\n# A tibble: 60 × 3\n       n alpha    fpr\n   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     1  0.01 0.0100\n 2     1  0.05 0.0500\n 3     1  0.1  0.1   \n 4     2  0.01 0.0199\n 5     2  0.05 0.0975\n 6     2  0.1  0.19  \n 7     3  0.01 0.0297\n 8     3  0.05 0.143 \n 9     3  0.1  0.271 \n10     4  0.01 0.0394\n# ℹ 50 more rows\n\n\n\n\nCode\nresults |&gt;\n  mutate(alpha = as.factor(alpha)) |&gt;\n  ggplot(aes(n, fpr, color = alpha, group = alpha)) +\n  geom_line() +\n  geom_point() +\n  theme_linedraw()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Writing functions</span>"
    ]
  },
  {
    "objectID": "chapters/3_writing_functions.html#primer-on-functions",
    "href": "chapters/3_writing_functions.html#primer-on-functions",
    "title": "3  Writing functions",
    "section": "3.5 Primer on functions",
    "text": "3.5 Primer on functions\nMost code we use are functions, e.g., mean(), setwd() and library().\nThese functions were written by others, but we can write our own.\n“It’s functions all the down”: you will use existing functions to write new ones. For example:\n\n\nCode\nvalues &lt;- c(4, 2, 6, 2, NA, 4, 3, 1, NA, 7, 5)\n\nmean(values) # returns NA \n\n\n[1] NA\n\n\nCode\nmean(values, na.rm = TRUE) # returns the mean after dropping NA\n\n\n[1] 3.777778\n\n\nCode\n# tired of writing 'na.rm = TRUE' repeatedly? write your own function to do it automatically\nmean_na_rm &lt;- function(x){\n  mean(x, na.rm = TRUE)\n}\n\nmean_na_rm(values) # returns the mean after dropping NA\n\n\n[1] 3.777778\n\n\nWhat if we usually want to round() to two decimal places, and we’re tired of writing digits = 2 every time?\n\n\nCode\nmean_of_values &lt;- mean_na_rm(values)\n\nround(mean_of_values, digits = 2)\n\n\n[1] 3.78\n\n\nCode\n# write a function to always round to two decimal places\nround_2 &lt;- function(x){\n  round(x, digits = 2)\n}\n\nround_2(mean_of_values)\n\n\n[1] 3.78\n\n\n\n3.5.1 General structure of a function\nFunctions (usually) have ‘inputs’, they have code that they run (‘do stuff’), and they (almost always) return ‘outputs’. The often specify their requirements and include checks that their inputs are correctly formatted.\nNote that this is pseudo-code only: chunk is set not to run (eval=FALSE).\n\n\nCode\n# define function\nfunction_name &lt;- function(argument_1, # first argument is often the data, if the function takes a data frame as an argument\n                          argument_2 = \"default\", # arguments can have defaults\n                          argument_3) {\n  # required packages\n  require(dplyr)\n  \n  # checks\n  # well written functions contain checks. \n  # e.g., if the function assumes that argument_1 is a data frame, check that this is the case.\n  # note that it is more useful to write the function first and add checks later.\n  if(!is.data.frame(argument_1)){\n    stop(\"argument_1 must be a data frame\")\n  }\n  \n  # code that does things\n  object_to_be_returned &lt;- input_data_frame |&gt;\n    # do things\n    mutate(value = value + 1)\n  \n  # object to be returned\n  return(object_to_be_returned)\n}\n\n\n\n\n3.5.2 Example function: t-test p-value\n\n\nCode\n# data to be analyzed using the analysis function\ndata_simulated_intervention &lt;- \n  tibble(condition = \"intervention\", \n         score = rnorm(n = 50, mean = 0, sd = 1))\n\ndata_simulated_control &lt;- \n  tibble(condition = \"control\", \n         score = rnorm(n = 50, mean = 0, sd = 1))\n\ndata_simulated &lt;- \n  bind_rows(data_simulated_intervention,\n            data_simulated_control)\n\n\n\n\nCode\n# define function\nt_test_p_value &lt;- function(data) {\n  \n  res &lt;- t.test(formula = score ~ condition, \n                data = data)\n  \n  return(res$p.value)\n}\n\n# call function\nt_test_p_value(data_simulated)\n\n\n[1] 0.6399378\n\n\nHow would I build this from scratch? What’s the first thing I would type?\n\n\n3.5.3 General things to remember when writing functions\n\nIf you can’t immediately write the code, write pseudo-code first!\nBuild the ‘do stuff’ part outside of a function first!\nWrap the ‘do stuff’ with input and output after you have ‘do stuff’ working. Why: so you don’t have to fight variable scoping.\nThe function must be present in your environment to be usable, and must be called to be used\nCheck that your function actually works as you expect, not just that it runs. Give it lots of different input values that should and should not work, and check you get the correct outputs.\nDon’t try to abstract more than you need. One function should do one thing. Elaborate the function only as needed.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Writing functions</span>"
    ]
  },
  {
    "objectID": "chapters/3_writing_functions.html#practice-writing-functions",
    "href": "chapters/3_writing_functions.html#practice-writing-functions",
    "title": "3  Writing functions",
    "section": "3.6 Practice writing functions",
    "text": "3.6 Practice writing functions\nWrite functions below that can be applied to the following data sets, i.e., use these data sets to guide how you write the functions and test that they work.\n\n3.6.1 Exercise 1\nThe purpose of a t-test is to test differences in means between groups.\nWeirdly, base-R’s built in t.test() function does not actually return an estimate of this difference in means between groups, as Uri Simonsohn recently pointed out.\nWrite a function that fits a t-test, returns results in tidy format, and includes an estimate of the difference in means between groups.\nThere are two ways to do this: 1) calculating the means from the data directly, 2) calculating the difference from the two estimated means returned by the t.test() function. Practice writing functions by implementing both.\n\n\n3.6.2 Exercise 2\n\n\nCode\nlibrary(forcats)\nlibrary(faux)\n\nset.seed(42)\n\n# data for t test\ndata_intervention &lt;- \n  tibble(condition = \"intervention\", \n         score = rnorm(n = 50, mean = 0, sd = 1))\n\ndata_control &lt;- \n  tibble(condition = \"control\", \n         score = rnorm(n = 50, mean = 0, sd = 1))\n\ndata_for_ttest &lt;- \n  bind_rows(data_intervention,\n            data_control) |&gt;\n  # control's factor levels must be ordered so that intervention is the first level and control is the second\n  # this ensures that positive Cohen's d values refer to intervention &gt; control and not the other way around.\n  mutate(condition = fct_relevel(condition, \"intervention\", \"control\"))\n\n\n# data for correlation\ndata_for_correlation &lt;- rnorm_multi(n = 100, \n                                    vars = 2, \n                                    mu = 0, \n                                    sd = 1, \n                                    r = 0.5, \n                                    varnames = c(\"X\", \"Y\"))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Writing functions</span>"
    ]
  },
  {
    "objectID": "chapters/3_writing_functions.html#further-reading",
    "href": "chapters/3_writing_functions.html#further-reading",
    "title": "3  Writing functions",
    "section": "3.7 Further reading",
    "text": "3.7 Further reading\nAlthough we have practiced writing custom functions to extract statistical results / model parameters, it is worth knowing that the {easystats} family of packages includes {parameters} package, which does a very good job of extracting model parameters from a very wide range of models including base R functions, {lavaan}, {psych}, and other packages. If you want to extract values from a model, consider using {parameters} to do a lot of the work for you when writing your function.\nSeparately, the {report} package will fully report the results of many common analyses for you. e.g.:\n\n\nCode\nlibrary(report)\n\ndata_simulated_intervention &lt;- \n  tibble(condition = \"intervention\", \n         score = rnorm(n = 50, mean = 0, sd = 1))\n\ndata_simulated_control &lt;- \n  tibble(condition = \"control\", \n         score = rnorm(n = 50, mean = 0, sd = 1))\n\ndata_simulated &lt;- \n  bind_rows(data_simulated_intervention,\n            data_simulated_control)\n\nt.test(score ~ condition, data = data_simulated) |&gt;\n  report::report()\n\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Welch Two Sample t-test testing the difference of score by condition (mean\nin group control = 0.13, mean in group intervention = -0.06) suggests that the\neffect is positive, statistically not significant, and small (difference =\n0.19, 95% CI [-0.16, 0.54], t(95.08) = 1.08, p = 0.283; Cohen's d = 0.22, 95%\nCI [-0.18, 0.62])",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Writing functions</span>"
    ]
  },
  {
    "objectID": "chapters/3_writing_functions.html#to-add",
    "href": "chapters/3_writing_functions.html#to-add",
    "title": "3  Writing functions",
    "section": "3.8 To add",
    "text": "3.8 To add\n\nfunction extraction resources https://www.appsilon.com/post/r-studio-shortcuts-and-tips-part-2\ndocumenting functions using roxygen, organizing them into an R package to make them easy to load and include help menus, or writing unit tests them.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Writing functions</span>"
    ]
  },
  {
    "objectID": "chapters/4_data_generating_process_functions.html",
    "href": "chapters/4_data_generating_process_functions.html",
    "title": "4  Data generating processes",
    "section": "",
    "text": "TODO",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data generating processes</span>"
    ]
  },
  {
    "objectID": "chapters/5_analysis_functions.html",
    "href": "chapters/5_analysis_functions.html",
    "title": "5  Analysis functions",
    "section": "",
    "text": "TODO",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysis functions</span>"
    ]
  },
  {
    "objectID": "chapters/6_creating_simulation_experiments.html",
    "href": "chapters/6_creating_simulation_experiments.html",
    "title": "6  Creating simulation experiments",
    "section": "",
    "text": "6.1 Dependencies\nCode\nlibrary(tidyr)\nlibrary(dplyr)\nIn our earlier chapter, you saw examples of generating data, and specifying different parameter values in doing so. This is at the core of simulation: generating data with different parameter values, running the process lots of times, and summarising across each set of parameter values. But how do we actually create these many types of parameter values? Or, in other words: how do we actually design simulation experiments?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Creating simulation experiments</span>"
    ]
  },
  {
    "objectID": "chapters/6_creating_simulation_experiments.html#varying-experiment-parameters-manually",
    "href": "chapters/6_creating_simulation_experiments.html#varying-experiment-parameters-manually",
    "title": "6  Creating simulation experiments",
    "section": "6.2 Varying experiment parameters manually",
    "text": "6.2 Varying experiment parameters manually\nThe way that we ideally design simulation experiments is through the use of a parameter grid. This is, basically, a dataframe or tibble that specifies all of the variations of our simulations that we would want to run. For example, imagine we want to use our generate_data() and analyze() functions from the previous lessons to analyse data from two groups and get the p-value. Remember:\n\n\nCode\ngenerate_data &lt;- function(n_per_condition,\n                          mean_control,\n                          mean_intervention,\n                          sd) {\n\n  data_control &lt;-\n    tibble(condition = \"control\",\n           score = rnorm(n = n_per_condition, mean = mean_control, sd = sd))\n\n  data_intervention &lt;-\n    tibble(condition = \"intervention\",\n           score = rnorm(n = n_per_condition, mean = mean_intervention, sd = sd))\n\n  data_combined &lt;- bind_rows(data_control, data_intervention)\n\n  return(data_combined)\n}\n\nanalyze &lt;- function(data) {\n\n  res_t_test &lt;- t.test(formula = score ~ condition,\n                       data = data,\n                       var.equal = TRUE,\n                       alternative = \"two.sided\")\n\n  res &lt;- tibble(p = res_t_test$p.value)\n\n  return(res)\n}\n\n\nNow, let’s suppose we want to generate data for when n_per_condition is either 30 or 50, and where mean_intervention is either 0.3 or 0.5 (in all cases, we want mean_control = 0 and sd = 1). We could do this by just typing out each simulation, like:\n\n\nCode\n# n = 30, mean = .3\ngenerate_data(\n  n_per_condition = 30,\n  mean_control = 0,\n  mean_intervention = .3,\n  sd = 1\n) |&gt;\n  analyze()\n\n\n# A tibble: 1 × 1\n      p\n  &lt;dbl&gt;\n1 0.160\n\n\nCode\n# n = 50, mean = .3\ngenerate_data(\n  n_per_condition = 30,\n  mean_control = 0,\n  mean_intervention = .3,\n  sd = 1\n) |&gt;\n  analyze()\n\n\n# A tibble: 1 × 1\n      p\n  &lt;dbl&gt;\n1 0.489\n\n\nCode\n# n = 30, mean = .5\ngenerate_data(\n  n_per_condition = 30,\n  mean_control = 0,\n  mean_intervention = .3,\n  sd = 1\n) |&gt;\n  analyze()\n\n\n# A tibble: 1 × 1\n      p\n  &lt;dbl&gt;\n1 0.699\n\n\nCode\n# n = 50, mean = .5\ngenerate_data(\n  n_per_condition = 30,\n  mean_control = 0,\n  mean_intervention = .3,\n  sd = 1\n) |&gt;\n  analyze()\n\n\n# A tibble: 1 × 1\n      p\n  &lt;dbl&gt;\n1 0.366\n\n\nClearly this works, but the problem is pretty obvious: if we want to repeat these processes lots of times, or look at lots of different parameter combinations, then this will involve a lot of copy-pasting! This is not ideal. We will cover how to more efficiently “repeat the process lots of times” in the next lesson. But for now, let’s focus on how to look at lots of different parameter combinations more efficiently.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Creating simulation experiments</span>"
    ]
  },
  {
    "objectID": "chapters/6_creating_simulation_experiments.html#varying-experiment-parameters-using-parameter-grids",
    "href": "chapters/6_creating_simulation_experiments.html#varying-experiment-parameters-using-parameter-grids",
    "title": "6  Creating simulation experiments",
    "section": "6.3 Varying experiment parameters using parameter grids",
    "text": "6.3 Varying experiment parameters using parameter grids\nInstead of copy-pasting each setting, we can instead create a parameter grid, which specifies each combination of parameters we’re interested in. For example, suppose we want to create a grid that specifies the parameters we specified above (mean_intervention either 0.3 or 0.5, n_per_condition either 30 or 50, sd = 1, mean_control = 0). We could specify a parameter grid by repeating each level of each variable as a single row in a tibble, like this:\n\n\nCode\nparameter_grid &lt;- tibble(\n  mean_control = rep(0, 4), # repeat this 4 times - once for each combination\n  sd = 1, # repeat this 4 times - once for each combination\n  mean_intervention = rep(c(0.3, 0.5), each = 2),\n  n_per_condition = rep(c(30, 50), 2)\n)\n\nprint(parameter_grid)\n\n\n# A tibble: 4 × 4\n  mean_control    sd mean_intervention n_per_condition\n         &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;           &lt;dbl&gt;\n1            0     1               0.3              30\n2            0     1               0.3              50\n3            0     1               0.5              30\n4            0     1               0.5              50\n\n\nAs you can see above, this worked: we now have a grid that specifies each combination of parameters we’re interested in! We could then run an analysis on each of these, doing something like:\n\n\nCode\n# generate data based on first row of parameter_grid\ngenerate_data(\n  n_per_condition = parameter_grid$n_per_condition[1],\n  mean_intervention = parameter_grid$mean_intervention[1],\n  mean_control = parameter_grid$mean_control[1],\n  sd = parameter_grid$sd[1]\n) |&gt;\n  analyze()\n\n\n# A tibble: 1 × 1\n      p\n  &lt;dbl&gt;\n1 0.768\n\n\nCode\n# generate data based on second row of parameter_grid\ngenerate_data(\n  n_per_condition = parameter_grid$n_per_condition[2],\n  mean_intervention = parameter_grid$mean_intervention[2],\n  mean_control = parameter_grid$mean_control[2],\n  sd = parameter_grid$sd[2]\n) |&gt;\n  analyze()\n\n\n# A tibble: 1 × 1\n       p\n   &lt;dbl&gt;\n1 0.0619\n\n\nAgain, we will cover how to do this mapping of parameter grids to simulations more efficiently in the next lesson. But for now, there is one other inefficiency we should handle.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Creating simulation experiments</span>"
    ]
  },
  {
    "objectID": "chapters/6_creating_simulation_experiments.html#creating-parameter-grids-more-efficiently-expand_grid",
    "href": "chapters/6_creating_simulation_experiments.html#creating-parameter-grids-more-efficiently-expand_grid",
    "title": "6  Creating simulation experiments",
    "section": "6.4 Creating parameter grids more efficiently: expand_grid()",
    "text": "6.4 Creating parameter grids more efficiently: expand_grid()\nIn the example above, we constructed our parameter grid like this:\n\n\nCode\nparameter_grid &lt;- tibble(\n  mean_control = rep(0, 4), # repeat this 4 times - once for each combination\n  sd = 1, # repeat this 4 times - once for each combination\n  mean_intervention = rep(c(0.3, 0.5), each = 2),\n  n_per_condition = rep(c(30, 50), 2)\n)\n\n\nHowever, this is actually not an ideal approach. You can see we are using the rep() function, which repeats a value a number of times. This can be confusing, because we need to count the overall number of times we want each variable to be present, and we also want to have all combinations of parameters while not repeating anything. Especially if we ever want to move to more complex simulations (e.g., with many different parameter combinations) then it would become quite difficult to ensure that all combinations are accurately covered.\nInstead, we can use the handy function expand_grid(). With this function, we give each desired value of each parameter once, and then expand_grid() maps this into a tibble with all combinations of all parameters.\nFor our example above, this looks like:\n\n\nCode\nexperiment_parameters &lt;- expand_grid(\n  n_per_condition = c(30, 50),\n  mean_intervention = c(0.3, .5),\n  mean_control = 0,\n  sd = 1\n)\n\n\nexpand_grid() greatly simplifies the process of specifying parameters for our simulations.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Creating simulation experiments</span>"
    ]
  },
  {
    "objectID": "chapters/6_creating_simulation_experiments.html#sanity-checking-your-parameter-grid",
    "href": "chapters/6_creating_simulation_experiments.html#sanity-checking-your-parameter-grid",
    "title": "6  Creating simulation experiments",
    "section": "6.5 Sanity checking your parameter grid",
    "text": "6.5 Sanity checking your parameter grid\nIt can be good practice to sanity-check your parameter grid - specifically, to make sure that the number of rows in the grid is consistent with what you would expect. For example, in our example above, we would expect experiment_parameters to have 4 rows: one for each combination of mean_intervention and n_per_condition. Let’s double-check this by counting the number of rows:\n\n\nCode\nexperiment_parameters |&gt; \n  nrow()\n\n\n[1] 4\n\n\nSuccess! But of course, we could have made a mistake: for example, maybe we did something wrong in our expand_grid() call that meant we haven’t got full coverage of the parameter space we wanted. We can check this in more depth by combining this with a call to the distinct() function, which will keep only those rows which have distinct combinations of variables:\n\n\nCode\nexperiment_parameters |&gt; \n  distinct() |&gt;\n  nrow()\n\n\n[1] 4\n\n\nLooks good!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Creating simulation experiments</span>"
    ]
  },
  {
    "objectID": "chapters/6_creating_simulation_experiments.html#types-of-simulation-designs",
    "href": "chapters/6_creating_simulation_experiments.html#types-of-simulation-designs",
    "title": "6  Creating simulation experiments",
    "section": "6.6 Types of Simulation Designs",
    "text": "6.6 Types of Simulation Designs\nThere are different types of experiment designs that we might want to use in our simulation studies, usually depending on how many total combinations are possible, which of those combinations we actually find interesting, and how much time we are willing to let the simulation run for. Let’s start with the most simple design first: a fully-crossed design.\n\n6.6.1 Fully crossed designs\nA fully-crossed design is what expand_grid() gives you by default: every level of every parameter is combined with every level of every other parameter. This is especially relevant for small simulation designs (e.g., like our one above) or where we care about every level of the simulation.\nFully-crossed designs are also easy to interpret: we can look at every combination of every parameter, summarise over them, and look at the whole trend. If the early parts of this course, we will be using fully-crossed designs most often for this reason.\nLet’s suppose, for example, we want to expand our experiment above to also have two different standard deviation possibilities (0.5 or 1) and two different mean_control values (0 and 0.1). A fully-crossed version of this would be:\n\n\nCode\nexperiment_parameters &lt;- expand_grid(\n  n_per_condition = c(30, 50),\n  mean_intervention = c(0.3, 0.5),\n  sd = c(0.5, 1),\n  mean_control = c(0, 0.1)\n)\n\nexperiment_parameters |&gt; nrow()\n\n\n[1] 16\n\n\nFully-crossed designs are ideal when you want to understand how performance changes across multiple factors independently. This means that each parameter should plausibly be able to take each value regardless of the others (e.g., a mean_intervention of 0.5 should be possible under any of the conditions of the other parameters).\nHowever, sometimes parameter combinations are not possible. For example, imagine we’re simulating human heights across different age groups. A mean height of 180 cm might be perfectly plausible for a dataset when the mean age is 18, but it would be implausible when the mean age is 12. Likewise, a mean height of 140cm might be possible among 12 year olds, but not adults.\nIf we simulated this as a fully-crossed design, this would look something like:\n\n\nCode\nheight_parameters &lt;- expand_grid(\n  mean_height = c(140, 160, 180),\n  mean_age = c(12, 18, 24)\n)\n\nheight_parameters\n\n\n# A tibble: 9 × 2\n  mean_height mean_age\n        &lt;dbl&gt;    &lt;dbl&gt;\n1         140       12\n2         140       18\n3         140       24\n4         160       12\n5         160       18\n6         160       24\n7         180       12\n8         180       18\n9         180       24\n\n\n\n\n6.6.2 Non-fully-crossed designs\nIn non-fully-crossed designs, we remove values that we do not think are possible, or that we don’t care about. This means that we would, for example, exclude the possibility of scarily-tall children in our height_parameters specification above.\nThe best way to achieve this is to start with a fully-crossed design, and then use filter() to remove combinations we think are implausible. For example:\n\n\nCode\nheight_parameters &lt;- expand_grid(\n  mean_height = c(140, 160, 180),\n  mean_age = c(12, 18, 24)\n) |&gt;\n  filter(!(mean_height %in% c(160, 180) & mean_age == 12),\n         !(mean_height == 140 & mean_age &gt; 12))\n\nprint(height_parameters)\n\n\n# A tibble: 5 × 2\n  mean_height mean_age\n        &lt;dbl&gt;    &lt;dbl&gt;\n1         140       12\n2         160       18\n3         160       24\n4         180       18\n5         180       24\n\n\nLet’s go back to our previous example with experiment_parameters:\n\n\nCode\nexperiment_parameters &lt;- expand_grid(\n  n_per_condition = c(30, 50),\n  mean_intervention = c(0.3, 0.5),\n  sd = c(0.5, 1),\n  mean_control = c(0, 0.1)\n)\n\n\nSuppose we want to make a more elaborate experiment, with n_per_condition ranging from 50 to 100 in steps of 10 participants, and with mean_intervention ranging from .1 to .5 in steps of .1 (we can do this using seq(), which creates a sequence of numbers).\n\n\nCode\nexperiment_parameters &lt;- expand_grid(\n  n_per_condition = seq(from = 50, to = 100, by = 10),\n  mean_intervention = seq(from = 0.1, to = 0.5, by = 0.1),\n  sd = c(0.5, 1),\n  mean_control = c(0, 0.1)\n)\n\nexperiment_parameters \n\n\n# A tibble: 120 × 4\n   n_per_condition mean_intervention    sd mean_control\n             &lt;dbl&gt;             &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;\n 1              50               0.1   0.5          0  \n 2              50               0.1   0.5          0.1\n 3              50               0.1   1            0  \n 4              50               0.1   1            0.1\n 5              50               0.2   0.5          0  \n 6              50               0.2   0.5          0.1\n 7              50               0.2   1            0  \n 8              50               0.2   1            0.1\n 9              50               0.3   0.5          0  \n10              50               0.3   0.5          0.1\n# ℹ 110 more rows\n\n\nThat’s 120 combinations! But suppose that we think intervention effect sizes less than 0.3 are only interesting when our sample size is greater than 70 per condition. We could then filter this out as before:\n\n\nCode\nexperiment_parameters &lt;- expand_grid(\n  n_per_condition = seq(from = 50, to = 100, by = 10),\n  mean_intervention = seq(from = 0.1, to = 0.5, by = 0.1),\n  sd = c(0.5, 1),\n  mean_control = c(0, 0.1)\n) |&gt;\n  filter(!(n_per_condition &lt; 70 & mean_intervention &lt; 0.3))\n\nexperiment_parameters |&gt;\n  nrow()\n\n\n[1] 104\n\n\n16 rows have been removed, and we can verify that there are no remaining combinations where n_per_condition is less than 70 when mean_intervention is less than 0.3 by using filter() to keep only those rows that would meet these criteria:\n\n\nCode\nexperiment_parameters |&gt;\n  filter(n_per_condition &lt; 70 & mean_intervention &lt; .3)\n\n\n# A tibble: 0 × 4\n# ℹ 4 variables: n_per_condition &lt;dbl&gt;, mean_intervention &lt;dbl&gt;, sd &lt;dbl&gt;,\n#   mean_control &lt;dbl&gt;\n\n\nEmpty!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Creating simulation experiments</span>"
    ]
  },
  {
    "objectID": "chapters/6_creating_simulation_experiments.html#designs-with-dependencies",
    "href": "chapters/6_creating_simulation_experiments.html#designs-with-dependencies",
    "title": "6  Creating simulation experiments",
    "section": "6.7 Designs with dependencies",
    "text": "6.7 Designs with dependencies\nIn fully-crossed designs, the presence of variables is independent from each other: all levels in a given variable are assumed to be possible across all levels of all other variables.\nIn non-fully-crossed designs, we recognise that not all variable combinations are possible, and remove those impossible/implausible combinations. In other words, there are dependencies between variables: the value of one variable depends on the value of other(s).\nWe can actually take these kinds of dependencies a step further. Instead of just creating a fully-crossed design and filtering out the ones that are impossible or implausible, we can instead define variables in the first place in relation to others.\nLet’s start with a simple example. Imagine we want to simulate all values of the width, height, and area of rectangles when width is between 10 and 20, and height is between 15 and 25. We know that area is dependent on height and width (since area = height * width). If we would use the “simulate fully-crossed and then filter” method, we’d do something like:\n\n\nCode\nrectangle_parameters &lt;- expand_grid(\n  width = 10:20,\n  height = 15:25,\n  area = 150:500 # smallest area = 10*15 = 150, largest area = 20*25 = 500\n)\n\nrectangle_parameters |&gt;\n  nrow()\n\n\n[1] 42471\n\n\nThis gives 42,471 possibilities. Then we could filter, like:\n\n\nCode\nrectangle_parameters &lt;- expand_grid(\n  width = 10:20,\n  height = 15:25,\n  area = 150:500 # smallest area = 10*15 = 150, largest area = 20*25 = 500\n) |&gt;\n  filter(area == width * height)\n\nrectangle_parameters |&gt;\n  nrow()\n\n\n[1] 121\n\n\nWhich gives us the 121 actual possibilities. But why bother simulate all of those unnecessary values in the first place, when we could instead just calculate area directly for each height and width value? This would be much more efficient. And we can do just this, using mutate():\n\n\nCode\nrectangle_parameters &lt;- expand_grid(\n  width = 10:20,\n  height = 15:25\n  ) |&gt;\n  mutate(area = width * height)\n\nrectangle_parameters |&gt;\n  nrow()\n\n\n[1] 121\n\n\nWhen we simulate our data, there will be many cases where these types of “functional relationships” exist: i.e., where one variable is defined based on its relationship with other variable(s). In this case, the most efficient approach is to (i) simulate the other variables, and then (ii) use mutate() to create the variable which is dependent on those other variables.\nHere’s another example. Imagine we’re simulating exam results for students, and we want to vary:\n\nhow many questions the exam has (n_questions: 10, 20, 40, or 60)\nthe average difficulty of questions (“easy” or “hard”)\nthe time limit for the exam (time_limit_minutes: between 10 and 120 minutes\n\nA fully-crossed design would treat these as independent: any time limit could go with any number of questions. But in reality, if we simulate a 10-question quiz with easy questions we might reasonably only want to consider a 10 or 20 minute time limit, whereas a 60-question exam with hard questions might need 60–120 minutes. So the time limit we want to consider in our simulation depends on the number of questions and the difficulty.\nIf we use the “fully-crossed then filter” method, we might do:\n\n\nCode\nexam_parameters &lt;- expand_grid(\n  n_questions = c(10, 20, 40, 60),\n  difficulty = c(\"easy\", \"hard\"),\n  time_limit_minutes = seq(10, 120, by = 10)\n)\n\nexam_parameters |&gt; nrow()\n\n\n[1] 96\n\n\nThis again gives us many combinations we do not care about. We could filter() them:\n\n\nCode\nexam_parameters &lt;- expand_grid(\n  n_questions = c(10, 20, 40, 60),\n  difficulty = c(\"easy\", \"hard\"),\n  time_limit_minutes = seq(10, 120, by = 5)\n) |&gt;\n  filter(\n    (n_questions == 10 & time_limit_minutes %in% c(10, 20)) |\n    (n_questions == 20 & time_limit_minutes %in% c(20, 30)) |\n    (n_questions == 40 & time_limit_minutes %in% c(40, 60)) |\n    (n_questions == 60 & time_limit_minutes %in% c(60, 120))\n  )\n\nexam_parameters \n\n\n# A tibble: 16 × 3\n   n_questions difficulty time_limit_minutes\n         &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;\n 1          10 easy                       10\n 2          10 easy                       20\n 3          10 hard                       10\n 4          10 hard                       20\n 5          20 easy                       20\n 6          20 easy                       30\n 7          20 hard                       20\n 8          20 hard                       30\n 9          40 easy                       40\n10          40 easy                       60\n11          40 hard                       40\n12          40 hard                       60\n13          60 easy                       60\n14          60 easy                      120\n15          60 hard                       60\n16          60 hard                      120\n\n\nBut this gets messy quickly. Looking above, we only filtered by n_questions, not question_difficulty, and it’s already a little hard to read. Plus, we are generating many rows just to immediately discard them.\nInstead, we can define time_limit_minutes from the start based on n_questions and question_difficulty. Here, we’ll generate n_questions and difficulty, and then derive a plausible time limit with mutate().\nWe could, for example, assume:\n\nEasy questions takes 1 minute on average, and\nHard questions take twice as long as easy questions on average.\n\n\n\nCode\nexam_parameters &lt;- expand_grid(\n  n_questions = c(10, 20, 40, 60),\n  difficulty = c(\"easy\", \"hard\")\n) |&gt;\n  mutate(\n    # baseline: 1 minute per question\n    base_time = n_questions * 1,\n    # hard exams take twice as long per question \n    time_limit_minutes = if_else(difficulty == \"hard\",\n                                round(base_time * 2),\n                                base_time)\n  ) |&gt;\n  select(-base_time)\n\nexam_parameters\n\n\n# A tibble: 8 × 3\n  n_questions difficulty time_limit_minutes\n        &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;\n1          10 easy                       10\n2          10 hard                       20\n3          20 easy                       20\n4          20 hard                       40\n5          40 easy                       40\n6          40 hard                       80\n7          60 easy                       60\n8          60 hard                      120",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Creating simulation experiments</span>"
    ]
  },
  {
    "objectID": "chapters/6_creating_simulation_experiments.html#summary",
    "href": "chapters/6_creating_simulation_experiments.html#summary",
    "title": "6  Creating simulation experiments",
    "section": "6.8 Summary",
    "text": "6.8 Summary\nIn this chapter, we covered how to design simulation experiments by specifying the parameter values we want to vary. Key takeaways:\n\nParameter grids are tibbles where each row represents one combination of simulation parameters. They replace error-prone copy-pasting with a structured, inspectable object.\nexpand_grid() is the workhorse for creating parameter grids. You supply each parameter’s desired values once, and it returns all combinations automatically.\nSanity checking your grid with nrow() and distinct() helps catch mistakes early.\nFully-crossed designs (the expand_grid() default) include every combination of every parameter level. These are simple and interpretable, but can become large quickly and may include implausible combinations.\nNon-fully-crossed designs start fully crossed and then use filter() to remove implausible or uninteresting combinations.\nDesigns with dependencies use mutate() to derive one parameter from others (e.g., area = width * height), avoiding the need to enumerate and then filter impossible values. This is both more efficient and more readable than the fully-crossed-then-filter approach.\n\nIn the next chapter, we will learn how to run simulations across these parameter grids efficiently using the map() family of functions.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Creating simulation experiments</span>"
    ]
  },
  {
    "objectID": "chapters/6_creating_simulation_experiments.html#exercises",
    "href": "chapters/6_creating_simulation_experiments.html#exercises",
    "title": "6  Creating simulation experiments",
    "section": "6.9 Exercises",
    "text": "6.9 Exercises\n\n6.9.1 Exercise 1: Basic parameter grid\nUse expand_grid() to create a parameter grid for a simulation where:\n\nn_per_condition is 20, 50, or 100\nmean_intervention is 0.2 or 0.8\nsd is 1\ncorrelation_between_conditions is 0.3 or 0.5.\n\nHow many rows does the resulting grid have? Verify using nrow() and distinct().\n\n\n6.9.2 Exercise 2: Filtering implausible combinations\nStarting from the grid below, use filter() to remove any rows where n_per_condition is less than 50 and mean_intervention is less than 0.3. How many rows remain?\n\n\nCode\nexpand_grid(\n  n_per_condition = c(20, 50, 100, 200),\n  mean_intervention = c(0.1, 0.3, 0.5),\n  sd = c(0.5, 1)\n)\n\n\n# A tibble: 24 × 3\n   n_per_condition mean_intervention    sd\n             &lt;dbl&gt;             &lt;dbl&gt; &lt;dbl&gt;\n 1              20               0.1   0.5\n 2              20               0.1   1  \n 3              20               0.3   0.5\n 4              20               0.3   1  \n 5              20               0.5   0.5\n 6              20               0.5   1  \n 7              50               0.1   0.5\n 8              50               0.1   1  \n 9              50               0.3   0.5\n10              50               0.3   1  \n# ℹ 14 more rows\n\n\n\n\n6.9.3 Exercise 3: Dependent parameters with mutate()\nYou are simulating a reading comprehension study. Your parameters are:\n\nn_passages: the number of passages participants read (5, 10, or 20)\npassage_difficulty: “easy” or “hard”\n\nThe total time allowed (time_limit_minutes) depends on the other two parameters: participants get 2 minutes per easy passage and 4 minutes per hard passage. Use expand_grid() and mutate() to create the parameter grid with time_limit_minutes derived from the other columns.\n\n\n6.9.4 Exercise 4: Choosing the right design\nFor each scenario below, determine whether you would use (a) a fully-crossed design, (b) a non-fully-crossed design with filter(), or (c) a design with dependencies using mutate().\n\nYou vary sample size (50, 100, 200) and effect size (0.2, 0.5, 0.8), and all combinations are plausible.\nYou vary the number of predictors (2, 5, 10) and the number of observations (20, 50, 100, 500), but you want to exclude cases where the number of observations is smaller than 10 times the number of predictors.\nYou vary the number of items on a test (10, 20, 40) and want the total test time to always equal 2 minutes per item.\nYou are simulating a clinical trial and vary the dropout rate (5%, 15%, 30%) and treatment effect size (0.3, 0.5, 0.7). All combinations are realistic because patients drop out for many reasons unrelated to efficacy.\nYou are simulating ecological data on bird species counts across habitats. You vary habitat type (forest, wetland, urban) and survey area size (1 km², 5 km², 25 km²), but want to exclude large survey areas for urban habitats because contiguous urban green spaces larger than 5 km² are unrealistic in your study region.\n\n\n\n6.9.5 Exercise 5: Build your own simulation design\nThink of a research question from an area of research you find interesting. Define at least three parameters you would want to vary, create a parameter grid using expand_grid(), and include at least one dependency or filter. Use nrow() and distinct() to sanity-check your grid. Write a brief comment (2–3 sentences) explaining why you chose each parameter value and why you included the dependency or filter.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Creating simulation experiments</span>"
    ]
  },
  {
    "objectID": "chapters/7_mapping.html",
    "href": "chapters/7_mapping.html",
    "title": "7  Mapping over functions",
    "section": "",
    "text": "7.1 Overview of tutorial\nIn the last lesson, you wrote a data-generation function (generate_data()) and an analysis function (analyze()), then called them once with the pipe:\nA simulation study is just doing that many times, and then summarizing the results.\nThe big question for this lesson:\nHow do we repeat the same “generate → analyze” workflow lots of times, without copy–pasting code?\nAnswer: mapping. In the tidyverse, this is usually done with the {purrr} package (and, later, {furrr} for parallel processing).\nBy the end of this chapter, you should be able to:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mapping over functions</span>"
    ]
  },
  {
    "objectID": "chapters/7_mapping.html#overview-of-tutorial",
    "href": "chapters/7_mapping.html#overview-of-tutorial",
    "title": "7  Mapping over functions",
    "section": "",
    "text": "generate one dataset\n\nanalyze it\n\nget a result (e.g., one p-value)\n\n\n\n\n\n\n\nUse map() to repeat a simulation many times.\nUse map_dbl() / map_int() / map_chr() to get clean vectors out.\nUse map_dfr() to bind outputs into a tibble.\nUse map2() and pmap() to run parameter sweeps.\nUnderstand what it means when mapping produces nested data frames (list-columns).\nUse future_map() / future_pmap() (from {furrr}) when you want to parallelize.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mapping over functions</span>"
    ]
  },
  {
    "objectID": "chapters/7_mapping.html#dependencies",
    "href": "chapters/7_mapping.html#dependencies",
    "title": "7  Mapping over functions",
    "section": "7.2 Dependencies",
    "text": "7.2 Dependencies\n\n\nCode\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(furrr)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# set up parallelization (we will use this later in the chapter)\nplan(multisession)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mapping over functions</span>"
    ]
  },
  {
    "objectID": "chapters/7_mapping.html#recap-generate-and-analyze-functions",
    "href": "chapters/7_mapping.html#recap-generate-and-analyze-functions",
    "title": "7  Mapping over functions",
    "section": "7.3 Recap: generate and analyze functions",
    "text": "7.3 Recap: generate and analyze functions\nHere is the same example setup you saw earlier: two conditions (control vs intervention), normally distributed scores, and a two-sample t-test.\n\n\nCode\ngenerate_data &lt;- function(n_per_condition,\n                          mean_control,\n                          mean_intervention,\n                          sd) {\n\n  data_control &lt;-\n    tibble(condition = \"control\",\n           score = rnorm(n = n_per_condition, mean = mean_control, sd = sd))\n\n  data_intervention &lt;-\n    tibble(condition = \"intervention\",\n           score = rnorm(n = n_per_condition, mean = mean_intervention, sd = sd))\n\n  data_combined &lt;- bind_rows(data_control, data_intervention)\n\n  return(data_combined)\n}\n\nanalyze &lt;- function(data) {\n\n  res_t_test &lt;- t.test(formula = score ~ condition,\n                       data = data,\n                       var.equal = TRUE,\n                       alternative = \"two.sided\")\n\n  res &lt;- tibble(p = res_t_test$p.value)\n\n  return(res)\n}",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mapping over functions</span>"
    ]
  },
  {
    "objectID": "chapters/7_mapping.html#doing-it-once",
    "href": "chapters/7_mapping.html#doing-it-once",
    "title": "7  Mapping over functions",
    "section": "7.4 Doing it once",
    "text": "7.4 Doing it once\n\n\nCode\nresults &lt;-\n  generate_data(n_per_condition = 50,\n                mean_control = 0,\n                mean_intervention = 0.5,\n                sd = 1) |&gt;\n  analyze()\n\nresults\n\n\n# A tibble: 1 × 1\n      p\n  &lt;dbl&gt;\n1 0.191",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mapping over functions</span>"
    ]
  },
  {
    "objectID": "chapters/7_mapping.html#why-copypaste-is-the-enemy",
    "href": "chapters/7_mapping.html#why-copypaste-is-the-enemy",
    "title": "7  Mapping over functions",
    "section": "7.5 Why copy–paste is the enemy",
    "text": "7.5 Why copy–paste is the enemy\nYou already saw the “bad” way: calling the same code block over and over and binding it with bind_rows().\nThis is bad because:\n\nIt’s slow to write and easy to make mistakes.\nIt’s hard to scale from 25 iterations to 10,000 iterations.\nIt’s hard to turn into a proper experiment (varying parameters systematically).\n\nSo: we need a principled way to repeat the same computation. To do this, we can use the “map()” family of functions in R.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mapping over functions</span>"
    ]
  },
  {
    "objectID": "chapters/7_mapping.html#the-core-idea-of-mapping",
    "href": "chapters/7_mapping.html#the-core-idea-of-mapping",
    "title": "7  Mapping over functions",
    "section": "7.6 The core idea of mapping",
    "text": "7.6 The core idea of mapping\n\n7.6.1 map() repeats a function over a vector/list\npurrr::map() takes:\n\na vector/list of inputs (.x)\na function (.f)\nand returns a list of outputs (always a list).\n\nLet’s look at a simple example. Imagine we have a vector of numbers (from 1 to 4) and, for each number, we want to add 10. We can use map() to do this:\n\n\nCode\nnumbers &lt;- c(1, 2, 3, 4)\n\nmap(numbers, ~ .x + 10)\n\n\n[[1]]\n[1] 11\n\n[[2]]\n[1] 12\n\n[[3]]\n[1] 13\n\n[[4]]\n[1] 14\n\n\nNotice the output is a list, even though each element is just a number. This is on purpose: returning a list is the “safest” default, because in real work you might return complicated objects (models, data frames, plots…) and lists handle these diverse objects most effectively, and prevent errors from arising.\n\n\n7.6.2 The anonymous-function shorthand: ~\nWhen you look at the example above, you might be wondering: what is the ~ symbol, and what is it doing? This is referred to as the anonymous-function (lambda) shorthand in {purrr} formula syntax. This lets you define a short function within the map() family of functions. In the case above, our function was “add 10 to the input”, where .x stands for the current input.\nMore concretely, the two lines of code below are equivalent:\nLine 1:\n\n\nCode\nmap(numbers, function(x) x + 10)\n\n\n[[1]]\n[1] 11\n\n[[2]]\n[1] 12\n\n[[3]]\n[1] 13\n\n[[4]]\n[1] 14\n\n\nLine 2:\n\n\nCode\nmap(numbers, ~ .x + 10)\n\n\n[[1]]\n[1] 11\n\n[[2]]\n[1] 12\n\n[[3]]\n[1] 13\n\n[[4]]\n[1] 14\n\n\nYou’ll see the ~ .x style constantly in tidyverse simulation code.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mapping over functions</span>"
    ]
  },
  {
    "objectID": "chapters/7_mapping.html#a-first-simulation-with-map",
    "href": "chapters/7_mapping.html#a-first-simulation-with-map",
    "title": "7  Mapping over functions",
    "section": "7.7 A first simulation with map()",
    "text": "7.7 A first simulation with map()\n\n7.7.1 Map over iterations\nIdeally, simulations need an iteration index, which tells us what “round” of the simulation a given set of outputs are part of. We can often define these iterations simply, like:\n\n\nCode\niterations &lt;- 1:10\niterations\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nNow we map over iterations. For now, the iteration number doesn’t do anything - we just need to supply map() with a set of values to run across.\n\n\nCode\nsimulation_results_list &lt;-\n  map(iterations, ~ {\n    generate_data(n_per_condition = 50,\n                  mean_control = 0,\n                  mean_intervention = 0.5,\n                  sd = 1) |&gt;\n      analyze()\n  })\n\nsimulation_results_list\n\n\n[[1]]\n# A tibble: 1 × 1\n           p\n       &lt;dbl&gt;\n1 0.00000283\n\n[[2]]\n# A tibble: 1 × 1\n        p\n    &lt;dbl&gt;\n1 0.00587\n\n[[3]]\n# A tibble: 1 × 1\n       p\n   &lt;dbl&gt;\n1 0.0288\n\n[[4]]\n# A tibble: 1 × 1\n        p\n    &lt;dbl&gt;\n1 0.00713\n\n[[5]]\n# A tibble: 1 × 1\n        p\n    &lt;dbl&gt;\n1 0.00283\n\n[[6]]\n# A tibble: 1 × 1\n        p\n    &lt;dbl&gt;\n1 0.00808\n\n[[7]]\n# A tibble: 1 × 1\n      p\n  &lt;dbl&gt;\n1 0.198\n\n[[8]]\n# A tibble: 1 × 1\n       p\n   &lt;dbl&gt;\n1 0.0107\n\n[[9]]\n# A tibble: 1 × 1\n         p\n     &lt;dbl&gt;\n1 0.000169\n\n[[10]]\n# A tibble: 1 × 1\n      p\n  &lt;dbl&gt;\n1 0.174\n\n\nYou should see a list of tibbles, each tibble containing one p-value.\nIf we change the number of iterations, then the number of times map() is called will change:\n\n\nCode\niterations &lt;- 1:20\niterations\n\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\n\n\n\nCode\nsimulation_results_list &lt;-\n  map(iterations, ~ {\n    generate_data(n_per_condition = 50,\n                  mean_control = 0,\n                  mean_intervention = 0.5,\n                  sd = 1) |&gt;\n      analyze()\n  })\n\nsimulation_results_list\n\n\n[[1]]\n# A tibble: 1 × 1\n        p\n    &lt;dbl&gt;\n1 0.00246\n\n[[2]]\n# A tibble: 1 × 1\n       p\n   &lt;dbl&gt;\n1 0.0662\n\n[[3]]\n# A tibble: 1 × 1\n      p\n  &lt;dbl&gt;\n1 0.136\n\n[[4]]\n# A tibble: 1 × 1\n       p\n   &lt;dbl&gt;\n1 0.0262\n\n[[5]]\n# A tibble: 1 × 1\n        p\n    &lt;dbl&gt;\n1 0.00659\n\n[[6]]\n# A tibble: 1 × 1\n        p\n    &lt;dbl&gt;\n1 0.00327\n\n[[7]]\n# A tibble: 1 × 1\n       p\n   &lt;dbl&gt;\n1 0.0359\n\n[[8]]\n# A tibble: 1 × 1\n        p\n    &lt;dbl&gt;\n1 0.00175\n\n[[9]]\n# A tibble: 1 × 1\n        p\n    &lt;dbl&gt;\n1 0.00605\n\n[[10]]\n# A tibble: 1 × 1\n       p\n   &lt;dbl&gt;\n1 0.0275\n\n[[11]]\n# A tibble: 1 × 1\n        p\n    &lt;dbl&gt;\n1 0.00370\n\n[[12]]\n# A tibble: 1 × 1\n        p\n    &lt;dbl&gt;\n1 0.00213\n\n[[13]]\n# A tibble: 1 × 1\n        p\n    &lt;dbl&gt;\n1 0.00798\n\n[[14]]\n# A tibble: 1 × 1\n         p\n     &lt;dbl&gt;\n1 0.000337\n\n[[15]]\n# A tibble: 1 × 1\n          p\n      &lt;dbl&gt;\n1 0.0000505\n\n[[16]]\n# A tibble: 1 × 1\n      p\n  &lt;dbl&gt;\n1 0.136\n\n[[17]]\n# A tibble: 1 × 1\n       p\n   &lt;dbl&gt;\n1 0.0297\n\n[[18]]\n# A tibble: 1 × 1\n       p\n   &lt;dbl&gt;\n1 0.0260\n\n[[19]]\n# A tibble: 1 × 1\n       p\n   &lt;dbl&gt;\n1 0.0187\n\n[[20]]\n# A tibble: 1 × 1\n          p\n      &lt;dbl&gt;\n1 0.0000357\n\n\n\n\n7.7.2 Extract one element\nElements from the list output of map() are accessed with [[ ]]. For example, if we want the output of the first iteration, we can do this:\n\n\nCode\nsimulation_results_list[[1]]\n\n\n# A tibble: 1 × 1\n        p\n    &lt;dbl&gt;\n1 0.00246",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mapping over functions</span>"
    ]
  },
  {
    "objectID": "chapters/7_mapping.html#turning-a-list-into-a-tidy-tibble",
    "href": "chapters/7_mapping.html#turning-a-list-into-a-tidy-tibble",
    "title": "7  Mapping over functions",
    "section": "7.8 Turning a list into a tidy tibble",
    "text": "7.8 Turning a list into a tidy tibble\nEven though lists are helpful for preventing errors, we ideally would prefer the output to be in a tidy format, like a dataframe or a tibble, since this is easier to work with later. To get this type of tidier output, we can use the map_dfr() function, which returns dataframes as output instead of lists.\nIf each iteration returns a tibble with the same columns (in our case, a single column called p), map_dfr() will map and then bind the outputs together into a single dataframe.\n\n\nCode\nsimulation_results &lt;-\n  map_dfr(iterations, ~ {\n    generate_data(n_per_condition = 50,\n                  mean_control = 0,\n                  mean_intervention = 0.5,\n                  sd = 1) |&gt;\n      analyze()\n  })\n\n\nsimulation_results\n\n\n# A tibble: 20 × 1\n              p\n          &lt;dbl&gt;\n 1 0.00191     \n 2 0.0260      \n 3 0.00143     \n 4 0.00995     \n 5 0.234       \n 6 0.0911      \n 7 0.0122      \n 8 0.000197    \n 9 0.000533    \n10 0.000443    \n11 0.000172    \n12 0.00106     \n13 0.000571    \n14 0.0000000331\n15 0.0567      \n16 0.156       \n17 0.000000896 \n18 0.00219     \n19 0.0137      \n20 0.0360      \n\n\nAs you can see, now simulation_results is a single tibble, not a list.\n\n7.8.1 Add the iteration number\nWhen we look at the nice output above from map_dfr, we’re missing one important piece of information: the iteration number! As we said above, we ideally want to keep this number so we know what result belongs to what step of the simulation.\nWe can do this by going back to the original map() function with some small additions. Specifically:\n\nWe create a tibble with a single column, called iteration (or whatever we want), and assign this a vector of values from 1 to the number of iterations we want. For example, if we want 20 iterations:\n\n\n\nCode\ntibble(iteration = 1:20)\n\n\n# A tibble: 20 × 1\n   iteration\n       &lt;int&gt;\n 1         1\n 2         2\n 3         3\n 4         4\n 5         5\n 6         6\n 7         7\n 8         8\n 9         9\n10        10\n11        11\n12        12\n13        13\n14        14\n15        15\n16        16\n17        17\n18        18\n19        19\n20        20\n\n\n\nWe mutate a new column into this new tibble, called results (or whatever we want to call it). Then we run map() within the mutate() call using the iteration column and our generate_data() and analyze() functions, like this:\n\n\n\nCode\nsimulation_results &lt;-\n  tibble(iteration = iterations) |&gt;\n  mutate(results = map(iteration, ~ {\n    generate_data(n_per_condition = 50,\n                  mean_control = 0,\n                  mean_intervention = 0.5,\n                  sd = 1) |&gt;\n      analyze()\n  }))\n\nsimulation_results\n\n\n# A tibble: 20 × 2\n   iteration results         \n       &lt;int&gt; &lt;list&gt;          \n 1         1 &lt;tibble [1 × 1]&gt;\n 2         2 &lt;tibble [1 × 1]&gt;\n 3         3 &lt;tibble [1 × 1]&gt;\n 4         4 &lt;tibble [1 × 1]&gt;\n 5         5 &lt;tibble [1 × 1]&gt;\n 6         6 &lt;tibble [1 × 1]&gt;\n 7         7 &lt;tibble [1 × 1]&gt;\n 8         8 &lt;tibble [1 × 1]&gt;\n 9         9 &lt;tibble [1 × 1]&gt;\n10        10 &lt;tibble [1 × 1]&gt;\n11        11 &lt;tibble [1 × 1]&gt;\n12        12 &lt;tibble [1 × 1]&gt;\n13        13 &lt;tibble [1 × 1]&gt;\n14        14 &lt;tibble [1 × 1]&gt;\n15        15 &lt;tibble [1 × 1]&gt;\n16        16 &lt;tibble [1 × 1]&gt;\n17        17 &lt;tibble [1 × 1]&gt;\n18        18 &lt;tibble [1 × 1]&gt;\n19        19 &lt;tibble [1 × 1]&gt;\n20        20 &lt;tibble [1 × 1]&gt;\n\n\n\nLastly, we can see that the results column contains a tibble per row. We simply need to extract the values from this tibble, which we can do using the unnest() function. The whole workflow together, then, looks like:\n\n\n\nCode\nsimulation_results &lt;-\n  tibble(iteration = iterations) |&gt;\n  mutate(results = map(iteration, ~ {\n    generate_data(n_per_condition = 50,\n                  mean_control = 0,\n                  mean_intervention = 0.5,\n                  sd = 1) |&gt;\n      analyze()\n  })) |&gt;\n  unnest(results)\n\nsimulation_results\n\n\n# A tibble: 20 × 2\n   iteration        p\n       &lt;int&gt;    &lt;dbl&gt;\n 1         1 0.194   \n 2         2 0.00205 \n 3         3 0.0755  \n 4         4 0.985   \n 5         5 0.00306 \n 6         6 0.0843  \n 7         7 0.619   \n 8         8 0.000205\n 9         9 0.000712\n10        10 0.00646 \n11        11 0.000353\n12        12 0.00416 \n13        13 0.000860\n14        14 0.203   \n15        15 0.0166  \n16        16 0.00121 \n17        17 0.136   \n18        18 0.0628  \n19        19 0.0308  \n20        20 0.000520\n\n\nThis “make a tibble, map into a list-column, then unnest()” pattern is one of the most fundamental and important ways to do tidy simulation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mapping over functions</span>"
    ]
  },
  {
    "objectID": "chapters/7_mapping.html#another-map_-variant",
    "href": "chapters/7_mapping.html#another-map_-variant",
    "title": "7  Mapping over functions",
    "section": "7.9 Another map_*() variant",
    "text": "7.9 Another map_*() variant\nThere are several different variants of the map() function. You already saw one above: the map_dfr() function, which gave a dataframe as output instead of the list output that map() gives. The other map_*() variants follow a similar pattern - they change the type of output that comes from the map_*() function.\nFor example, imagine you want one number per iteration, rather than a dataframe or list. You might want a numeric vector of p-values. In this case, we could use map_dbl().\n\n7.9.1 map_dbl() returns a numeric vector\nTo use map_dbl(), your function must return one numeric value.\n\n\nCode\niterations &lt;- 1:10\n\np_values &lt;-\n  map_dbl(iterations, ~ {\n    generate_data(n_per_condition = 50,\n                  mean_control = 0,\n                  mean_intervention = 0.5,\n                  sd = 1) |&gt;\n      analyze() |&gt;\n      pull(p)\n  })\n\np_values\n\n\n [1] 0.0045278359 0.0002771313 0.0897464240 0.0006214345 0.0002971459\n [6] 0.0495928582 0.0057253737 0.1107939585 0.0001095016 0.0164376677\n\n\nNow p_values is a plain numeric vector. With this vector, we can do a quick summary, like look at the proportion of p-values less than 0.05:\n\n\nCode\nmean(p_values &lt; .05)\n\n\n[1] 0.8\n\n\nThat number is an estimate of our statistical power under these parameter settings (with potentially lots of error because we only used 10 iterations). Try increasing to iterations &lt;- 1:1000 and rerun.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mapping over functions</span>"
    ]
  },
  {
    "objectID": "chapters/7_mapping.html#null-vs-alternative-using-mapping",
    "href": "chapters/7_mapping.html#null-vs-alternative-using-mapping",
    "title": "7  Mapping over functions",
    "section": "7.10 Null vs alternative using mapping",
    "text": "7.10 Null vs alternative using mapping\nLet’s recreate the “distribution of p-values under null vs alternative” simulations, but now using what we’ve learned from the map() functions.\nLet’s say we want to simulate two conditions:\n\nmean_intervention = 0 (null - there is no true effect)\nmean_intervention = 0.5 (alternative - there is a true effect)\n\nwith one of two sample sizes:\n\nn_per_condition = 20\nn_per_condition = 50\n\nand do 2000 iterations for each combination.\nThis means we have four combinations in total. We could handle this using map(), defining each of those conditions separately, for example like:\n\n\nCode\niterations &lt;- 1:2000\n\nsimulation_results_null_twenty_per_group &lt;-\n  tibble(iteration = iterations) |&gt;\n  mutate(results = map(iteration, ~ {\n    generate_data(n_per_condition = 20,\n                  mean_control = 0,\n                  mean_intervention = 0,\n                  sd = 1) |&gt;\n      analyze()\n  })) |&gt;\n  unnest(results)\n\nsimulation_results_null_fifty_per_group &lt;-\n  tibble(iteration = iterations) |&gt;\n  mutate(results = map(iteration, ~ {\n    generate_data(n_per_condition = 50,\n                  mean_control = 0,\n                  mean_intervention = 0,\n                  sd = 1) |&gt;\n      analyze()\n  })) |&gt;\n  unnest(results)\n\nsimulation_results_alternative_twenty_per_group &lt;-\n  tibble(iteration = iterations) |&gt;\n  mutate(results = map(iteration, ~ {\n    generate_data(n_per_condition = 20,\n                  mean_control = 0,\n                  mean_intervention = 0.5,\n                  sd = 1) |&gt;\n      analyze()\n  })) |&gt;\n  unnest(results)\n\nsimulation_results_alternative_fifty_per_group &lt;-\n  tibble(iteration = iterations) |&gt;\n  mutate(results = map(iteration, ~ {\n    generate_data(n_per_condition = 50,\n                  mean_control = 0,\n                  mean_intervention = 0.5,\n                  sd = 1) |&gt;\n      analyze()\n  })) |&gt;\n  unnest(results)\n\nhead(simulation_results_null_twenty_per_group)\n\n\n# A tibble: 6 × 2\n  iteration     p\n      &lt;int&gt; &lt;dbl&gt;\n1         1 0.709\n2         2 0.483\n3         3 0.102\n4         4 0.766\n5         5 0.332\n6         6 0.962\n\n\nCode\nhead(simulation_results_null_fifty_per_group)\n\n\n# A tibble: 6 × 2\n  iteration      p\n      &lt;int&gt;  &lt;dbl&gt;\n1         1 0.153 \n2         2 0.753 \n3         3 0.914 \n4         4 0.0445\n5         5 0.169 \n6         6 0.446 \n\n\nCode\nhead(simulation_results_alternative_twenty_per_group)\n\n\n# A tibble: 6 × 2\n  iteration       p\n      &lt;int&gt;   &lt;dbl&gt;\n1         1 0.00978\n2         2 0.0160 \n3         3 0.292  \n4         4 0.0816 \n5         5 0.233  \n6         6 0.0456 \n\n\nCode\nhead(simulation_results_alternative_fifty_per_group)\n\n\n# A tibble: 6 × 2\n  iteration         p\n      &lt;int&gt;     &lt;dbl&gt;\n1         1 0.0299   \n2         2 0.0260   \n3         3 0.0000736\n4         4 0.0279   \n5         5 0.0514   \n6         6 0.0207   \n\n\nAs you can see, this involves some impractical copy-pasting, which (as we discussed previously) we want to avoid.\nOur issue is that we have two different parameters (n_per_condition, mean_intervention) that we want to change. To achieve this, we cannot use map(), because map() can only handle one parameter changing at a time. Instead, we need a map() function that can handle two inputs at once. Fortunately, map2() was made for exactly this purpose! There also exist similar extensions for map2_*() as there are for map_*(), for example map2_dfr() and map2_dbl(): these behave identically to their map_*() equivalents, but just take two inputs instead of one.\n\n7.10.1 Use map2() to map multple parameters at once\nLet’s first define our experiment parameters in a dataframe:\n\n\nCode\nset.seed(42) # Ian: we can omit the set seed if they haven't learned it at this point\n\nexperiment_parameters &lt;- tibble(\n  iteration = rep(1:2000, 4), # repeat each value of simulation 4 times\n  n_per_condition = rep(rep(c(20, 50), each = 2000), 2), # repeat each condition 2000 times, and do this twice\n  mean_intervention = rep(rep(c(0, .5, .5, 0), each = 2000)), # repeat each condition value 2000 times, but specify them in different combinations with n_per_condition\n  mean_control = 0, # mean control always = 0\n  sd = 1 # sd always = 1\n)\n\nhead(experiment_parameters, 10)\n\n\n# A tibble: 10 × 5\n   iteration n_per_condition mean_intervention mean_control    sd\n       &lt;int&gt;           &lt;dbl&gt;             &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1         1              20                 0            0     1\n 2         2              20                 0            0     1\n 3         3              20                 0            0     1\n 4         4              20                 0            0     1\n 5         5              20                 0            0     1\n 6         6              20                 0            0     1\n 7         7              20                 0            0     1\n 8         8              20                 0            0     1\n 9         9              20                 0            0     1\n10        10              20                 0            0     1\n\n\nAs you can see, we now have our range of experiments defined: 2000 iterations for each combination.\nWe’ll now run all of this over map2() and store:\n\nthe generated dataset in a list-column (generated_data)\nthe analysis result in a list-column (results)\nthen unnest results\n\nTo do this, we need to specify one input in map2() with the .x argument (just like map()), and another input with the .y argument. We specify the function (generate_data(), in this case) with .f.\n\n\nCode\nsimulation_results &lt;- experiment_parameters |&gt;\n  mutate(\n    generated_data = map2(\n      .x = n_per_condition,\n      .y = mean_intervention,\n      .f = ~ generate_data(\n        n_per_condition   = .x,\n        mean_control      = 0,\n        mean_intervention = .y,\n        sd                = 1\n      )\n    ),\n    results = map(generated_data, analyze)\n  ) |&gt;\n  unnest(results)\n\nsimulation_results\n\n\n# A tibble: 8,000 × 7\n   iteration n_per_condition mean_intervention mean_control    sd generated_data\n       &lt;int&gt;           &lt;dbl&gt;             &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;        \n 1         1              20                 0            0     1 &lt;tibble&gt;      \n 2         2              20                 0            0     1 &lt;tibble&gt;      \n 3         3              20                 0            0     1 &lt;tibble&gt;      \n 4         4              20                 0            0     1 &lt;tibble&gt;      \n 5         5              20                 0            0     1 &lt;tibble&gt;      \n 6         6              20                 0            0     1 &lt;tibble&gt;      \n 7         7              20                 0            0     1 &lt;tibble&gt;      \n 8         8              20                 0            0     1 &lt;tibble&gt;      \n 9         9              20                 0            0     1 &lt;tibble&gt;      \n10        10              20                 0            0     1 &lt;tibble&gt;      \n# ℹ 7,990 more rows\n# ℹ 1 more variable: p &lt;dbl&gt;\n\n\n\n\n7.10.2 Plot distribution of p-values for both conditions\n\n\nCode\nsimulation_results |&gt;\n  ggplot() +\n  aes(x = p) +\n  geom_histogram() +\n  facet_wrap(~mean_intervention)\n\n\n\n\n\n\n\n\n\nThe left panel shows the distribution of p-values from our simulations when mean_intervention = 0; the right panel shows the distribution of p-values from our simulations when mean_intervention = 0.5.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mapping over functions</span>"
    ]
  },
  {
    "objectID": "chapters/7_mapping.html#creating-parameter-grids-more-effectively-expand_grid",
    "href": "chapters/7_mapping.html#creating-parameter-grids-more-effectively-expand_grid",
    "title": "7  Mapping over functions",
    "section": "7.11 Creating parameter grids more effectively: expand_grid()",
    "text": "7.11 Creating parameter grids more effectively: expand_grid()\nIn the example above, you will recall we constructed our parameter grid like this:\n\n\nCode\nexperiment_parameters &lt;- tibble(\n  iteration = rep(1:2000, 4), # repeat each value of simulation 4 times\n  n_per_condition = rep(rep(c(20, 50), each = 2000), 2), # repeat each condition 2000 times, and do this twice\n  mean_intervention = rep(rep(c(0, .5, .5, 0), each = 2000)), # repeat each condition value 2000 times, but specify them in different combinations with n_per_condition\n  mean_control = 0, # mean control always = 0\n  sd = 1 # sd always = 1\n)\n\n\nHowever, this is actually not an ideal approach. First: you can see we are using nested rep() functions, which can be confusing. Second, if we ever want to move to more complex simulations (e.g., with many different parameter combinations) then it would become quite difficult to ensure that all combinations are accurately covered.\nInstead, we can use the handy function expand_grid(). With this function, we give each desired value of each parameter once, and then expand_grid() maps this into a tibble with all combinations of all parameters.\nFor our example above, this looks like:\n\n\nCode\nexperiment_parameters &lt;- expand_grid(\n  iteration = 1:2000,\n  n_per_condition = c(20, 50),\n  mean_intervention = c(0, .5),\n  mean_control = 0,\n  sd = 1\n)\n\n\nexpand_grid() greatly simplifies the process of specifying parameters for our simulations.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mapping over functions</span>"
    ]
  },
  {
    "objectID": "chapters/7_mapping.html#mapping-with-multiple-inputs-pmap",
    "href": "chapters/7_mapping.html#mapping-with-multiple-inputs-pmap",
    "title": "7  Mapping over functions",
    "section": "7.12 Mapping with multiple inputs: pmap()",
    "text": "7.12 Mapping with multiple inputs: pmap()\nAbove, we used map2() to map two inputs into a simulation. But there will be many cases where we want to map many different inputs at once - not just one or two.\nFor example: maybe we want to vary all values of n_per_condition, mean_control, mean_intervention, and sd. We could use the combination of expand_grid() and pmap() to achieve this.\nWith pmap(), we supply all of the parameters as a list to the .l argument, and our function (generate_data()) to the .f argument. We’ll use the same approach as we used previously: run pmap() within a mutate() call, then map() the result onto analyze(), then call unnest() on the results.\n\n\nCode\n# specify the parameters\nexperiment_parameters &lt;- expand_grid(\n  iteration = 1:100, # we'll use 100 iterations to make sure this doesn't take too long\n  n_per_condition = c(20, 50),\n  mean_intervention = c(0, .5),\n  mean_control = c(0, .1),\n  sd = c(.5, 1)\n)\n\n# use pmap() across this\nsimulation_results &lt;- experiment_parameters |&gt;\n  mutate(\n    generated_data = pmap(\n      .l = list(n_per_condition, mean_control, mean_intervention, sd),\n      .f = generate_data),\n    results = map(generated_data, analyze)\n  ) |&gt;\n  unnest(results)\n\nsimulation_results\n\n\n# A tibble: 1,600 × 7\n   iteration n_per_condition mean_intervention mean_control    sd generated_data\n       &lt;int&gt;           &lt;dbl&gt;             &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;        \n 1         1              20               0            0     0.5 &lt;tibble&gt;      \n 2         1              20               0            0     1   &lt;tibble&gt;      \n 3         1              20               0            0.1   0.5 &lt;tibble&gt;      \n 4         1              20               0            0.1   1   &lt;tibble&gt;      \n 5         1              20               0.5          0     0.5 &lt;tibble&gt;      \n 6         1              20               0.5          0     1   &lt;tibble&gt;      \n 7         1              20               0.5          0.1   0.5 &lt;tibble&gt;      \n 8         1              20               0.5          0.1   1   &lt;tibble&gt;      \n 9         1              50               0            0     0.5 &lt;tibble&gt;      \n10         1              50               0            0     1   &lt;tibble&gt;      \n# ℹ 1,590 more rows\n# ℹ 1 more variable: p &lt;dbl&gt;",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mapping over functions</span>"
    ]
  },
  {
    "objectID": "chapters/7_mapping.html#pmap-for-parameter-sweeps",
    "href": "chapters/7_mapping.html#pmap-for-parameter-sweeps",
    "title": "7  Mapping over functions",
    "section": "7.13 pmap() for parameter sweeps",
    "text": "7.13 pmap() for parameter sweeps\nThis is the most common simulation pattern in this course:\n\nMake a parameter grid (expand_grid())\nUse pmap() to run the simulation for each row\nSummarize results grouped by the manipulated parameters\n\nHere’s another example: statistical power as a function of sample size and effect size.\n\n7.13.1 Specify the parameter grid\n\n\nCode\nset.seed(42)\n\nexperiment_parameters &lt;- expand_grid(\n  n_per_condition = c(20, 50, 100),\n  mean_intervention = c(0, 0.2, 0.5),\n  iteration = 1:1000\n) |&gt;\n  mutate(mean_control = 0,\n         sd = 1)\n\nexperiment_parameters\n\n\n# A tibble: 9,000 × 5\n   n_per_condition mean_intervention iteration mean_control    sd\n             &lt;dbl&gt;             &lt;dbl&gt;     &lt;int&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1              20                 0         1            0     1\n 2              20                 0         2            0     1\n 3              20                 0         3            0     1\n 4              20                 0         4            0     1\n 5              20                 0         5            0     1\n 6              20                 0         6            0     1\n 7              20                 0         7            0     1\n 8              20                 0         8            0     1\n 9              20                 0         9            0     1\n10              20                 0        10            0     1\n# ℹ 8,990 more rows\n\n\n\n\n7.13.2 Run the simulations\n\n\nCode\nsimulation_results &lt;- experiment_parameters |&gt;\n  mutate(\n    generated_data = pmap(\n      .l = list(n_per_condition, mean_control, mean_intervention, sd), \n      .f = generate_data),\n    results = map(generated_data, analyze)\n  ) |&gt;\n  unnest(results)\n\nsimulation_results\n\n\n# A tibble: 9,000 × 7\n   n_per_condition mean_intervention iteration mean_control    sd generated_data\n             &lt;dbl&gt;             &lt;dbl&gt;     &lt;int&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;        \n 1              20                 0         1            0     1 &lt;tibble&gt;      \n 2              20                 0         2            0     1 &lt;tibble&gt;      \n 3              20                 0         3            0     1 &lt;tibble&gt;      \n 4              20                 0         4            0     1 &lt;tibble&gt;      \n 5              20                 0         5            0     1 &lt;tibble&gt;      \n 6              20                 0         6            0     1 &lt;tibble&gt;      \n 7              20                 0         7            0     1 &lt;tibble&gt;      \n 8              20                 0         8            0     1 &lt;tibble&gt;      \n 9              20                 0         9            0     1 &lt;tibble&gt;      \n10              20                 0        10            0     1 &lt;tibble&gt;      \n# ℹ 8,990 more rows\n# ℹ 1 more variable: p &lt;dbl&gt;\n\n\n\n\n7.13.3 Get results per parameter combination\nFinally, we can group_by() the different parameters we manipulated (n_per_condition, mean_intervention) and summarise() the proportion of significant p-values in each case:\n\n\nCode\npower_summary &lt;- simulation_results |&gt;\n  mutate(significant = p &lt; .05) |&gt;\n  group_by(n_per_condition, mean_intervention) |&gt;\n  summarize(power = mean(significant), .groups = \"drop\")\n\npower_summary \n\n\n# A tibble: 9 × 3\n  n_per_condition mean_intervention power\n            &lt;dbl&gt;             &lt;dbl&gt; &lt;dbl&gt;\n1              20               0   0.044\n2              20               0.2 0.077\n3              20               0.5 0.356\n4              50               0   0.042\n5              50               0.2 0.182\n6              50               0.5 0.671\n7             100               0   0.051\n8             100               0.2 0.302\n9             100               0.5 0.927",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mapping over functions</span>"
    ]
  },
  {
    "objectID": "chapters/7_mapping.html#nested-data-frames-list-columns-are-useful",
    "href": "chapters/7_mapping.html#nested-data-frames-list-columns-are-useful",
    "title": "7  Mapping over functions",
    "section": "7.14 Nested data frames (list-columns) are useful",
    "text": "7.14 Nested data frames (list-columns) are useful\nWhen you map a function that returns a data frame, you get a list-column where each cell contains a data frame.\nThis is extremely useful because it keeps your workflow tidy:\n\nOne row = one simulation condition (or one iteration)\nOne list-column = the simulated dataset or the model object\nAnother list-column = the results\n\nYou already saw this with generated_data and results. The only new skill is:\n\nknowing that list-columns exist\nbeing comfortable with unnest() when you want to “flatten” results",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mapping over functions</span>"
    ]
  },
  {
    "objectID": "chapters/7_mapping.html#parallel-mapping-with-furrr",
    "href": "chapters/7_mapping.html#parallel-mapping-with-furrr",
    "title": "7  Mapping over functions",
    "section": "7.15 Parallel mapping with {furrr}",
    "text": "7.15 Parallel mapping with {furrr}\nWith the map() family of functions we have discussed in this chapter, simulations are run in sequence. For example, if we are running 100 iterations, this means that iteration 1 is run first; then iteration 2 is run; then iteration 3; etc.\nWhen you’re doing hundreds of thousands, millions, (or billions!) of iterations, running simulations in sequence can take a long time. In such cases, we ideally would run many of the simulations in parallel to speed things up (e.g., running the first 5 iterations at once; then the next 5; etc.). This is called parallelisation, and it can dramatically speed up simulations.\nThe package for running parallelised simulations is called {furrr}. {furrr} has versions of the map() functions that mirror those in {purrr}:\n\nfuture_map() is the parallelised version of map()\nfuture_map_dfr() is the parallelised version of map_dfr()\nfuture_pmap() is the parallelised version of pmap()\n\nHere is the same “simulate grid” idea, but using future_pmap().\n\n\nCode\nsimulation_parallel &lt;- experiment_parameters |&gt;\n  mutate(\n    generated_data = future_pmap(\n      .l = list(n_per_condition, mean_control, mean_intervention, sd),\n      .f = generate_data,\n      .progress = TRUE,\n      .options = furrr_options(seed = TRUE)\n    ),\n    results = future_map(\n      generated_data,\n      analyze,\n      .progress = TRUE,\n      .options = furrr_options(seed = TRUE)\n    )\n  )\n\nsimulation_parallel |&gt;\n  unnest(results) |&gt;\n  group_by(mean_intervention) |&gt;\n  mutate(significant = p &lt; .05) |&gt;\n  summarise(proportion_significant = mean(significant)) \n\n\n# A tibble: 3 × 2\n  mean_intervention proportion_significant\n              &lt;dbl&gt;                  &lt;dbl&gt;\n1               0                   0.0557\n2               0.2                 0.188 \n3               0.5                 0.658",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mapping over functions</span>"
    ]
  },
  {
    "objectID": "chapters/7_mapping.html#summary",
    "href": "chapters/7_mapping.html#summary",
    "title": "7  Mapping over functions",
    "section": "7.16 Summary",
    "text": "7.16 Summary\nYou now have the core “engine” for simulation in tidyverse form:\n\nexpand_grid() creates the experiment\nmap() / map2() / pmap() repeats the simulation\nlist-columns store complex outputs cleanly\nunnest() flattens results for summarizing and plotting\nmap_dbl() and map_dfr() help you get outputs in the shape you want\nfuture_*() variants can make big simulations faster",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mapping over functions</span>"
    ]
  },
  {
    "objectID": "chapters/7_mapping.html#exercises",
    "href": "chapters/7_mapping.html#exercises",
    "title": "7  Mapping over functions",
    "section": "7.17 Exercises",
    "text": "7.17 Exercises\n\n7.17.1 Warm-up: map a simple function\n\nCreate iterations &lt;- 1:20.\nUse map() to compute sqrt() of each iteration.\nWhat type of object does map() return?\n\n\n\n7.17.2 Map a simulation 200 times\n\nSet iterations &lt;- 1:200.\nUse map_dfr() to run generate_data(…) |&gt; analyze() 200 times.\nCompute the proportion of p &lt; .05.\n\n\n\n7.17.3 Extract a numeric vector of p-values\n\nRepeat the last exercise, but use map_dbl() to return a numeric vector.\nPlot a histogram of the p-values.\n\n\n\n7.17.4 Make a tiny experiment with expand_grid() + pmap()\n\nUse expand_grid() to create an experiment with:\n\nmean_intervention = c(0.3, 0.7)\niteration = 1:1000\nn_per_condition = c(25, 100)\nmean_control = 0\nsd = c(0.5, 1)\n\nUse pmap() + map() to generate and analyze.\nPlot the distribution of p-values and estimate power for each condition (one number per effect size).\n\n\n\n7.17.5 (Optional) Parallelize\n\nReplace your pmap() call with future_pmap().\nDoes it run faster?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mapping over functions</span>"
    ]
  },
  {
    "objectID": "chapters/8_summarizing_results.html",
    "href": "chapters/8_summarizing_results.html",
    "title": "8  Summarizing results",
    "section": "",
    "text": "TODO",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Summarizing results</span>"
    ]
  },
  {
    "objectID": "chapters/9_planning_and_reporting.html",
    "href": "chapters/9_planning_and_reporting.html",
    "title": "9  Planning and reporting",
    "section": "",
    "text": "TODO",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Planning and reporting</span>"
    ]
  },
  {
    "objectID": "chapters/10_simulation_template.html",
    "href": "chapters/10_simulation_template.html",
    "title": "10  Simulation template",
    "section": "",
    "text": "TODO",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simulation template</span>"
    ]
  },
  {
    "objectID": "chapters/violating_assumptions.html",
    "href": "chapters/violating_assumptions.html",
    "title": "11  Assessing the impact of violating the assumption of normality",
    "section": "",
    "text": "11.1 Dependencies\nCode\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(purrr) \nlibrary(furrr)\nlibrary(forcats)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(sn)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(janitor)\nlibrary(effsize)\n\n# set up parallelization\nplan(multisession)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assessing the impact of violating the assumption of normality</span>"
    ]
  },
  {
    "objectID": "chapters/violating_assumptions.html#skew-normal-distributions",
    "href": "chapters/violating_assumptions.html#skew-normal-distributions",
    "title": "11  Assessing the impact of violating the assumption of normality",
    "section": "11.2 Skew-normal distributions",
    "text": "11.2 Skew-normal distributions\nIn order to violate normality, we will need to use a different non-normal distribution. For this example we’ll use the skew-normal distribution. Where the normal distribution of defined by two parameters, mean and standard deviation, other distributions are controlled by other parameters with different naming conventions, and often more than two parameters.\nThe skew-normal distribution is defined by:\n\n‘location’, akin to mean, is controlled via the parameter \\(\\xi\\), i.e., ‘xi’ in sn(). In fact, mean is referred to as ‘location’ in many distributions.\n‘scale’, akin to SD, is controlled via the parameter \\(\\omega\\), i.e., ‘omega’ in sn(). Likewise, ‘scale’ is a common way of referring to measures of dispersion like SD.\n‘slant’/‘skew’, is controlled via parameter \\(\\alpha\\), i.e., ‘alpha’ in sn().\n\nNote that when alpha = 0, skew-normal data is the same as normal data:\n\n\nCode\nmu    &lt;- 0    # population mean \nsigma &lt;- 1    # population standard deviation\nskew  &lt;- 12   # skewness parameter\n\n# convert m, sd, and skewness into skew-normal parameters scale, location, and alpha (skew)\ndelta_val &lt;- skew / sqrt(1 + skew^2)  # delta is an intermediate value\nscale_val &lt;- sigma / sqrt(1 - 2 * delta_val^2 / pi)  \nlocation_val &lt;- mu - scale_val * delta_val * sqrt(2 / pi) \n\n# generate data\nset.seed(42)  \n\nsample_size &lt;- 1000000\n\ndata_combined &lt;- tibble(\n  normal = rnorm(n = sample_size, \n                 mean = mu, \n                 sd = sigma),\n  skewnormal = rsn(n = sample_size, \n                   xi = location_val, \n                   omega = scale_val, \n                   alpha = skew)\n)\n\n# table\ndata_combined |&gt;\n  summarize(mean_normal = mean(normal),\n            mean_skewnormal = mean(skewnormal),\n            sd_normal = sd(normal),\n            sd_skewnormal = sd(skewnormal)) |&gt;\n  mutate_if(is.numeric, round_half_up, digits = 2) |&gt;\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\nmean_normal\nmean_skewnormal\nsd_normal\nsd_skewnormal\n\n\n\n\n0\n0\n1\n1\n\n\n\n\n\nCode\n# plot\nggplot(data_combined) +\n  geom_density(aes(x = normal)) +\n  geom_density(aes(x = skewnormal)) +\n  labs(x = \"Scores\",\n       y = \"Density\",\n       title = \"Normal vs. Skew-Normal Distributions with equal mean and SD\") +\n  scale_x_continuous(breaks = breaks_pretty(n = 9), limits = c(-4, 4)) +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\n\n11.2.1 Math for converting normal parameters + skew to skew-normal parameters\nYou don’t to understand the following math, but here is is for nerds.\nThe parameters that define normal data, i.e., population mean (\\(\\mu\\)) and population SD (\\(\\sigma\\)) plus a skew parameter (\\(\\alpha\\)) can be converted to the parameters that directly define a skew-normal distribution, location (\\(\\xi\\)), scale (\\(\\omega\\)), and skew (\\(\\alpha\\)).\nThe intermediate variable \\(\\delta\\) is calculated from skew (\\(\\alpha\\)):\n\\[\n\\delta = \\frac{\\alpha}{\\sqrt{\\,1 + \\alpha^2\\,}}\n\\]\nScale (\\(\\omega\\)) is calculated from \\(\\delta\\) and skew (\\(\\alpha\\)):\n\\[\n\\omega\n= \\frac{\\sigma}{\\sqrt{\\,1 \\;-\\; \\frac{2\\,\\delta^2}{\\pi}\\,}}\n\\]\nLocation (\\(\\xi\\)) is calculated from population mean (\\(\\mu\\)), scale (\\(\\omega\\)), and skew (\\(\\alpha\\)).\n\\[\n\\xi\n= \\mu\n\\;-\\;\n\\omega \\,\\delta \\,\\sqrt{\\frac{2}{\\pi}}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assessing the impact of violating the assumption of normality</span>"
    ]
  },
  {
    "objectID": "chapters/violating_assumptions.html#impact-of-non-normality-on-the-t-tests-false-positive-rate",
    "href": "chapters/violating_assumptions.html#impact-of-non-normality-on-the-t-tests-false-positive-rate",
    "title": "11  Assessing the impact of violating the assumption of normality",
    "section": "11.3 1. Impact of non-normality on the t-test’s false-positive rate",
    "text": "11.3 1. Impact of non-normality on the t-test’s false-positive rate\n\n11.3.1 Data generation function\n\n\nCode\n# define data generating function ----\ngenerate_data &lt;- function(n_control,\n                          n_intervention,\n                          location_control, # location, akin to mean\n                          location_intervention,\n                          scale_control, # scale, akin to SD\n                          scale_intervention,\n                          skew_control, # slant/skew. When 0, produces normal/gaussian data\n                          skew_intervention) {\n  \n  data_control &lt;- \n    tibble(condition = \"control\",\n           score = rsn(n = n_control, \n                       xi = location_control, # location, akin to mean\n                       omega = scale_control, # scale, akin to SD\n                       alpha = skew_control)) # slant/skew. When 0, produces normal/gaussian data\n  \n  data_intervention &lt;- \n    tibble(condition = \"intervention\",\n           score = rsn(n = n_intervention, \n                       xi = location_intervention, # location, akin to mean\n                       omega = scale_intervention, # scale, akin to SD\n                       alpha = skew_intervention)) # slant/skew. When 0, produces normal/gaussian data\n  \n  data &lt;- \n    bind_rows(data_control,\n              data_intervention) |&gt;\n    # order the levels of condition correctly so that the direction of cohens d is correct\n    mutate(condition = fct_relevel(condition, \"intervention\", \"control\"))\n    \n  return(data)\n}\n\n\n\n\n11.3.2 Analysis function\n\n\nCode\n# define data analysis function ----\nanalyze &lt;- function(data) {\n\n  res_t_test &lt;- t.test(formula = score ~ condition, \n                       data = data,\n                       var.equal = TRUE,\n                       alternative = \"two.sided\")\n  \n  res_cohens_d &lt;- cohen.d(formula = score ~ condition,\n                          data = data,\n                          pooled = TRUE)\n  \n  res &lt;- tibble(p = res_t_test$p.value, \n                cohens_d = res_cohens_d$estimate)\n\n  return(res)\n}\n\n\n\n\n11.3.3 Simulation parameters\n\n11.3.3.1 Exercise - setting up the experiment correctly\nWe want to construct a simulation where the skew parameter is either 0 or 12 AND for the skew parameter to be the same in both conditions: both 0 or both 12.\nThis presents a problem for our expand_grid call. Why? That is, what is wrong with the below code? How would we fix it?\nThis is an important lesson in thinking carefully about how you set up a simulation and whether it does what you intend it to.\n\n\nCode\n# define experiment parameters ----\nexperiment_parameters &lt;- expand_grid(\n  n_control = 100,\n  n_intervention = 100,\n  mu_control = 0,\n  mu_intervention = 0, \n  sigma_control = 1,\n  sigma_intervention = 1,\n  skew_control = c(0, 12),\n  skew_intervention = c(0, 12),\n  iteration = 1:1000 \n) \n\n\n\n\n11.3.3.2 Solution\n\n\nCode\n# define experiment parameters ----\nexperiment_parameters &lt;- expand_grid(\n  n_control = 100,\n  n_intervention = 100,\n  mu_control = 0,\n  mu_intervention = 0, \n  sigma_control = 1,\n  sigma_intervention = 1,\n  skew_control = c(0, 12),\n  iteration = 1:1000 \n) |&gt;\n  mutate(skew_intervention = skew_control) \n\n\n# do some sanity checks on the experiment's grid\nexperiment_parameters |&gt;\n  count(skew_control, skew_intervention) |&gt;\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\nskew_control\nskew_intervention\nn\n\n\n\n\n0\n0\n1000\n\n\n12\n12\n1000\n\n\n\n\n\n\n\n11.3.3.3 Experiment parameters\nI have added the normal-to-skew-normal parameter conversions and the convenience variables.\n\n\nCode\n# define experiment parameters ----\nexperiment_parameters &lt;- expand_grid(\n  n_control = 100,\n  n_intervention = 100,\n  mu_control = 0,\n  mu_intervention = 0, \n  sigma_control = 1,\n  sigma_intervention = 1,\n  skew_control = c(0, 12),\n  iteration = 1:1000 \n) |&gt;\n  mutate(skew_intervention = skew_control) |&gt;\n  \n  # make an intuitive label for the conditions\n  mutate(distribution = case_when(skew_intervention == 0 ~ \"Normal data\",\n                                  skew_intervention == 12 ~ \"Skew-Normal data\")) |&gt;\n  \n  # calculate skew-normal parameters\n  # don't worry about the math, it's not important to understand\n  mutate(delta_control = skew_control / sqrt(1 + skew_control^2),\n         delta_intervention = skew_intervention / sqrt(1 + skew_intervention^2),\n         scale_control = sigma_control / sqrt(1 - 2 * delta_control^2 / pi),\n         scale_intervention = sigma_intervention / sqrt(1 - 2 * delta_intervention^2 / pi),\n         location_control = mu_control - scale_control * delta_control * sqrt(2 / pi),\n         location_intervention = mu_intervention - scale_intervention * delta_intervention * sqrt(2 / pi)) \n\n\n\n\n\n11.3.4 Run simulation\n\n\nCode\n# set the seed ----\nset.seed(42)\n\n# run simulation ----\nsimulation &lt;- \n  # using the experiment parameters\n  experiment_parameters |&gt;\n  \n  # generate data using the data generating function and the parameters relevant to data generation\n  mutate(generated_data = pmap(list(n_control,\n                                    n_intervention,\n                                    location_control,\n                                    location_intervention,\n                                    scale_control,\n                                    scale_intervention,\n                                    skew_control,\n                                    skew_intervention),\n                               generate_data)) |&gt;\n  \n  # apply the analysis function to the generated data using the parameters relevant to analysis\n  mutate(analysis_results = pmap(list(generated_data),\n                                 analyze))\n\n\n\n\n11.3.5 Summarize results\nSummarize the proportion of significant p-values, i.e., the false positive rate, since population difference in locations in zero.\nQuick check on your own learning: When would this proportion not represent the false positive rate?\n\n\nCode\n# summarise simulation results over the iterations ----\nsimulation_summary &lt;- simulation |&gt;\n  unnest(analysis_results) |&gt;\n  group_by(distribution) |&gt;\n  summarize(proportion_significant = mean(p &lt; .05)) \n\n# print table\nsimulation_summary |&gt;\n  mutate_if(is.numeric, janitor::round_half_up, digits = 2) |&gt; # note: only ever do rounding at the point of printing results!\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\ndistribution\nproportion_significant\n\n\n\n\nNormal data\n0.04\n\n\nSkew-Normal data\n0.04\n\n\n\n\n\n\nWhat does this tell us?\nWhat does this not tell us?\nIf we want to understand why violating the assumption of normality might be bad, what else can we do?",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assessing the impact of violating the assumption of normality</span>"
    ]
  },
  {
    "objectID": "chapters/violating_assumptions.html#impact-of-non-normality-on-the-t-tests-false-positive-and-true-positive-power-rates",
    "href": "chapters/violating_assumptions.html#impact-of-non-normality-on-the-t-tests-false-positive-and-true-positive-power-rates",
    "title": "11  Assessing the impact of violating the assumption of normality",
    "section": "11.4 2. Impact of non-normality on the t-test’s false-positive and true-positive (power) rates",
    "text": "11.4 2. Impact of non-normality on the t-test’s false-positive and true-positive (power) rates\n\n11.4.1 Simulation parameters\n\n11.4.1.1 Exercise\nHow would we modify the parameters to study not only the false-positive rate but also the true positive rate?\n\n\nCode\n# define experiment parameters ----\nexperiment_parameters &lt;- expand_grid(\n  n_control = 100,\n  n_intervention = 100,\n  mu_control = 0,\n  mu_intervention = 0,\n  sigma_control = 1,\n  sigma_intervention = 1,\n  skew_control = c(0, 12),\n  iteration = 1:1000 \n) |&gt;\n  mutate(skew_intervention = skew_control) |&gt;\n  \n  # make an intuitive label for the conditions\n  mutate(distribution = case_when(skew_intervention == 0 ~ \"Normal data\",\n                                  skew_intervention == 12 ~ \"Skew-Normal data\"),\n         population_effect_size = paste(\"Cohen's d =\", mu_intervention)) |&gt;\n  \n  # calculate skew-normal parameters\n  # don't worry about the math, it's not important to understand\n  mutate(delta_control = skew_control / sqrt(1 + skew_control^2),\n         delta_intervention = skew_intervention / sqrt(1 + skew_intervention^2),\n         scale_control = sigma_control / sqrt(1 - 2 * delta_control^2 / pi),\n         scale_intervention = sigma_intervention / sqrt(1 - 2 * delta_intervention^2 / pi),\n         location_control = mu_control - scale_control * delta_control * sqrt(2 / pi),\n         location_intervention = mu_intervention - scale_intervention * delta_intervention * sqrt(2 / pi)) \n\n\n\n\n11.4.1.2 Solution\n\n\nCode\n# define experiment parameters ----\nexperiment_parameters &lt;- expand_grid(\n  n_control = 100,\n  n_intervention = 100,\n  mu_control = 0,\n  mu_intervention = c(0, 0.5), # multiple location values\n  sigma_control = 1,\n  sigma_intervention = 1,\n  skew_control = c(0, 12),\n  iteration = 1:1000 \n) |&gt;\n  mutate(skew_intervention = skew_control) |&gt;\n  \n  # make an intuitive label for the conditions\n  mutate(distribution = case_when(skew_intervention == 0 ~ \"Normal data\",\n                                  skew_intervention == 12 ~ \"Skew-Normal data\"),\n         population_effect_size = paste(\"Cohen's d =\", mu_intervention)) |&gt;\n  \n  # calculate skew-normal parameters\n  # don't worry about the math, it's not important to understand\n  mutate(delta_control = skew_control / sqrt(1 + skew_control^2),\n         delta_intervention = skew_intervention / sqrt(1 + skew_intervention^2),\n         scale_control = sigma_control / sqrt(1 - 2 * delta_control^2 / pi),\n         scale_intervention = sigma_intervention / sqrt(1 - 2 * delta_intervention^2 / pi),\n         location_control = mu_control - scale_control * delta_control * sqrt(2 / pi),\n         location_intervention = mu_intervention - scale_intervention * delta_intervention * sqrt(2 / pi)) \n\n\n\n\n\n11.4.2 Run simulation\nExactly the same as last time, only parameters differ.\n\n\nCode\n# set the seed ----\nset.seed(42)\n\n# run simulation ----\nsimulation &lt;- \n  # using the experiment parameters\n  experiment_parameters |&gt;\n  \n  # generate data using the data generating function and the parameters relevant to data generation\n  mutate(generated_data = pmap(list(n_control,\n                                    n_intervention,\n                                    location_control,\n                                    location_intervention,\n                                    scale_control,\n                                    scale_intervention,\n                                    skew_control,\n                                    skew_intervention),\n                               generate_data)) |&gt;\n  \n  # apply the analysis function to the generated data using the parameters relevant to analysis\n  mutate(analysis_results = pmap(list(generated_data),\n                                 analyze))\n\n\n\n\n11.4.3 Summarize results\nSummarize the proportion of significant p-values, i.e., the false positive rate, since population difference in locations in zero.\n\n11.4.3.1 Exercise\nHow do we need to modify the summarization compared to last time? Modify the below code.\n\n\nCode\n# summarise simulation results over the iterations\nsimulation_summary &lt;- simulation |&gt;\n  unnest(analysis_results) |&gt;\n  group_by(distribution) |&gt;\n  summarize(proportion_significant = mean(p &lt; .05)) \n\nsimulation_summary |&gt;\n  mutate_if(is.numeric, janitor::round_half_up, digits = 2) |&gt; # note: only ever do rounding at the point of printing results!\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\ndistribution\nproportion_significant\n\n\n\n\nNormal data\n0.49\n\n\nSkew-Normal data\n0.49\n\n\n\n\n\n\n\n11.4.3.2 Solution\n\n\nShow solution\n# summarise simulation results over the iterations ----\nsimulation_summary &lt;- simulation |&gt;\n  unnest(analysis_results) |&gt;\n  # group_by ALL the manipulated factors\n  group_by(distribution, population_effect_size) |&gt; \n  summarize(proportion_significant = mean(p &lt; .05)) \n\nsimulation_summary |&gt;\n  mutate_if(is.numeric, janitor::round_half_up, digits = 2) |&gt; # note: only ever do rounding at the point of printing results!\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\ndistribution\npopulation_effect_size\nproportion_significant\n\n\n\n\nNormal data\nCohen's d = 0\n0.04\n\n\nNormal data\nCohen's d = 0.5\n0.94\n\n\nSkew-Normal data\nCohen's d = 0\n0.04\n\n\nSkew-Normal data\nCohen's d = 0.5\n0.93\n\n\n\n\n\n\nWhat does this tell us?\nWhat does this not tell us?\nIf we want to understand why violating the assumption of normality might be bad, what else can we do?",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assessing the impact of violating the assumption of normality</span>"
    ]
  },
  {
    "objectID": "chapters/violating_assumptions.html#impact-of-non-normality-on-the-t-tests-false-positive-and-true-positive-power-rates-and-estimates-of-cohenss-d",
    "href": "chapters/violating_assumptions.html#impact-of-non-normality-on-the-t-tests-false-positive-and-true-positive-power-rates-and-estimates-of-cohenss-d",
    "title": "11  Assessing the impact of violating the assumption of normality",
    "section": "11.5 3. Impact of non-normality on the t-test’s false-positive and true-positive (power) rates and estimates of Cohens’s d",
    "text": "11.5 3. Impact of non-normality on the t-test’s false-positive and true-positive (power) rates and estimates of Cohens’s d\nAs well as the false-positive and false-negative rate of the t-test’s p-values, we could also examine the estimates of standardized effect size.\n\n11.5.1 Summarize results\nWe can summarize the results of the previous simulation differently, by adding Cohen’s d too.\n\n\nCode\n# summarise simulation results over the iterations ----\nsimulation_summary &lt;- simulation |&gt;\n  unnest(analysis_results) |&gt;\n  # group by ALL the manipulated factors\n  group_by(distribution, \n           population_effect_size) |&gt;\n  summarize(proportion_significant = mean(p &lt; .05),\n            cohens_d = mean(cohens_d)) |&gt;\n  select(distribution, \n         population_effect_size,\n         cohens_d,\n         proportion_significant)\n\n# table\nsimulation_summary |&gt;\n  mutate_if(is.numeric, janitor::round_half_up, digits = 2) |&gt;\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\ndistribution\npopulation_effect_size\ncohens_d\nproportion_significant\n\n\n\n\nNormal data\nCohen's d = 0\n0.0\n0.04\n\n\nNormal data\nCohen's d = 0.5\n0.5\n0.94\n\n\nSkew-Normal data\nCohen's d = 0\n0.0\n0.04\n\n\nSkew-Normal data\nCohen's d = 0.5\n0.5\n0.93\n\n\n\n\n\n\nThe false-positive and false-negative rates of the t-test’s p-value are maintained when data is skewed.\nThe effect size recovery is maintained when data is skewed.\n\nSo, why do we care about the assumption of normality for a t-test if violating it in these ways doesn’t substantially change much?",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assessing the impact of violating the assumption of normality</span>"
    ]
  },
  {
    "objectID": "chapters/violating_assumptions.html#at-home-exercises",
    "href": "chapters/violating_assumptions.html#at-home-exercises",
    "title": "11  Assessing the impact of violating the assumption of normality",
    "section": "11.6 At-home exercises",
    "text": "11.6 At-home exercises\n\n11.6.1 Collate the final simulation above into a single code chunk with all the pieces to run the full simulation\n\n\nCode\n# remove all objects from environment to ensure you're starting from a blank page\nrm(list = ls())\n\n# paste necessary code in here\n\n\n\n\n11.6.2 Elaborate the simulation\nRight now the simulation only examines a fixed sample size. Perhaps the supposed negative impact of violating normality would be seen at other sample sizes? Vary this or indeed other things to understand when violating normality matters. If you wanted to get more complex, examine the impact of using another distribution other than skew normal that still has knowable population means and SDs, such as a bounded uniform distribution.\n\n\n11.6.3 Develop a simulation to examine the impact of violating other statistical assumptions\nThe Student’s t-test also assumes equal variances between the samples (var.equal = TRUE), whereas the Welches’ t-test does not (var.equal = FALSE). Using just normal data, and varying between experimental conditions whether the SDs are equal or unequal, how much does violating the Student’s t-test undermine its false-positive or false-negative (power) rates? Does using a Welches’ t-test resolve this? If so, what are the downsides of Welches’ t-test, i.e., why don’t we use it by default?\nNote that this isn’t a trivial exercise, it would take you some time to answer. People have published papers on these exact questions (Delacre, Lakens and Leys, 2017).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assessing the impact of violating the assumption of normality</span>"
    ]
  },
  {
    "objectID": "chapters/testing_assumptions_and_conditional_tests.html",
    "href": "chapters/testing_assumptions_and_conditional_tests.html",
    "title": "12  The statistical power of assumptions tests and the conditional use of non-parameteric tests",
    "section": "",
    "text": "12.1 Check your understanding\nWhat is the relationship between a linear regression, a Student’s t-test and a Wilcox rank-sum test? Or between linear regression and ANOVA?\nWhat is the difference between a hypothesis test like a Student’s t-test and a test of assumptions such as Shapiro-Wilk’s test (for normality) or Levene’s test (for homogeneity of variances)?",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The statistical power of assumptions tests and the conditional use of non-parameteric tests</span>"
    ]
  },
  {
    "objectID": "chapters/testing_assumptions_and_conditional_tests.html#answer",
    "href": "chapters/testing_assumptions_and_conditional_tests.html#answer",
    "title": "12  The statistical power of assumptions tests and the conditional use of non-parameteric tests",
    "section": "12.2 Answer",
    "text": "12.2 Answer\nAlmost everything is a linear model.\n\n\nCode\nknitr::include_graphics(\"../figs/common_statistical_tests_are_linear_models_cheat_sheet.png\")",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The statistical power of assumptions tests and the conditional use of non-parameteric tests</span>"
    ]
  },
  {
    "objectID": "chapters/testing_assumptions_and_conditional_tests.html#overview",
    "href": "chapters/testing_assumptions_and_conditional_tests.html#overview",
    "title": "12  The statistical power of assumptions tests and the conditional use of non-parameteric tests",
    "section": "12.3 Overview",
    "text": "12.3 Overview\nWhat do most statistics textbooks tell you to do when trying to test if two groups’ means differ?\n\nCheck assumptions of an independent Student’s t-test are met, e.g., normality of data and homogeneity of variances.\nIf so, run an interpret an independent Student’s t-test.\nIf not, then perhaps perhaps either ‘interpret results with caution’ (which always feels vague) or run and interpret a non-parametric test instead.\n\nWhy? What benefits are there for doing so? Or what bad things happen if you don’t?\nIn a previous session, we observed that violations of the assumption of normality actually has very little impact on the false-positive and false-negative rates of a t-test, as long as the two conditions have similarly non-normal data, which is plausible in many situations. This lesson seeks to answer two related questions:\n\nJust like hypothesis tests, assumptions tests are just inferential tests of other properties (e.g., differences in SDs rather than differences in means), and as such they have false-positive rates and false-negative rates (statistical power). What is the power of these tests under different degrees of violations of assumptions? I.e. what proportion of the time do they get it wrong?\nWhat is the aggregate benefit of choosing a hypothesis test based on the results of assumption tests? This multi-step researcher behavior can itself be simulated.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The statistical power of assumptions tests and the conditional use of non-parameteric tests</span>"
    ]
  },
  {
    "objectID": "chapters/testing_assumptions_and_conditional_tests.html#dependencies",
    "href": "chapters/testing_assumptions_and_conditional_tests.html#dependencies",
    "title": "12  The statistical power of assumptions tests and the conditional use of non-parameteric tests",
    "section": "12.4 Dependencies",
    "text": "12.4 Dependencies\n\n\nCode\n# dependencies\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(readr)\nlibrary(purrr) \nlibrary(furrr)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(sn)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# set up parallelization\nplan(multisession)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The statistical power of assumptions tests and the conditional use of non-parameteric tests</span>"
    ]
  },
  {
    "objectID": "chapters/testing_assumptions_and_conditional_tests.html#power-of-students-t-test",
    "href": "chapters/testing_assumptions_and_conditional_tests.html#power-of-students-t-test",
    "title": "12  The statistical power of assumptions tests and the conditional use of non-parameteric tests",
    "section": "12.5 Power of Student’s t-test",
    "text": "12.5 Power of Student’s t-test\nCode taken from the previous lesson, with some simplifications.\n\n\nCode\n# remove all objects from environment ----\nrm(list = ls())\n\n# define data generating function ----\ngenerate_data &lt;- function(n,\n                          location_control, # location, akin to mean\n                          location_intervention,\n                          scale, # scale, akin to SD\n                          skew) { # slant/skew. When 0, produces normal/gaussian data\n  \n  data_control &lt;- \n    tibble(condition = \"control\",\n           score = rsn(n = n, \n                       xi = location_control, # location, akin to mean\n                       omega = scale, # scale, akin to SD\n                       alpha = skew)) # slant/skew. When 0, produces normal/gaussian data\n  \n  data_intervention &lt;- \n    tibble(condition = \"intervention\",\n           score = rsn(n = n, \n                       xi = location_intervention, # location, akin to mean\n                       omega = scale, # scale, akin to SD\n                       alpha = skew)) # slant/skew. When 0, produces normal/gaussian data\n  \n  data &lt;- bind_rows(data_control,\n                    data_intervention) \n  \n  return(data)\n}\n\n\n# define data analysis function ----\nanalyze_data_students_t &lt;- function(data) {\n  \n  hypothesis_test_students_t &lt;- t.test(formula = score ~ condition, \n                                       data = data,\n                                       var.equal = TRUE,\n                                       alternative = \"two.sided\")\n  \n  res &lt;- tibble(hypothesis_test_p_students_t = hypothesis_test_students_t$p.value) \n  \n  return(res)\n}\n\n\n# set the seed ----\n# for the pseudo random number generator to make results reproducible\nset.seed(42)\n\n\n# define experiment parameters ----\nexperiment_parameters &lt;- expand_grid(\n  n = c(10, 25, 50, 100, 200),\n  mu_control = 0,\n  mu_intervention = 0.5, \n  sigma = 1,\n  skew = c(0, 2, 4, 8, 12),     # slant/skew. When 0, produces normal/gaussian data\n  iteration = 1:1000 \n) |&gt;\n  # calculate skew-normal parameters\n  # don't worry about the math, it's not important to understand\n  mutate(delta = skew / sqrt(1 + skew^2),\n         scale = sigma / sqrt(1 - 2 * delta^2 / pi),\n         location_control = mu_control - scale * delta * sqrt(2 / pi),\n         location_intervention = mu_intervention - scale * delta * sqrt(2 / pi)) \n\n\n# run simulation ----\nsimulation &lt;- \n  # using the experiment parameters\n  experiment_parameters |&gt;\n  \n  # generate data using the data generating function and the parameters relevant to data generation\n  mutate(generated_data = future_pmap(.l = list(n = n,\n                                                location_control = location_control,\n                                                location_intervention = location_intervention,\n                                                scale = scale,\n                                                skew = skew),\n                                      .f = generate_data,\n                                      .progress = TRUE,\n                                      .options = furrr_options(seed = TRUE))) |&gt;\n  \n  # apply the analysis function to the generated data using the parameters relevant to analysis\n  mutate(analysis_results_students_t = future_pmap(.l = list(data = generated_data),\n                                                   .f = analyze_data_students_t,\n                                                   .progress = TRUE,\n                                                   .options = furrr_options(seed = TRUE)))\n\n# summarise simulation results over the iterations ----\n## ie what proportion of p values are significant (&lt; .05)\nsimulation_summary &lt;- simulation |&gt;\n  unnest(analysis_results_students_t) |&gt;\n  mutate(skew = as.factor(skew)) |&gt;\n  group_by(n, \n           location_control,\n           location_intervention,\n           scale, \n           skew) |&gt;\n  summarize(proportion_significant_students_t = mean(hypothesis_test_p_students_t &lt; .05),\n            .groups = \"drop\")\n\n# plot\nggplot(simulation_summary, aes(n*2, proportion_significant_students_t, color = skew)) +\n  geom_hline(yintercept = 0.80, linetype = \"dotted\") +\n  geom_hline(yintercept = 0.05, linetype = \"dotted\") +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = breaks_pretty(n = 9), name = \"Total sample size\") +\n  scale_y_continuous(limits = c(0, 1), breaks = breaks_pretty(n = 9), name = \"Proportion significant results\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.2, end = 0.8)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The statistical power of assumptions tests and the conditional use of non-parameteric tests</span>"
    ]
  },
  {
    "objectID": "chapters/testing_assumptions_and_conditional_tests.html#power-of-students-t-test-vs.-wilcoxon-rank-sum-test",
    "href": "chapters/testing_assumptions_and_conditional_tests.html#power-of-students-t-test-vs.-wilcoxon-rank-sum-test",
    "title": "12  The statistical power of assumptions tests and the conditional use of non-parameteric tests",
    "section": "12.6 Power of Student’s t-test vs. Wilcoxon rank-sum test",
    "text": "12.6 Power of Student’s t-test vs. Wilcoxon rank-sum test\nAka Mann-Whitney U-test.\n\n12.6.1 Exercise\nWrite a function called analyze_data_wilcox() that uses wilcox.test() instead of t.test() to compare the differences in central tendency between the conditions.\n\n\n12.6.2 Answer\n\n\nShow solution\n# define data analysis function ----\nanalyze_data_wilcox &lt;- function(data) {\n  \n  hypothesis_test_wilcox &lt;- wilcox.test(formula = score ~ condition,  # added\n                                        data = data,\n                                        alternative = \"two.sided\")\n  \n  res &lt;- tibble(hypothesis_test_p_wilcox = hypothesis_test_wilcox$p.value)  # added \n  \n  return(res)\n}\n\n\n\n\n12.6.3 Simulation\n\n\nCode\n# run simulation ----\nsimulation_alt_analysis &lt;- \n  # using the experiment parameters\n  simulation |&gt;\n\n  # apply the analysis function to the generated data using the parameters relevant to analysis\n  mutate(analysis_results_wilcox = future_pmap(.l = list(data = generated_data),\n                                               .f = analyze_data_wilcox,\n                                               .progress = TRUE,\n                                               .options = furrr_options(seed = TRUE)))\n\n\n# summarise simulation results over the iterations ----\n## ie what proportion of p values are significant (&lt; .05)\nsimulation_summary &lt;- simulation_alt_analysis |&gt;\n  unnest(analysis_results_students_t) |&gt;\n  unnest(analysis_results_wilcox) |&gt;\n  mutate(skew = as.factor(skew)) |&gt;\n  group_by(n, \n           location_control,\n           location_intervention,\n           scale, \n           skew) |&gt;\n  summarize(proportion_significant_students_t = mean(hypothesis_test_p_students_t &lt; .05),\n            proportion_significant_wilcox = mean(hypothesis_test_p_wilcox &lt; .05),\n            .groups = \"drop\")\n\n# plots\nggplot(simulation_summary, aes(n*2, proportion_significant_students_t, color = skew)) +\n  geom_hline(yintercept = 0.80, linetype = \"dotted\") +\n  geom_hline(yintercept = 0.05, linetype = \"dotted\") +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = breaks_pretty(n = 9), name = \"Total sample size\") +\n  scale_y_continuous(limits = c(0, 1), breaks = breaks_pretty(n = 9), name = \"Proportion significant results\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.2, end = 0.8) +\n  ggtitle(\"Power of Student's t-test for Cohen's d = 0.5\")\n\n\n\n\n\n\n\n\n\nCode\nggplot(simulation_summary, aes(n*2, proportion_significant_wilcox, color = skew)) +\n  geom_hline(yintercept = 0.80, linetype = \"dotted\") +\n  geom_hline(yintercept = 0.05, linetype = \"dotted\") +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = breaks_pretty(n = 9), name = \"Total sample size\") +\n  scale_y_continuous(limits = c(0, 1), breaks = breaks_pretty(n = 9), name = \"Proportion significant results\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.2, end = 0.8) +\n  ggtitle(\"Power of Wilcoxon rank-sum test for Cohen's d = 0.5\")\n\n\n\n\n\n\n\n\n\n\nWhat’s the upside and downside of each?",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The statistical power of assumptions tests and the conditional use of non-parameteric tests</span>"
    ]
  },
  {
    "objectID": "chapters/testing_assumptions_and_conditional_tests.html#power-of-shapiro-wilks-test-with-skew-normal-data",
    "href": "chapters/testing_assumptions_and_conditional_tests.html#power-of-shapiro-wilks-test-with-skew-normal-data",
    "title": "12  The statistical power of assumptions tests and the conditional use of non-parameteric tests",
    "section": "12.7 Power of Shapiro-Wilk’s test with skew-normal data",
    "text": "12.7 Power of Shapiro-Wilk’s test with skew-normal data\n\n12.7.1 Exercise\nWrite a function called test_assumption_of_normality() that uses shapiro.test() to assess for non-normality in each condition (control and intervention). It should return a p value for both tests.\n\n\n12.7.2 Answer\n\n\nCode\n# define data analysis function ----\ntest_assumption_of_normality &lt;- function(data) {\n  \n  fit_intervention &lt;- data |&gt;\n    filter(condition == \"intervention\") |&gt;\n    pull(score) |&gt;\n    shapiro.test()\n  \n  fit_control &lt;- data |&gt;\n    filter(condition == \"control\") |&gt;\n    pull(score) |&gt;\n    shapiro.test()\n  \n  res &lt;- tibble(assumption_test_p_sharpirowilks_intervention = fit_intervention$p.value, \n                assumption_test_p_sharpirowilks_control = fit_control$p.value) \n  \n  return(res)\n}\n\n\n\n\n12.7.3 Simulation\nTest for normality in each of two samples, and return a decision of non-normality if it is found in either.\n\n\nCode\n# run simulation ----\nsimulation_normality &lt;- \n  # using the experiment parameters\n  simulation_alt_analysis |&gt;\n\n  # apply the analysis function to the generated data using the parameters relevant to analysis\n  mutate(analysis_results_normality = future_pmap(.l = list(generated_data),\n                                                  .f = test_assumption_of_normality,\n                                                  .progress = TRUE,\n                                                  .options = furrr_options(seed = TRUE)))\n\n# summarise simulation results over the iterations ----\n## ie what proportion of p values are significant (&lt; .05)\nsimulation_summary &lt;- simulation_normality |&gt;\n  unnest(analysis_results_students_t) |&gt;\n  unnest(analysis_results_wilcox) |&gt;\n  unnest(analysis_results_normality) |&gt;\n  mutate(skew = as.factor(skew)) |&gt;\n  # choose the lower of the two shapiro wilk's tests' p values\n  mutate(assumption_test_p_sharpirowilks_lower = ifelse(assumption_test_p_sharpirowilks_intervention &lt; assumption_test_p_sharpirowilks_control, \n                                                        assumption_test_p_sharpirowilks_intervention, \n                                                        assumption_test_p_sharpirowilks_control)) |&gt;\n  group_by(n, \n           location_control,\n           location_intervention,\n           scale, \n           skew) |&gt;\n  summarize(proportion_significant_students_t = mean(hypothesis_test_p_students_t &lt; .05),\n            proportion_significant_wilcox = mean(hypothesis_test_p_wilcox &lt; .05),\n            proportion_significant_shapirowilks = mean(assumption_test_p_sharpirowilks_lower &lt; .05),\n            .groups = \"drop\")\n\n# plot\nggplot(simulation_summary, aes(n*2, proportion_significant_shapirowilks, color = skew)) +\n  geom_hline(yintercept = 0.80, linetype = \"dotted\") +\n  geom_hline(yintercept = 0.05, linetype = \"dotted\") +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = breaks_pretty(n = 9), name = \"Total sample size\") +\n  scale_y_continuous(limits = c(0, 1), breaks = breaks_pretty(n = 9), name = \"Proportion significant results\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.2, end = 0.8) +\n  ggtitle(\"Power of Shapiro-Wilks test applied to each condition\")\n\n\n\n\n\n\n\n\n\n\nWhat does this tell us?",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The statistical power of assumptions tests and the conditional use of non-parameteric tests</span>"
    ]
  },
  {
    "objectID": "chapters/testing_assumptions_and_conditional_tests.html#conditionally-running-a-students-t-test-or-a-wilcoxon-rank-sum-test-depending-on-shapiro-wilks-test-for-normality-in-both-samples",
    "href": "chapters/testing_assumptions_and_conditional_tests.html#conditionally-running-a-students-t-test-or-a-wilcoxon-rank-sum-test-depending-on-shapiro-wilks-test-for-normality-in-both-samples",
    "title": "12  The statistical power of assumptions tests and the conditional use of non-parameteric tests",
    "section": "12.8 Conditionally running a Student’s t-test or a Wilcoxon rank-sum test depending on Shapiro-Wilk’s test for normality in both samples",
    "text": "12.8 Conditionally running a Student’s t-test or a Wilcoxon rank-sum test depending on Shapiro-Wilk’s test for normality in both samples\n\n12.8.1 Exercise\nModify the code below to implement condition logic between how the tests are interpreted.\nI.e., for each iteration, calculate a conditional p value: if either of the Shapiro-Wilks tests are significant (evidence of non-normality), use the p value from the Wilcox test; if not, use the Student’s t test’s p value.\nSummarize the power of the conditional p value vs. the Wilcox’s test’s p value. This compares the workflows where we a) use Shapiro-Wilk’s tests to choose which method to use to test the hypothesis vs. b) just use the non-parametric test by default.\n\n\nCode\n# summarise simulation results over the iterations ----\nsimulation_summary &lt;- simulation_normality |&gt;\n  unnest(analysis_results_students_t) |&gt;\n  unnest(analysis_results_wilcox) |&gt;\n  unnest(analysis_results_normality) |&gt;\n  # choose the lower of the two shapiro wilk's tests' p values\n  mutate(assumption_test_p_sharpirowilks_lower = ifelse(assumption_test_p_sharpirowilks_intervention &lt; assumption_test_p_sharpirowilks_control, \n                                                        assumption_test_p_sharpirowilks_intervention, \n                                                        assumption_test_p_sharpirowilks_control)) |&gt;\n  # condition logic\n  mutate(hypothesis_p_conditional = ifelse(assumption_test_p_sharpirowilks_lower &lt; .05, \n                                           hypothesis_test_p_wilcox,\n                                           hypothesis_test_p_students_t)) |&gt;\n  # summarize\n  mutate(skew = as.factor(skew)) |&gt;\n  group_by(n, \n           location_control,\n           location_intervention,\n           scale, \n           skew) |&gt;\n  summarize(proportion_significant_conditional = mean(hypothesis_p_conditional &lt; .05),\n            proportion_significant_wilcox = mean(hypothesis_test_p_wilcox &lt; .05),\n            .groups = \"drop\") |&gt;\n  mutate(power_diff = proportion_significant_conditional - proportion_significant_wilcox)\n\n\n\n\n12.8.2 Answer\nThis stimulation tests normality in both conditions’ data, as well as testing for differences in the central tendency using both parametric and non-parametric tests. Which test of the differences in central tendency is used for each simulated data set is determined by whether the assumption of normality is detectably violated.\n\n\nShow solution\n# summarise simulation results over the iterations ----\nsimulation_summary &lt;- simulation_normality |&gt;\n  unnest(analysis_results_students_t) |&gt;\n  unnest(analysis_results_wilcox) |&gt;\n  unnest(analysis_results_normality) |&gt;\n  # choose the lower of the two shapiro wilk's tests' p values\n  mutate(assumption_test_p_sharpirowilks_lower = ifelse(assumption_test_p_sharpirowilks_intervention &lt; assumption_test_p_sharpirowilks_control, \n                                                        assumption_test_p_sharpirowilks_intervention, \n                                                        assumption_test_p_sharpirowilks_control)) |&gt;\n  # condition logic\n  mutate(hypothesis_p_conditional = ifelse(assumption_test_p_sharpirowilks_lower &lt; .05, \n                                           hypothesis_test_p_wilcox,\n                                           hypothesis_test_p_students_t)) |&gt;\n  # summarize\n  mutate(skew = as.factor(skew)) |&gt;\n  group_by(n, \n           location_control,\n           location_intervention,\n           scale, \n           skew) |&gt;\n  summarize(proportion_significant_conditional = mean(hypothesis_p_conditional &lt; .05),\n            proportion_significant_wilcox = mean(hypothesis_test_p_wilcox &lt; .05),\n            .groups = \"drop\") |&gt;\n  mutate(power_diff = proportion_significant_conditional - proportion_significant_wilcox)\n\n# plots\nggplot(simulation_summary, aes(n*2, proportion_significant_conditional, color = skew)) +\n  geom_hline(yintercept = 0.80, linetype = \"dotted\") +\n  geom_hline(yintercept = 0.05, linetype = \"dotted\") +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = breaks_pretty(n = 9), name = \"Total sample size\") +\n  scale_y_continuous(limits = c(0, 1), breaks = breaks_pretty(n = 9), name = \"Proportion significant results\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.2, end = 0.8) +\n  ggtitle(\"Power of conditional Student's t-test vs. Wilcox test for Cohen's d = 0.5\")\n\n\n\n\n\n\n\n\n\nShow solution\n# ggsave(\"plots/conditional.png\",\n#        width = 7,\n#        height = 4)\n\nggplot(simulation_summary, aes(n*2, proportion_significant_wilcox, color = skew)) +\n  geom_hline(yintercept = 0.80, linetype = \"dotted\") +\n  geom_hline(yintercept = 0.05, linetype = \"dotted\") +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = breaks_pretty(n = 9), name = \"Total sample size\") +\n  scale_y_continuous(limits = c(0, 1), breaks = breaks_pretty(n = 9), name = \"Proportion significant results\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.2, end = 0.8) +\n  ggtitle(\"Power of Wilcoxon rank-sum test for Cohen's d = 0.5\")\n\n\n\n\n\n\n\n\n\nShow solution\n# ggsave(\"plots/nonparametric.png\",\n#        width = 7,\n#        height = 4)\n\nggplot(simulation_summary, aes(power_diff)) +\n  geom_histogram(binwidth = 0.01) +\n  theme_linedraw() +\n  xlab(\"Difference in power between conditional tests\\nvs. always running non-parametric\")\n\n\n\n\n\n\n\n\n\n\nWhat does this tell us about whether we should (a) test the assumption of normality and then run a (non)parameteric test conditionally or just run the non-parametric test by default?\nWhat interpretation issues might be an argument against doing this by default?\nAre you starting to see why statisticians usually reply with “it depends” when we ask them questions?",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The statistical power of assumptions tests and the conditional use of non-parameteric tests</span>"
    ]
  },
  {
    "objectID": "chapters/testing_assumptions_and_conditional_tests.html#recap",
    "href": "chapters/testing_assumptions_and_conditional_tests.html#recap",
    "title": "12  The statistical power of assumptions tests and the conditional use of non-parameteric tests",
    "section": "12.9 Recap",
    "text": "12.9 Recap\nWhat did you learn here?\n\nAbout statistical tests?\nAbout using Monte Carlo simulations to understand not just single tests but to understand workflows?\nAbout how to use this {purrr} simulation workflow to analyze data more than one way and make conditional decisions, in order to model the behavior of scientists?",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The statistical power of assumptions tests and the conditional use of non-parameteric tests</span>"
    ]
  },
  {
    "objectID": "chapters/testing_assumptions_and_conditional_tests.html#at-home-exercises",
    "href": "chapters/testing_assumptions_and_conditional_tests.html#at-home-exercises",
    "title": "12  The statistical power of assumptions tests and the conditional use of non-parameteric tests",
    "section": "12.10 At-home exercises",
    "text": "12.10 At-home exercises\n\n12.10.1 Collate the simulation above into a single code chunk with all the pieces to run the full simulation\n\n\n12.10.2 Elaborate the simulation\nOne relatively simpler extension would be to assess how much better or worse just using the Student’s t test by default is compared to a) using Student’s t-test vs. Wilcox conditionally based on the Shapiro Wilks tests or b) using the Wilcox test by default. I.e., try adding c) using Student’s t-test by default.\nRight now the simulation only examines a fixed effect size. Perhaps the supposed negative impact of violating normality would be seen at other effect sizes? Vary this or indeed other things to make an informed decision about how to choose which test(s) to run to make the inference about differences in means between conditions.\n\n\n12.10.3 Develop a simulation to examine the power of the conditional use of (non)parametric based on heteroscedasticity vs using non-parametric tests by default\nDecisions about which hypothesis test is used are also made on the basis of other tests of those assumptions. For example, heteroscedasticity (equal SDs) can be tested using leveneTest(). This has lower power than Bartlett’s test but does not assume normality.\nWrite a simulation that is analogous to the one above, but which compares the statistical power of a) the conditional use of Student’s t test or Wilcox test based on the result of Levene’s test vs. b) using Wilcox test by default.\nFor simplicity:\n\nUse a true effect population effect size of Cohen’s d = 0.5 in all simulations.\nUse only normal data.\nVary degree of heteroscedasticity between the groups, i.e., set it to 1, 1.5, or 2 in the intervention group. Work out what you’ll need to set it to in the control group and why.\nUse the same sample sizes as as in the above simulation (i.e., n = c(10, 25, 50, 100, 200)).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>The statistical power of assumptions tests and the conditional use of non-parameteric tests</span>"
    ]
  },
  {
    "objectID": "chapters/p_hacking.html",
    "href": "chapters/p_hacking.html",
    "title": "13  p-hacking via different forms of selecting reporting",
    "section": "",
    "text": "13.1 Overview\np-hacking is increasing the false-positive rate through analytic choices. These choices are often a) flexible, in that many different options might be tried until significance is found (Gelman called this the “Garden of Forking Paths”), and b) undisclosed. However, it need not be either in a given instance: perhaps the very first analytic strategy tried produces the significant result (but if it hadn’t, other strategies would have been tried), or perhaps the author fully discloses the analytic choices but does not make clear to the reader how this undermines the severity of the test (i.e., it takes a very close reading to understand that the results present weak evidence, or the verbal conclusions oversell this).\np-hacking can occur because of the uniform distribution of p-values under the null hypothesis. Because all values are equally likely, you just have to keep rolling the dice to eventually find p &lt; .05.\nThis lesson illustrates a few different examples of one broad form of p-hacking called selective reporting.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>p-hacking via different forms of selecting reporting</span>"
    ]
  },
  {
    "objectID": "chapters/p_hacking.html#dependencies",
    "href": "chapters/p_hacking.html#dependencies",
    "title": "13  p-hacking via different forms of selecting reporting",
    "section": "13.2 Dependencies",
    "text": "13.2 Dependencies\n\n\nCode\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(tibble)\nlibrary(purrr) \nlibrary(furrr)\nlibrary(faux)\nlibrary(janitor)\n#library(afex)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# set up parallelization\nplan(multisession)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>p-hacking via different forms of selecting reporting</span>"
    ]
  },
  {
    "objectID": "chapters/p_hacking.html#selective-reporting-of-studies",
    "href": "chapters/p_hacking.html#selective-reporting-of-studies",
    "title": "13  p-hacking via different forms of selecting reporting",
    "section": "13.3 Selective reporting of studies",
    "text": "13.3 Selective reporting of studies\nIf I run two experiments, and only report the one that “works”, I will increase the false positive rate across studies. Perhaps it is because one really is better designed etc than the other. Or perhaps its just a false positive.\nWhat would you guess the false positive rate is for two studies?\nLet’s simulate it.\n\n13.3.1 Run simulation\n\n\nCode\ngenerate_data &lt;- function(n_per_condition,\n                          mean_control,\n                          mean_intervention,\n                          sd) {\n  \n  data_control &lt;- \n    tibble(condition = \"control\",\n           score = rnorm(n = n_per_condition, mean = mean_control, sd = sd))\n  \n  data_intervention &lt;- \n    tibble(condition = \"intervention\",\n           score = rnorm(n = n_per_condition, mean = mean_intervention, sd = sd))\n  \n  data_combined &lt;- bind_rows(data_control,\n                             data_intervention)\n  \n  return(data_combined)\n}\n\n\n# define data analysis function ----\nanalyze &lt;- function(data) {\n  \n  students_ttest &lt;- t.test(formula = score ~ condition,\n                           data = data,\n                           var.equal = TRUE,\n                           alternative = \"two.sided\")\n  \n  res &lt;- tibble(p = students_ttest$p.value)\n  \n  return(res)\n}\n\n\n# define experiment parameters ----\nexperiment_parameters &lt;- expand_grid(\n  n_per_condition = 100, \n  mean_control = 0,\n  mean_intervention = 0,\n  sd = 1,\n  iteration = 1:1000\n)\n\n\n# run simulation ----\nset.seed(42)\n\nsimulation &lt;- \n  # using the experiment parameters\n  experiment_parameters |&gt;\n  \n  # generate data using the data generating function and the parameters relevant to data generation\n  mutate(generated_data_study1 = future_pmap(.l = list(n_per_condition,\n                                                       mean_control,\n                                                       mean_intervention,\n                                                       sd),\n                                             .f = generate_data,\n                                             .progress = TRUE,\n                                             .options = furrr_options(seed = TRUE))) |&gt;\n  mutate(generated_data_study2 = future_pmap(.l = list(n_per_condition,\n                                                       mean_control,\n                                                       mean_intervention,\n                                                       sd),\n                                             .f = generate_data,\n                                             .progress = TRUE,\n                                             .options = furrr_options(seed = TRUE))) |&gt;\n  \n  # apply the analysis function to the generated data using the parameters relevant to analysis\n  mutate(results_study1 = future_pmap(.l = list(data = generated_data_study1),\n                                      .f = analyze,\n                                      .progress = TRUE,\n                                      .options = furrr_options(seed = TRUE))) |&gt;\n  mutate(results_study2 = future_pmap(.l = list(data = generated_data_study2),\n                                      .f = analyze,\n                                      .progress = TRUE,\n                                      .options = furrr_options(seed = TRUE)))\n\n\n# summarise simulation results over the iterations ----\nsimulation_summary &lt;- simulation |&gt;\n  # unnest and rename\n  unnest(results_study1) |&gt;\n  rename(p_study1 = p) |&gt;\n  unnest(results_study2) |&gt;\n  rename(p_study2 = p) |&gt;\n  # simulate flexible reporting\n  mutate(p_hacked = ifelse(p_study1 &lt; .05, p_study1, p_study2)) |&gt;\n  # summarize\n  summarize(prop_sig_study1 = mean(p_study1 &lt; .05),\n            prop_sig_study2 = mean(p_study2 &lt; .05),\n            prop_sig_hacked = mean(p_hacked &lt; .05),\n            .groups = \"drop\") |&gt;\n  pivot_longer(cols = everything(), \n               names_to = \"Source\",\n               values_to = \"proportion_significant\") |&gt;\n  mutate(Source = str_remove(Source, \"prop_sig_\"))\n\n\n\n\n13.3.2 Results\n\n\nCode\nsimulation_summary |&gt;\n  mutate_if(is.numeric, round_half_up, digits = 2) |&gt;\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\nSource\nproportion_significant\n\n\n\n\nstudy1\n0.04\n\n\nstudy2\n0.05\n\n\nhacked\n0.09\n\n\n\n\n\n\n\n13.3.3 Conclusions\nThe false positive rate for two studies is ~10%, because each study has a 5% false positive rate.\n\n\n13.3.4 Exercise: extend the simulation\nExtend the simulation so that four studies are run. What do you expect the false positive rate to be? What do you find?\n\n\n13.3.5 Solution\n\n\nCode\ngenerate_data &lt;- function(n_per_condition,\n                          mean_control,\n                          mean_intervention,\n                          sd) {\n  \n  data_control &lt;- \n    tibble(condition = \"control\",\n           score = rnorm(n = n_per_condition, mean = mean_control, sd = sd))\n  \n  data_intervention &lt;- \n    tibble(condition = \"intervention\",\n           score = rnorm(n = n_per_condition, mean = mean_intervention, sd = sd))\n  \n  data_combined &lt;- bind_rows(data_control,\n                             data_intervention)\n  \n  return(data_combined)\n}\n\n\n# define data analysis function ----\nanalyze &lt;- function(data) {\n  \n  students_ttest &lt;- t.test(formula = score ~ condition,\n                           data = data,\n                           var.equal = TRUE,\n                           alternative = \"two.sided\")\n  \n  res &lt;- tibble(p = students_ttest$p.value)\n  \n  return(res)\n}\n\n\n# define experiment parameters ----\nexperiment_parameters &lt;- expand_grid(\n  n_per_condition = 100, \n  mean_control = 0,\n  mean_intervention = 0,\n  sd = 1,\n  iteration = 1:1000\n)\n\n\n# run simulation ----\nset.seed(42)\n\nsimulation &lt;- \n  # using the experiment parameters\n  experiment_parameters |&gt;\n  \n  # generate data using the data generating function and the parameters relevant to data generation\n  mutate(generated_data_study1 = future_pmap(.l = list(n_per_condition = n_per_condition,\n                                                       mean_control = mean_control,\n                                                       mean_intervention = mean_intervention,\n                                                       sd = sd),\n                                             .f = generate_data,\n                                             .progress = TRUE,\n                                             .options = furrr_options(seed = TRUE))) |&gt;\n  mutate(generated_data_study2 = future_pmap(.l = list(n_per_condition = n_per_condition,\n                                                       mean_control = mean_control,\n                                                       mean_intervention = mean_intervention,\n                                                       sd = sd),\n                                             .f = generate_data,\n                                             .progress = TRUE,\n                                             .options = furrr_options(seed = TRUE))) |&gt;\n  mutate(generated_data_study3 = future_pmap(.l = list(n_per_condition = n_per_condition,\n                                                       mean_control = mean_control,\n                                                       mean_intervention = mean_intervention,\n                                                       sd = sd),\n                                             .f = generate_data,\n                                             .progress = TRUE,\n                                             .options = furrr_options(seed = TRUE))) |&gt;\n  mutate(generated_data_study4 = future_pmap(.l = list(n_per_condition = n_per_condition,\n                                                       mean_control = mean_control,\n                                                       mean_intervention = mean_intervention,\n                                                       sd = sd),\n                                             .f = generate_data,\n                                             .progress = TRUE,\n                                             .options = furrr_options(seed = TRUE))) |&gt;\n  \n  # apply the analysis function to the generated data using the parameters relevant to analysis\n  mutate(results_study1 = future_pmap(.l = list(data = generated_data_study1),\n                                      analyze,\n                                      .progress = TRUE,\n                                      .options = furrr_options(seed = TRUE))) |&gt;\n  mutate(results_study2 = future_pmap(.l = list(data = generated_data_study2),\n                                      analyze,\n                                      .progress = TRUE,\n                                      .options = furrr_options(seed = TRUE))) |&gt;\n  mutate(results_study3 = future_pmap(.l = list(data = generated_data_study3),\n                                      analyze,\n                                      .progress = TRUE,\n                                      .options = furrr_options(seed = TRUE))) |&gt;\n  mutate(results_study4 = future_pmap(.l = list(data = generated_data_study4),\n                                      analyze,\n                                      .progress = TRUE,\n                                      .options = furrr_options(seed = TRUE)))\n\n\n# summarise simulation results over the iterations ----\nsimulation_summary &lt;- simulation |&gt;\n  # unnest and rename\n  unnest(results_study1) |&gt;\n  rename(p_study1 = p) |&gt;\n  unnest(results_study2) |&gt;\n  rename(p_study2 = p) |&gt;\n  unnest(results_study3) |&gt;\n  rename(p_study3 = p) |&gt;\n  unnest(results_study4) |&gt;\n  rename(p_study4 = p) |&gt;\n  # simulate flexible reporting\n  mutate(p_hacked = case_when(p_study1 &lt; .05 ~ p_study1,\n                              p_study2 &lt; .05 ~ p_study2,\n                              p_study3 &lt; .05 ~ p_study3,\n                              TRUE ~ p_study4)) |&gt;\n  # summarize\n  summarize(prop_sig_study1 = mean(p_study1 &lt; .05),\n            prop_sig_study2 = mean(p_study2 &lt; .05),\n            prop_sig_study3 = mean(p_study3 &lt; .05),\n            prop_sig_study4 = mean(p_study4 &lt; .05),\n            prop_sig_hacked = mean(p_hacked &lt; .05),\n            .groups = \"drop\") |&gt;\n  pivot_longer(cols = everything(), \n               names_to = \"Source\",\n               values_to = \"proportion_significant\") |&gt;\n  mutate(Source = str_remove(Source, \"prop_sig_\"))\n\nsimulation_summary |&gt;\n  mutate_if(is.numeric, round_half_up, digits = 2) |&gt;\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\nSource\nproportion_significant\n\n\n\n\nstudy1\n0.04\n\n\nstudy2\n0.05\n\n\nstudy3\n0.04\n\n\nstudy4\n0.05\n\n\nhacked\n0.17",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>p-hacking via different forms of selecting reporting</span>"
    ]
  },
  {
    "objectID": "chapters/p_hacking.html#false-positive-rates",
    "href": "chapters/p_hacking.html#false-positive-rates",
    "title": "13  p-hacking via different forms of selecting reporting",
    "section": "13.4 False positive rates",
    "text": "13.4 False positive rates\nNote that these probabilities are merely additive. This might not be intuitive.\nWhile the false positive rate for any individual test is 5%, the probability of finding at least one significant results in exactly \\(k\\) number of tests follows the equation:\n\\[\nFPR = 1-(1-\\alpha)^k,\n\\]\nwhere \\(\\alpha\\) is the alpha value for the tests (e.g., 0.05) and \\(k\\) is the number of tests run.\nWe can write a function to calculate the FPR mathematically for different values of \\(\\alpha\\) and \\(k\\).\n\n\nCode\nfpr &lt;- function(k, alpha = 0.05){\n  fpr &lt;- 1 - (1 - alpha)^k\n  return(fpr)\n}\n\ndat_fpr &lt;- tibble(k = seq(from = 1, to = 10, by = 1)) |&gt;\n  mutate(fpr = map_dbl(k, fpr)) \n\ndat_fpr |&gt;\n  mutate_if(is.numeric, round_half_up, digits = 2) |&gt;\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\nk\nfpr\n\n\n\n\n1\n0.05\n\n\n2\n0.10\n\n\n3\n0.14\n\n\n4\n0.19\n\n\n5\n0.23\n\n\n6\n0.26\n\n\n7\n0.30\n\n\n8\n0.34\n\n\n9\n0.37\n\n\n10\n0.40",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>p-hacking via different forms of selecting reporting</span>"
    ]
  },
  {
    "objectID": "chapters/p_hacking.html#selective-reporting-of-hypotheses-harking",
    "href": "chapters/p_hacking.html#selective-reporting-of-hypotheses-harking",
    "title": "13  p-hacking via different forms of selecting reporting",
    "section": "13.5 Selective reporting of hypotheses / HARKing",
    "text": "13.5 Selective reporting of hypotheses / HARKing\nAka lack of familywise error correction. NB this might sometimes be seen more like selective interpretation if all results are reported, or just low test severity.\nANOVAs are, statistically speaking, somewhat dangerous because a) they by default include interactions among independent variables, but b) they do not by default apply familywise error corrections.\nThis means that if I include \\(k\\) independent variables, it produces \\(2^k - 1\\) p values for its main and interaction effects. This can rapidly increase the FPR. Until relatively recently, many people were also taught that ANOVA inherently corrects for multiple testing, when it does not - I was taught this in my Bachelors.\n\n13.5.1 Run simulation - not working\nI’ve written the generate data code in a more abstract way so that the number of independent variables can be changed easily.\n\n\nCode\n# # define generate data function ----\n# generate_data &lt;- function(n,\n#                           ivs,\n#                           mu = 0,\n#                           sd = 1) { \n#   \n#   n_iv &lt;- function(n) {\n#     strings &lt;- paste(sapply(1:n, function(i) paste0(\"x\", i, \" = c(group1 = 'Condition 1', group2 = 'Condition 2')\")), collapse = \", \") \n#     single_string &lt;- paste(strings, collapse = \", \")\n#     list_string &lt;- paste0(\"list(\", single_string, \")\")\n#     return(list_string)\n#   }\n#   \n#   data &lt;- sim_design(between = eval(parse(text = n_iv(ivs))), \n#                      n = 100, \n#                      mu = mu, \n#                      sd = sd,\n#                      plot = FALSE) |&gt;\n#     mutate(id = as.factor(id))\n#   \n#   return(data)\n# }\n# \n# # define data analysis function ----\n# analyse_data &lt;- function(data, ivs) {\n#   \n#   # generate a list of IVs\n#   generate_c_string &lt;- function(n) {\n#     sapply(1:n, function(i) paste0(\"x\", i))\n#   }\n#   \n#   # define contrasts option so it doesn't print message on every iteration\n#   options(contrasts = c(\"contr.sum\", \"contr.poly\"))\n#   \n#   fit &lt;- afex::aov_ez(id = \"id\", \n#                       dv = \"y\", \n#                       between = generate_c_string(ivs), \n#                       data = data,\n#                       anova_table = \"pes\")\n#   \n#   results &lt;- fit$anova_table |&gt;\n#     rownames_to_column(var = \"parameter\") |&gt;\n#     rename(p = `Pr(&gt;F)`,\n#            partial_eta_2 = pes,\n#            num_df = `num Df`,\n#            den_df = `den Df`)\n#   \n#   return(results)\n# }\n# \n# \n# # simulation conditions ----\n# experiment_parameters_grid &lt;- expand_grid(\n#   n = 100, \n#   ivs = 2,\n#   mu = 0,\n#   sd = 1, \n#   iteration = 1:1000\n# )\n# \n# # run simulation ----\n# set.seed(42)\n# \n# simulation &lt;- \n#   # using the experiment parameters\n#   experiment_parameters_grid |&gt;\n#   \n#   # generate data using the data generating function and the parameters relevant to data generation\n#   mutate(generated_data = future_pmap(.l = list(n = n,\n#                                                 ivs = ivs,\n#                                                 mu = mu,\n#                                                 sd = sd),\n#                                       .f = generate_data,\n#                                       .progress = TRUE,\n#                                       .options = furrr_options(seed = TRUE))) |&gt;\n#   \n#   # apply the analysis function to the generated data using the parameters relevant to analysis\n#   mutate(analysis_results = future_pmap(.l = list(generated_data = generated_data,\n#                                                   ivs = ivs),\n#                                         .f = analyse_data,\n#                                         .progress = TRUE,\n#                                         .options = furrr_options(seed = TRUE)))\n\n\n\n\n13.5.2 Results\n\n13.5.2.1 FPR by effect\n\n\nCode\n# simulation_summary &lt;- simulation |&gt;\n#   unnest(analysis_results) |&gt;\n#   # summarize\n#   group_by(ivs, parameter) |&gt;\n#   summarize(proportion_positive_results = mean(p &lt; .05))\n# \n# simulation_summary |&gt;\n#   mutate_if(is.numeric, round_half_up, digits = 2) |&gt;\n#   kable() |&gt;\n#   kable_classic(full_width = FALSE)\n\n\n\n\n13.5.2.2 FPR by ANOVA\n\n\nCode\n# simulation_summary &lt;- simulation |&gt;\n#   unnest(analysis_results) |&gt;\n#   # summarize\n#   group_by(ivs, iteration) |&gt;\n#   summarize(minimum_p = min(p)) |&gt;\n#   group_by(ivs) |&gt;\n#   summarize(proportion_positive_results = mean(minimum_p &lt; .05))\n# \n# simulation_summary |&gt;\n#   mutate_if(is.numeric, round_half_up, digits = 2) |&gt;\n#   kable() |&gt;\n#   kable_classic(full_width = FALSE)\n\n\n\n\n13.5.2.3 FPR by ANOVA with familywise error corrections\n\n\nCode\n# simulation_summary &lt;- simulation |&gt;\n#   unnest(analysis_results) |&gt;\n#   # summarize\n#   group_by(ivs, iteration) |&gt;\n#   mutate(p_adjusted = p.adjust(p, method = \"holm\")) |&gt;\n#   summarize(minimum_p_adjusted = min(p_adjusted)) |&gt;\n#   group_by(ivs) |&gt;\n#   summarize(proportion_positive_results = mean(minimum_p_adjusted &lt; .05))\n# \n# simulation_summary |&gt;\n#   mutate_if(is.numeric, round_half_up, digits = 2) |&gt;\n#   kable() |&gt;\n#   kable_classic(full_width = FALSE)\n\n\n\n\n\n13.5.3 Conclusions\nThe results are similar to the first simulation on multiple studies because the tests are independent.\n\n\n13.5.4 Exercise: extend the simulation\nIncrease the number of IVs included and observe the increase in FPR. This should follow the mathematical solution for the FPR (with some error), but simulating it allows us to see it happening rather than taking the math at face value.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>p-hacking via different forms of selecting reporting</span>"
    ]
  },
  {
    "objectID": "chapters/p_hacking.html#selective-reporting-of-outcomes",
    "href": "chapters/p_hacking.html#selective-reporting-of-outcomes",
    "title": "13  p-hacking via different forms of selecting reporting",
    "section": "13.6 Selective reporting of outcomes",
    "text": "13.6 Selective reporting of outcomes\nAka outcome switching.\nThere are of course situations where the tests are not independent, and this makes it more complicated to understand the FPR. For example, imagine I am interested in the impact of CBT on depression, but I include multiple measures of depression and report the one that ‘worked’.\n\n13.6.1 Generate correlated data\nWe haven’t simulated correlated data before so lets do that first.\n\n\nCode\nset.seed(42)\n\ndat &lt;- \n  faux::rnorm_multi(n = 1000,\n                    mu = c(0, 0),\n                    sd = c(1, 1),\n                    r = -0.7,\n                    varnames = c(\"var1\", \"var2\"))\n\ncor(dat)\n\n\n           var1       var2\nvar1  1.0000000 -0.7083632\nvar2 -0.7083632  1.0000000\n\n\nCode\nggplot(dat, aes(var1, var2)) +\n  geom_point(alpha = 0.4) +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\n\n\n13.6.2 Generate data\nCorrelated outcomes, known groups differences.\n\n\nCode\ngenerate_data &lt;- function(n_per_condition,\n                          r_between_outcomes,\n                          mu_control, # vector of length 1 or k\n                          mu_intervention, # vector of length 1 or k\n                          sigma_control = 1, # vector of length 1 or k\n                          sigma_intervention = 1) {  # vector of length 1 or k\n  \n  # generate data by condition\n  data_control &lt;- \n    rnorm_multi(n = n_per_condition,\n                mu = mu_control,\n                sd = sigma_control,\n                r = r_between_outcomes,\n                varnames = c(\"outcome1\", \"outcome2\")) |&gt;\n    mutate(condition = \"Control\")\n  \n  data_intervention &lt;- \n    rnorm_multi(n = n_per_condition,\n                mu = mu_intervention,\n                sd = sigma_intervention,\n                r = r_between_outcomes,\n                varnames = c(\"outcome1\", \"outcome2\")) |&gt;\n    mutate(condition = \"Intervention\")\n  \n  # combine\n  data_combined &lt;- \n    bind_rows(data_control, \n              data_intervention) |&gt;\n    mutate(condition = fct_relevel(condition, \"Intervention\", \"Control\"))\n  \n  return(data_combined)  \n}\n\n\nCheck the function works\n\n\nCode\n# simulate data\nset.seed(42)\n\ndat &lt;- generate_data(n_per_condition = 100000,\n                     r_between_outcomes = 0.6,\n                     mu_control = 0,\n                     mu_intervention = c(1.5, 1.3))\n\n# parameter recovery\ndat |&gt;\n  group_by(condition) |&gt;\n  summarize(mean_outcome1 = mean(outcome1),\n            mean_outcome2 = mean(outcome2),\n            correlation = cor(outcome1, outcome2)) |&gt;\n  mutate_if(is.numeric, round_half_up, digits = 2)\n\n\n# A tibble: 2 × 4\n  condition    mean_outcome1 mean_outcome2 correlation\n  &lt;fct&gt;                &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 Intervention           1.5           1.3         0.6\n2 Control                0             0           0.6\n\n\n\n\n13.6.3 Run simulation\n\n\nCode\n# define data analysis function ----\nanalyze &lt;- function(data, outcome) {\n  \n  data_renamed &lt;- data |&gt;\n    rename(score = {{outcome}})\n  \n  students_ttest &lt;- t.test(formula = score ~ condition,\n                           data = data_renamed,\n                           var.equal = TRUE,\n                           alternative = \"two.sided\")\n  \n  res &lt;- tibble(p = students_ttest$p.value)\n  \n  return(res)\n}\n\n\n# define experiment parameters ----\nexperiment_parameters &lt;- expand_grid(\n  n_per_condition = 100, \n  r_between_outcomes = c(0, .2, .4, .6, .8, .9),\n  mu_control = 0,\n  mu_intervention = 0,\n  iteration = 1:1000\n) \n\n\n# run simulation ----\nset.seed(42)\n\nsimulation &lt;- \n  # using the experiment parameters\n  experiment_parameters |&gt;\n  \n  # generate data using the data generating function and the parameters relevant to data generation\n  mutate(generated_data = future_pmap(.l = list(n_per_condition,\n                                                r_between_outcomes,\n                                                mu_control,\n                                                mu_intervention),\n                                      .f = generate_data,\n                                      .progress = TRUE,\n                                      .options = furrr_options(seed = TRUE))) |&gt;\n  \n  # apply the analysis function to the generated data using the parameters relevant to analysis\n  mutate(results_outcome1 = future_pmap(.l = list(data = generated_data,\n                                                  outcome = \"outcome1\"),\n                                        .f = analyze,\n                                        .progress = TRUE,\n                                        .options = furrr_options(seed = TRUE)),\n         results_outcome2 = future_pmap(.l = list(data = generated_data,\n                                                  outcome = \"outcome2\"),\n                                        .f = analyze,\n                                        .progress = TRUE,\n                                        .options = furrr_options(seed = TRUE)))\n\n\n# summarise simulation results over the iterations ----\nsimulation_summary &lt;- simulation |&gt;\n  # unnest and rename\n  unnest(results_outcome1) |&gt;\n  rename(outcome1_p = p) |&gt;\n  unnest(results_outcome2) |&gt;\n  rename(outcome2_p = p) |&gt;\n  # simulate flexible reporting\n  mutate(hacked_p = ifelse(outcome1_p &lt; .05, outcome1_p, outcome2_p)) |&gt;\n  # summarize\n  group_by(n_per_condition,\n           r_between_outcomes,\n           mu_control,\n           mu_intervention) |&gt;\n  summarize(prop_sig_outcome1 = mean(outcome1_p &lt; .05),\n            prop_sig_outcome2 = mean(outcome2_p &lt; .05),\n            prop_sig_hacked = mean(hacked_p &lt; .05),\n            .groups = \"drop\") \n\n\n\n\n13.6.4 Results\n\n\nCode\nggplot(simulation_summary, aes(r_between_outcomes, prop_sig_outcome1)) +\n  geom_hline(yintercept = 0.05, linetype = \"dotted\") +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = breaks_pretty(n = 9), name = \"True correlation between outcomes\") + \n  scale_y_continuous(limits = c(0, .2), breaks = breaks_pretty(n = 5), name = \"Proportion significant results\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.2, end = 0.8) \n\n\n\n\n\n\n\n\n\nCode\nggplot(simulation_summary, aes(r_between_outcomes, prop_sig_outcome2)) +\n  geom_hline(yintercept = 0.05, linetype = \"dotted\") +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = breaks_pretty(n = 9), name = \"True correlation between outcomes\") + \n  scale_y_continuous(limits = c(0, .2), breaks = breaks_pretty(n = 5), name = \"Proportion significant results\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.2, end = 0.8) \n\n\n\n\n\n\n\n\n\nCode\nggplot(simulation_summary, aes(r_between_outcomes, prop_sig_hacked)) +\n  geom_hline(yintercept = 0.05, linetype = \"dotted\") +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = breaks_pretty(n = 9), name = \"True correlation between outcomes\") + \n  scale_y_continuous(limits = c(0, .2), breaks = breaks_pretty(n = 5), name = \"Proportion significant results\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.2, end = 0.8) \n\n\n\n\n\n\n\n\n\n\n\n13.6.5 Conclusions\nHow does the false positive rate change when the outcome measures are correlated?",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>p-hacking via different forms of selecting reporting</span>"
    ]
  },
  {
    "objectID": "chapters/p_hacking.html#check-your-learning",
    "href": "chapters/p_hacking.html#check-your-learning",
    "title": "13  p-hacking via different forms of selecting reporting",
    "section": "13.7 Check your learning",
    "text": "13.7 Check your learning\nWould this problem be solved/improved if the authors reported the results of all outcome variables, not just the ones that ‘worked’?\n\n13.7.1 Answer\nNo - read about conjunctive vs disjunctive inferences [REF].\n\n\n13.7.2 Exercise: extend the simulation\nExtend the simulation to include additional outcome variables.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>p-hacking via different forms of selecting reporting</span>"
    ]
  },
  {
    "objectID": "chapters/p_hacking.html#readings",
    "href": "chapters/p_hacking.html#readings",
    "title": "13  p-hacking via different forms of selecting reporting",
    "section": "13.8 Readings",
    "text": "13.8 Readings\n\nCramer, A.O.J., van Ravenzwaaij, D., Matzke, D. et al. Hidden multiplicity in exploratory multiway ANOVA: Prevalence and remedies. Psychon Bull Rev 23, 640–647 (2016). https://doi.org/10.3758/s13423-015-0913-5\nStefan, A. M. and Schönbrodt F. D. (2023) Big little lies: a compendium and simulation of p-hacking strategies. Royal Society Open Science. 10220346 http://doi.org/10.1098/rsos.220346",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>p-hacking via different forms of selecting reporting</span>"
    ]
  },
  {
    "objectID": "chapters/p_values_and_confidence_intervals.html",
    "href": "chapters/p_values_and_confidence_intervals.html",
    "title": "14  Understanding the link between p-values, Confidence Intervals, and power",
    "section": "",
    "text": "14.1 Overview of tutorial\nThis tutorial simulates a population effect size of Cohen’s d = 0.5 for different sample sizes, and examines the relationship between Cohen’s d, its 95% Confidence Interval, and the significance of the t-test’s p-value.\nBy the end of this lesson you should understand that p-values are re-expressions of the same information conveyed by Confidence Intervals, and that statistical power is a re-expression of the width of Confidence Intervals.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Understanding the link between p-values, Confidence Intervals, and power</span>"
    ]
  },
  {
    "objectID": "chapters/p_values_and_confidence_intervals.html#dependencies",
    "href": "chapters/p_values_and_confidence_intervals.html#dependencies",
    "title": "14  Understanding the link between p-values, Confidence Intervals, and power",
    "section": "14.2 Dependencies",
    "text": "14.2 Dependencies\n\n\nCode\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(purrr) \nlibrary(stringr)\nlibrary(forcats)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(janitor)\nlibrary(effsize)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Understanding the link between p-values, Confidence Intervals, and power</span>"
    ]
  },
  {
    "objectID": "chapters/p_values_and_confidence_intervals.html#simulation",
    "href": "chapters/p_values_and_confidence_intervals.html#simulation",
    "title": "14  Understanding the link between p-values, Confidence Intervals, and power",
    "section": "14.3 Simulation",
    "text": "14.3 Simulation\n\n\nCode\n# functions for simulation\ngenerate_data &lt;- function(n_per_condition,\n                          mean_control,\n                          mean_intervention,\n                          sd) {\n  \n  data_control &lt;- \n    tibble(condition = \"control\",\n           score = rnorm(n = n_per_condition, mean = mean_control, sd = sd))\n  \n  data_intervention &lt;- \n    tibble(condition = \"intervention\",\n           score = rnorm(n = n_per_condition, mean = mean_intervention, sd = sd))\n  \n  data_combined &lt;- bind_rows(data_control,\n                             data_intervention) |&gt;\n    mutate(condition = fct_relevel(condition, \"intervention\", \"control\"))\n  \n  return(data_combined)\n}\n\nanalyze &lt;- function(data) {\n\n  res_t_test &lt;- t.test(formula = score ~ condition, \n                       data = data,\n                       var.equal = TRUE,\n                       alternative = \"two.sided\")\n  \n  res_cohens_d &lt;- effsize::cohen.d(formula = score ~ condition,\n                                   data = data,\n                                   pooled = TRUE)\n  \n  res &lt;- tibble(p = res_t_test$p.value, \n                cohens_d = res_cohens_d$estimate,\n                cohens_d_ci_lower = res_cohens_d$conf.int[1],\n                cohens_d_ci_upper = res_cohens_d$conf.int[2])\n\n  return(res)\n}\n\n\n# set seed\nset.seed(42)\n\n# simulation parameters\nexperiment_parameters &lt;- expand_grid(\n  n_per_condition = seq(from = 10, to = 90, by = 10),\n  mean_control = 0,\n  mean_intervention = 0.5,\n  sd = 1,\n  iteration = 1:1000\n) \n\n# run simulation\nsimulation &lt;- experiment_parameters |&gt;\n  mutate(generated_data = pmap(list(n_per_condition, \n                                    mean_control,\n                                    mean_intervention,\n                                    sd),\n                               generate_data)) |&gt;\n  mutate(results = pmap(list(generated_data),\n                        analyze))",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Understanding the link between p-values, Confidence Intervals, and power</span>"
    ]
  },
  {
    "objectID": "chapters/p_values_and_confidence_intervals.html#cohens-d-by-sample-size",
    "href": "chapters/p_values_and_confidence_intervals.html#cohens-d-by-sample-size",
    "title": "14  Understanding the link between p-values, Confidence Intervals, and power",
    "section": "14.4 Cohen’s d by sample size",
    "text": "14.4 Cohen’s d by sample size\n\n\nCode\n# wrangle\nsimulation |&gt;\n  unnest(results) |&gt;\n  # plot\n  ggplot(aes(n_per_condition*2, cohens_d)) +\n  geom_jitter(alpha = 0.25) +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  scale_x_continuous(breaks = breaks_pretty(n = 10),\n                     name = \"Total N\") +\n  scale_y_continuous(breaks = breaks_pretty(n = 10),\n                     #limits = c(0,1),\n                     name = \"Cohen's d\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.3, end = 0.7) +\n  ggtitle(\"Population Cohen's d = 0.5\")\n\n\n\n\n\n\n\n\n\n\nEach point is a Cohen’s d from one dataset (i.e., one iteration).\nThe population Cohen’s d is 0.5. Notice how the distribution of sample Cohen’s d values vary by sample size: Cohen’s d values above 2.0 and below -0.5 are observed when sample size is very small. As sample size gets larger, they stabilize. However, individual datasets still generate Cohen’s d values that diverge substantially from the true value of 0.5.\n\n\n14.4.1 Ordered by statistical significance\nLet’s color the Cohen’s ds by their statistical significance.\n\n\nCode\n# wrangle\nsimulation |&gt;\n  unnest(results) |&gt;\n  mutate(significant = p &lt; .05) |&gt;\n  # plot\n  ggplot(aes(n_per_condition*2, cohens_d, color = significant)) +\n  geom_jitter(alpha = 0.25) +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  scale_x_continuous(breaks = breaks_pretty(n = 10),\n                     name = \"Total N\") +\n  scale_y_continuous(breaks = breaks_pretty(n = 10),\n                     #limits = c(0,1),\n                     name = \"Cohen's d\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.3, end = 0.7, direction = -1) +\n  ggtitle(\"Population Cohen's d = 0.5\")\n\n\n\n\n\n\n\n\n\n\nThere is clearly a cut-off value for each sample size that determines when a Cohen’s d value is associated with a significant vs. non-significant t-test p-value. What determines this cut-off? Do you have an intuition for it?\n\n\n\n14.4.2 Average effect size for significant effects\nQuick aside: Significant results are more likely to be published than non significant ones. If we only look at the significant results, what is the average effect size by N?\nPlot only the significant effect sizes, and add their means.\n\n\nCode\n# wrangle\nsimulation |&gt;\n  unnest(results) |&gt;\n  mutate(significant = p &lt; .05) |&gt;\n  filter(significant) |&gt;\n  # plot\n  ggplot(aes(n_per_condition*2, cohens_d, color = significant)) +\n  geom_jitter(alpha = 0.25) +\n  # add mean points \n  stat_summary(fun = \"mean\",\n               geom = \"point\",\n               size = 3,\n               color = \"#35608DFF\") +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  scale_x_continuous(breaks = breaks_pretty(n = 10),\n                     name = \"Total N\") +\n  scale_y_continuous(breaks = breaks_pretty(n = 10),\n                     #limits = c(0,1),\n                     name = \"Cohen's d\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.3, end = 0.7, direction = -1) +\n  ggtitle(\"Population Cohen's d = 0.5\")\n\n\n\n\n\n\n\n\n\n\nBack to the q: There is clearly a cut-off value for each sample size that determines when a Cohen’s d value is associated with a significant vs. non-significant t-test p-value. What determines this cut-off? Do you have an intuition for it?",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Understanding the link between p-values, Confidence Intervals, and power</span>"
    ]
  },
  {
    "objectID": "chapters/p_values_and_confidence_intervals.html#cohens-d-and-its-95-cis",
    "href": "chapters/p_values_and_confidence_intervals.html#cohens-d-and-its-95-cis",
    "title": "14  Understanding the link between p-values, Confidence Intervals, and power",
    "section": "14.5 Cohen’s d and its 95% CIs",
    "text": "14.5 Cohen’s d and its 95% CIs\nTo understand it further, let’s add the 95% Confidence Intervals.\n\n14.5.1 Randomly select one dataset per sample size\nTo understand the plot, we’ll plot just one data set (iteration) for each sample size.\nRe-run the chunk to select new random values. Notice how the magnitude of the Cohen’s d values ‘dance’ around between data sets due to random sampling error, but the width of the Confidence Intervals do not. Confidence width is steady as it is a function of sample size.\n\n\nCode\n# wrangle\nsimulation |&gt;\n  unnest(results) |&gt;\n  mutate(significant = p &lt; .05) |&gt;\n  # randomly sample 1 effects size per sample size\n  group_by(n_per_condition) |&gt;\n  slice_sample(n = 1) |&gt;\n  ungroup() |&gt;\n  # plot\n  ggplot(aes(n_per_condition*2, cohens_d, color = significant)) +\n  geom_point() +\n  # 95% CIs\n  geom_linerange(aes(ymin = cohens_d_ci_lower, ymax = cohens_d_ci_upper)) +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  scale_x_continuous(breaks = breaks_pretty(n = 10),\n                     name = \"Total N\") +\n  scale_y_continuous(breaks = breaks_pretty(n = 10),\n                     #limits = c(0,1),\n                     name = \"Cohen's d\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.3, end = 0.7) +\n  ggtitle(\"Population Cohen's d = 0.5\\nOne randomly selected dataset per sample size\")\n\n\n\n\n\n\n\n\n\n\nSignificant when 95% CI excludes zero effect size\n\n\n\n14.5.2 All datasets\nNow let’s plot all the data sets (iterations).\n\n\nCode\n# wrangle\nsimulation |&gt;\n  unnest(results) |&gt;\n  mutate(significant = p &lt; .05,\n         total_n = paste(\"N =\", n_per_condition*2),\n         total_n = fct_reorder(total_n, as.numeric(str_extract(total_n, \"\\\\d+\")))) |&gt;\n  # plot\n  ggplot(aes(iteration, cohens_d, color = significant)) +\n  geom_point(alpha = 0.5) +\n  geom_linerange(aes(ymin = cohens_d_ci_lower, ymax = cohens_d_ci_upper), alpha = 0.2) +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  scale_x_continuous(breaks = breaks_pretty(n = 10),\n                     name = \"Iteration\") +\n  scale_y_continuous(breaks = breaks_pretty(n = 10),\n                     #limits = c(0,1),\n                     name = \"Cohen's d\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.3, end = 0.7) +\n  facet_wrap(~ total_n, ncol = 3, scales = \"free_y\")\n\n\n\n\n\n\n\n\n\n\n\n14.5.3 Ordered by Cohen’s d\nThe above plot is hard to understand. Let’s order the Cohen’s ds from smallest to largest, so the small ones are on the left and the large ones are on the right.\n\n\nCode\n# wrangle\nsimulation |&gt;\n  unnest(results) |&gt;\n  mutate(significant = p &lt; .05,\n         total_n = paste(\"N =\", n_per_condition*2),\n         total_n = fct_reorder(total_n, as.numeric(str_extract(total_n, \"\\\\d+\")))) |&gt;\n  arrange(n_per_condition, cohens_d) |&gt;\n  group_by(n_per_condition) |&gt;\n  mutate(rank = row_number()) |&gt;\n  ungroup() |&gt;\n  # plot\n  ggplot(aes(rank, cohens_d, color = significant)) +\n  geom_point() +\n  geom_linerange(aes(ymin = cohens_d_ci_lower, ymax = cohens_d_ci_upper), alpha = 0.2) +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  scale_x_continuous(breaks = breaks_pretty(n = 10),\n                     name = \"Ranked iteration\") +\n  scale_y_continuous(breaks = breaks_pretty(n = 10),\n                     #limits = c(0,1),\n                     name = \"Cohen's d\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.3, end = 0.7) +\n  facet_wrap(~ total_n, ncol = 3)\n\n\n\n\n\n\n\n\n\n\nNotice the relationship between 95% CI and p value significance.\nA certain percentage of Cohen’s ds are green vs. blue. What is this percentage also called? What statistical property?",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Understanding the link between p-values, Confidence Intervals, and power</span>"
    ]
  },
  {
    "objectID": "chapters/p_values_and_confidence_intervals.html#mean-cohens-d-and-mean-interval-width-by-sample-size",
    "href": "chapters/p_values_and_confidence_intervals.html#mean-cohens-d-and-mean-interval-width-by-sample-size",
    "title": "14  Understanding the link between p-values, Confidence Intervals, and power",
    "section": "14.6 Mean Cohen’s d and mean interval width by sample size",
    "text": "14.6 Mean Cohen’s d and mean interval width by sample size\nNow let’s average over the Cohen’s ds in each condition to find the mean Cohen’s d and its mean 95% CIs.\n\n\nCode\n# wrangle\nsimulation_summary &lt;- simulation |&gt;\n  # unnest results\n  unnest(results) |&gt;\n  group_by(n_per_condition) |&gt;\n  summarize(proportion_significant = mean(p &lt; .05), \n            mean_cohens_d = mean(cohens_d),\n            mean_cohens_d_ci_lower = mean(cohens_d_ci_lower),\n            mean_cohens_d_ci_upper = mean(cohens_d_ci_upper)) |&gt;\n  mutate(centered_mean_cohens_d_ci_lower = mean_cohens_d_ci_lower - mean_cohens_d,\n         centered_mean_cohens_d_ci_upper = mean_cohens_d_ci_upper - mean_cohens_d)\n\n# plot results\np1 &lt;- ggplot(simulation_summary, aes(n_per_condition*2, mean_cohens_d)) +\n  geom_point() +\n  geom_linerange(aes(ymin = mean_cohens_d_ci_lower, ymax = mean_cohens_d_ci_upper)) +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  scale_x_continuous(breaks = breaks_pretty(n = 10),\n                     name = \"Total sample size\") +\n  scale_y_continuous(breaks = breaks_pretty(n = 5),\n                     #limits = c(0,1),\n                     name = \"Mean Cohen's d\\n(and mean 95% CIs)\") +\n  theme_linedraw() +\n  ggtitle(\"Population Cohen's d = 0.5\")\n\np1",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Understanding the link between p-values, Confidence Intervals, and power</span>"
    ]
  },
  {
    "objectID": "chapters/p_values_and_confidence_intervals.html#mean-cohens-d-and-power-by-sample-size",
    "href": "chapters/p_values_and_confidence_intervals.html#mean-cohens-d-and-power-by-sample-size",
    "title": "14  Understanding the link between p-values, Confidence Intervals, and power",
    "section": "14.7 Mean Cohen’s d and power by sample size",
    "text": "14.7 Mean Cohen’s d and power by sample size\n\n\nCode\np2 &lt;- ggplot(simulation_summary, aes(n_per_condition*2, proportion_significant)) +\n  geom_point() +\n  geom_hline(yintercept = 0.05, linetype = \"dotted\") +\n  geom_hline(yintercept = 0.80, linetype = \"dotted\") +\n  scale_x_continuous(breaks = breaks_pretty(n = 10),\n                     name = \"Total sample size\") +\n  scale_y_continuous(breaks = breaks_pretty(n = 5),\n                     limits = c(0,1),\n                     name = \"Proportion of significant\\np-values\") +\n  theme_linedraw() \n\np1 + p2 + plot_layout(ncol = 1)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Understanding the link between p-values, Confidence Intervals, and power</span>"
    ]
  },
  {
    "objectID": "chapters/p_values_and_confidence_intervals.html#exercise-power-for-equivalence-test",
    "href": "chapters/p_values_and_confidence_intervals.html#exercise-power-for-equivalence-test",
    "title": "14  Understanding the link between p-values, Confidence Intervals, and power",
    "section": "14.8 Exercise: Power for equivalence test",
    "text": "14.8 Exercise: Power for equivalence test\nSee Lakens et al., 2018 for a tutorial on the concepts below.\nUsing a Smallest Effect Size of Interest (SESOI) of Cohen’s d = 0.2, what is the power of a Two One-Sided Equivalence Test (TOST) for different sample sizes? What N is needed for 80% power to detect a true null effect size as equivalent to zero?\nNote: because reasons, use the 90% Confidence Interval instead of 95 (see Lakens et al., 2018).\n\n\nCode\nif(file.exists(\"materials/lakens et al 2018 figure 1.png\")){\n  knitr::include_graphics(\"materials/lakens et al 2018 figure 1.png\")\n}",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Understanding the link between p-values, Confidence Intervals, and power</span>"
    ]
  },
  {
    "objectID": "chapters/difference_between_significant_and_nonsignificant.html",
    "href": "chapters/difference_between_significant_and_nonsignificant.html",
    "title": "15  The differences between significant and non-significant is not itself significant",
    "section": "",
    "text": "15.1 Dependencies\nCode\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(purrr) \nlibrary(stringr)\nlibrary(forcats)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(janitor)\nlibrary(effsize)\nlibrary(metafor)\n\nformat_p_apa &lt;- function(p, threshold = 0.001) {\n  ifelse(\n    p &lt; threshold,\n    paste0(\"p &lt; \", sub(\"^0\\\\.\", \".\", format(threshold, nsmall = 3))),\n    paste0(\"p = \", sub(\"^0\\\\.\", \".\", format(round(p, 3), nsmall = 3)))\n  )\n}",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>The differences between significant and non-significant is not itself significant</span>"
    ]
  },
  {
    "objectID": "chapters/difference_between_significant_and_nonsignificant.html#simulation-functions",
    "href": "chapters/difference_between_significant_and_nonsignificant.html#simulation-functions",
    "title": "15  The differences between significant and non-significant is not itself significant",
    "section": "15.2 Simulation functions",
    "text": "15.2 Simulation functions\n\n\nCode\n# functions for simulation\ngenerate_data &lt;- function(n_per_condition,\n                          population_cohens_d) {\n  \n  data_control &lt;- \n    tibble(condition = \"control\",\n           score = rnorm(n = n_per_condition, mean = 0, sd = 1))\n  \n  data_intervention &lt;- \n    tibble(condition = \"intervention\",\n           score = rnorm(n = n_per_condition, mean = population_cohens_d, sd = 1))\n  \n  data_combined &lt;- bind_rows(data_control,\n                             data_intervention) |&gt;\n    mutate(condition = fct_relevel(condition, \"intervention\", \"control\"))\n  \n  return(data_combined)\n}\n\nanalyze &lt;- function(data, study) {\n\n  res_t_test &lt;- t.test(formula = score ~ condition, \n                       data = data,\n                       var.equal = TRUE,\n                       alternative = \"two.sided\")\n  \n  res_cohens_d &lt;- effsize::cohen.d(formula = score ~ condition,\n                                   data = data,\n                                   pooled = TRUE)\n  \n  res &lt;- tibble(study = study,\n                p = res_t_test$p.value, \n                cohens_d = res_cohens_d$estimate,\n                cohens_d_ci_lower = res_cohens_d$conf.int[1],\n                cohens_d_ci_upper = res_cohens_d$conf.int[2],\n                cohens_d_se = sqrt(res_cohens_d$var))\n\n  return(res)\n}\n\nmeta_analyze &lt;- function(results_study_1, results_study_2) {\n  \n  dat &lt;- bind_rows(results_study_1,\n                   results_study_2)\n  \n  fit &lt;- rma(data = dat, \n             yi = cohens_d, \n             sei = cohens_d_se, \n             mods = ~ study, \n             method = \"FE\")\n\n  res &lt;- tibble(cohens_d = fit$b[2],\n                cohens_d_ci_lower = fit$ci.lb[2],\n                cohens_d_ci_upper = fit$ci.ub[2],\n                #df = fit$QMdf[1],\n                #Q = fit$QM,\n                p = fit$QMp) # p of moderation\n    #mutate(formatted = paste0(\"Q(\", df, \") = \", round_half_up(Q, 3), \", \", format_p_apa(p)))\n\n  return(res)\n}",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>The differences between significant and non-significant is not itself significant</span>"
    ]
  },
  {
    "objectID": "chapters/difference_between_significant_and_nonsignificant.html#different-population-effect-large-n",
    "href": "chapters/difference_between_significant_and_nonsignificant.html#different-population-effect-large-n",
    "title": "15  The differences between significant and non-significant is not itself significant",
    "section": "15.3 Different population effect, large N",
    "text": "15.3 Different population effect, large N\n\n\nCode\n# set seed\nset.seed(42)\n\n# simulation parameters\nexperiment_parameters &lt;- expand_grid(\n  study_1_n_per_condition = 300,\n  study_1_population_cohens_d = 0.5,\n  study_2_n_per_condition = 300,\n  study_2_population_cohens_d = 0,\n  iteration = 1:1000\n) \n\n# run simulation\nsimulation &lt;- experiment_parameters |&gt;\n  mutate(generated_data_study_1 = pmap(list(n_per_condition = study_1_n_per_condition, \n                                            population_cohens_d = study_1_population_cohens_d),\n                                       generate_data)) |&gt;\n  mutate(generated_data_study_2 = pmap(list(n_per_condition = study_2_n_per_condition, \n                                            population_cohens_d = study_2_population_cohens_d),\n                                       generate_data)) |&gt;\n  mutate(results_study1 = pmap(list(data = generated_data_study_1),\n                                analyze,\n                                study = \"study 1\")) |&gt;\n  mutate(results_study2 = pmap(list(data = generated_data_study_2),\n                                analyze,\n                                study = \"study 2\")) |&gt;\n  mutate(results_meta = pmap(list(results_study_1 = results_study1,\n                                  results_study_2 = results_study2),\n                             meta_analyze)) |&gt;\n  # wrangle results\n  unnest(results_study1, names_sep = \"_\") |&gt;\n  unnest(results_study2, names_sep = \"_\") |&gt;\n  unnest(results_meta, names_sep = \"_\") |&gt;\n  select(-generated_data_study_1, -generated_data_study_2) |&gt;\n  # this is a complex pivot, don't worry if you don't immediately understand it\n  pivot_longer(\n    cols = c(\n      results_study1_p,\n      results_study1_cohens_d,\n      results_study1_cohens_d_ci_lower,\n      results_study1_cohens_d_ci_upper,\n      results_study1_cohens_d_se,\n      results_study2_p,\n      results_study2_cohens_d,\n      results_study2_cohens_d_se,\n      results_study2_cohens_d_ci_lower,\n      results_study2_cohens_d_ci_upper,\n      results_meta_p,\n      results_meta_cohens_d,\n      # results_meta_cohens_d_se, # not created\n      results_meta_cohens_d_ci_lower,\n      results_meta_cohens_d_ci_upper\n    ),\n    names_to = c(\"source\", \"metric\"),\n    names_pattern = \"results_(study\\\\d+|meta)_(.+)\",\n    values_to = \"value\"\n  ) |&gt;\n  pivot_wider(\n    names_from = metric,\n    values_from = value\n  )\n\nsimulation_summary &lt;- simulation |&gt;\n  group_by(study_1_n_per_condition,\n           study_1_population_cohens_d,\n           study_2_n_per_condition,\n           study_2_population_cohens_d,\n           source) |&gt;\n  summarize(proportion_significant = mean(p &lt; .05),\n            mean_cohens_d = mean(cohens_d),\n            mean_cohens_d_ci_lower = mean(cohens_d_ci_lower),\n            mean_cohens_d_ci_upper = mean(cohens_d_ci_upper),\n            .groups = \"drop\") |&gt;\n  mutate(source = str_replace(source, \"meta\", \"difference\")) |&gt;\n  mutate(source = fct_relevel(source, \"study1\", \"study2\", \"difference\"))\n\n\np_effect_size &lt;- ggplot(simulation_summary, aes(source, mean_cohens_d)) +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  geom_linerange(aes(ymin = mean_cohens_d_ci_lower, ymax = mean_cohens_d_ci_upper)) +\n  geom_point() +\n  scale_y_continuous(breaks = breaks_pretty(n = 10),\n                     name = \"Mean Cohen's d\") +\n  xlab(\"\") +\n  theme_linedraw() +\n  theme(panel.grid.minor = element_blank())\n\np_proportion_significant &lt;- ggplot(simulation_summary, aes(source, proportion_significant)) +\n  geom_hline(yintercept = 0.05, linetype = \"dashed\") +\n  geom_hline(yintercept = 0.80, linetype = \"dashed\") +\n  geom_bar(stat = \"identity\") +\n  scale_y_continuous(breaks = breaks_pretty(n = 10), limits = c(0, 1), name = \"Proportion of significant p values\") +\n  xlab(\"\") +\n  # geom_text(aes(x = \"study1\", y = 1.1, label = paste(\"Population\\nCohen's d =\", study_1_population_cohens_d))) +\n  # geom_text(aes(x = \"study2\", y = 1.1, label = paste(\"Population\\nCohen's d =\", study_2_population_cohens_d))) +\n  # geom_text(aes(x = \"difference\", y = 1.1, label = paste(\"Population\\nCohen's d =\", study_2_population_cohens_d - study_1_population_cohens_d))) +\n  theme_linedraw() +\n  theme(panel.grid.minor = element_blank())\n\np_effect_size + p_proportion_significant + plot_layout(ncol = 2)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>The differences between significant and non-significant is not itself significant</span>"
    ]
  },
  {
    "objectID": "chapters/difference_between_significant_and_nonsignificant.html#different-population-effect-small-n",
    "href": "chapters/difference_between_significant_and_nonsignificant.html#different-population-effect-small-n",
    "title": "15  The differences between significant and non-significant is not itself significant",
    "section": "15.4 Different population effect, small N",
    "text": "15.4 Different population effect, small N\n\n\nCode\n# set seed\nset.seed(42)\n\n# simulation parameters\nexperiment_parameters &lt;- expand_grid(\n  study_1_n_per_condition = 40,\n  study_1_population_cohens_d = 0.5,\n  study_2_n_per_condition = 40,\n  study_2_population_cohens_d = 0,\n  iteration = 1:1000\n) \n\n# run simulation\nsimulation &lt;- experiment_parameters |&gt;\n  mutate(generated_data_study_1 = pmap(list(n_per_condition = study_1_n_per_condition, \n                                            population_cohens_d = study_1_population_cohens_d),\n                                       generate_data)) |&gt;\n  mutate(generated_data_study_2 = pmap(list(n_per_condition = study_2_n_per_condition, \n                                            population_cohens_d = study_2_population_cohens_d),\n                                       generate_data)) |&gt;\n  mutate(results_study1 = pmap(list(data = generated_data_study_1),\n                                analyze,\n                                study = \"study 1\")) |&gt;\n  mutate(results_study2 = pmap(list(data = generated_data_study_2),\n                                analyze,\n                                study = \"study 2\")) |&gt;\n  mutate(results_meta = pmap(list(results_study_1 = results_study1,\n                                  results_study_2 = results_study2),\n                             meta_analyze)) |&gt;\n  # wrangle results\n  unnest(results_study1, names_sep = \"_\") |&gt;\n  unnest(results_study2, names_sep = \"_\") |&gt;\n  unnest(results_meta, names_sep = \"_\") |&gt;\n  select(-generated_data_study_1, -generated_data_study_2) |&gt;\n  # this is a complex pivot, don't worry if you don't immediately understand it\n  pivot_longer(\n    cols = c(\n      results_study1_p,\n      results_study1_cohens_d,\n      results_study1_cohens_d_ci_lower,\n      results_study1_cohens_d_ci_upper,\n      results_study1_cohens_d_se,\n      results_study2_p,\n      results_study2_cohens_d,\n      results_study2_cohens_d_se,\n      results_study2_cohens_d_ci_lower,\n      results_study2_cohens_d_ci_upper,\n      results_meta_p,\n      results_meta_cohens_d,\n      # results_meta_cohens_d_se, # not created\n      results_meta_cohens_d_ci_lower,\n      results_meta_cohens_d_ci_upper\n    ),\n    names_to = c(\"source\", \"metric\"),\n    names_pattern = \"results_(study\\\\d+|meta)_(.+)\",\n    values_to = \"value\"\n  ) |&gt;\n  pivot_wider(\n    names_from = metric,\n    values_from = value\n  )\n\nsimulation_summary &lt;- simulation |&gt;\n  group_by(study_1_n_per_condition,\n           study_1_population_cohens_d,\n           study_2_n_per_condition,\n           study_2_population_cohens_d,\n           source) |&gt;\n  summarize(proportion_significant = mean(p &lt; .05),\n            mean_cohens_d = mean(cohens_d),\n            mean_cohens_d_ci_lower = mean(cohens_d_ci_lower),\n            mean_cohens_d_ci_upper = mean(cohens_d_ci_upper),\n            .groups = \"drop\") |&gt;\n  mutate(source = str_replace(source, \"meta\", \"difference\")) |&gt;\n  mutate(source = fct_relevel(source, \"study1\", \"study2\", \"difference\"))\n\n\np_effect_size &lt;- ggplot(simulation_summary, aes(source, mean_cohens_d)) +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  geom_linerange(aes(ymin = mean_cohens_d_ci_lower, ymax = mean_cohens_d_ci_upper)) +\n  geom_point() +\n  scale_y_continuous(breaks = breaks_pretty(n = 10),\n                     name = \"Mean Cohen's d\") +\n  xlab(\"\") +\n  theme_linedraw() +\n  theme(panel.grid.minor = element_blank())\n\np_proportion_significant &lt;- ggplot(simulation_summary, aes(source, proportion_significant)) +\n  geom_hline(yintercept = 0.05, linetype = \"dashed\") +\n  geom_hline(yintercept = 0.80, linetype = \"dashed\") +\n  geom_bar(stat = \"identity\") +\n  scale_y_continuous(breaks = breaks_pretty(n = 10), limits = c(0, 1), name = \"Proportion of significant p values\") +\n  xlab(\"\") +\n  # geom_text(aes(x = \"study1\", y = 1.1, label = paste(\"Population\\nCohen's d =\", study_1_population_cohens_d))) +\n  # geom_text(aes(x = \"study2\", y = 1.1, label = paste(\"Population\\nCohen's d =\", study_2_population_cohens_d))) +\n  # geom_text(aes(x = \"difference\", y = 1.1, label = paste(\"Population\\nCohen's d =\", study_2_population_cohens_d - study_1_population_cohens_d))) +\n  theme_linedraw() +\n  theme(panel.grid.minor = element_blank())\n\np_effect_size + p_proportion_significant + plot_layout(ncol = 2)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>The differences between significant and non-significant is not itself significant</span>"
    ]
  },
  {
    "objectID": "chapters/difference_between_significant_and_nonsignificant.html#different-population-effect-small-original-study-and-large-replication",
    "href": "chapters/difference_between_significant_and_nonsignificant.html#different-population-effect-small-original-study-and-large-replication",
    "title": "15  The differences between significant and non-significant is not itself significant",
    "section": "15.5 Different population effect, small original study and large “replication”",
    "text": "15.5 Different population effect, small original study and large “replication”\nNB not really a replication as the second study has a different effect size, but its a very rough simulation assuming p hacking took place in the original study and boosted the effect size\n[needs work to be clearer]\n\n\nCode\n# set seed\nset.seed(42)\n\n# simulation parameters\nexperiment_parameters &lt;- expand_grid(\n  study_1_n_per_condition = 40,\n  study_1_population_cohens_d = 0.5,\n  study_2_n_per_condition = 200,\n  study_2_population_cohens_d = 0,\n  iteration = 1:1000\n) \n\n# run simulation\nsimulation &lt;- experiment_parameters |&gt;\n  mutate(generated_data_study_1 = pmap(list(n_per_condition = study_1_n_per_condition, \n                                            population_cohens_d = study_1_population_cohens_d),\n                                       generate_data)) |&gt;\n  mutate(generated_data_study_2 = pmap(list(n_per_condition = study_2_n_per_condition, \n                                            population_cohens_d = study_2_population_cohens_d),\n                                       generate_data)) |&gt;\n  mutate(results_study1 = pmap(list(data = generated_data_study_1),\n                                analyze,\n                                study = \"study 1\")) |&gt;\n  mutate(results_study2 = pmap(list(data = generated_data_study_2),\n                                analyze,\n                                study = \"study 2\")) |&gt;\n  mutate(results_meta = pmap(list(results_study_1 = results_study1,\n                                  results_study_2 = results_study2),\n                             meta_analyze)) |&gt;\n  # wrangle results\n  unnest(results_study1, names_sep = \"_\") |&gt;\n  unnest(results_study2, names_sep = \"_\") |&gt;\n  unnest(results_meta, names_sep = \"_\") |&gt;\n  select(-generated_data_study_1, -generated_data_study_2) |&gt;\n  # this is a complex pivot, don't worry if you don't immediately understand it\n  pivot_longer(\n    cols = c(\n      results_study1_p,\n      results_study1_cohens_d,\n      results_study1_cohens_d_ci_lower,\n      results_study1_cohens_d_ci_upper,\n      results_study1_cohens_d_se,\n      results_study2_p,\n      results_study2_cohens_d,\n      results_study2_cohens_d_se,\n      results_study2_cohens_d_ci_lower,\n      results_study2_cohens_d_ci_upper,\n      results_meta_p,\n      results_meta_cohens_d,\n      # results_meta_cohens_d_se, # not created\n      results_meta_cohens_d_ci_lower,\n      results_meta_cohens_d_ci_upper\n    ),\n    names_to = c(\"source\", \"metric\"),\n    names_pattern = \"results_(study\\\\d+|meta)_(.+)\",\n    values_to = \"value\"\n  ) |&gt;\n  pivot_wider(\n    names_from = metric,\n    values_from = value\n  )\n\nsimulation_summary &lt;- simulation |&gt;\n  group_by(study_1_n_per_condition,\n           study_1_population_cohens_d,\n           study_2_n_per_condition,\n           study_2_population_cohens_d,\n           source) |&gt;\n  summarize(proportion_significant = mean(p &lt; .05),\n            mean_cohens_d = mean(cohens_d),\n            mean_cohens_d_ci_lower = mean(cohens_d_ci_lower),\n            mean_cohens_d_ci_upper = mean(cohens_d_ci_upper),\n            .groups = \"drop\") |&gt;\n  mutate(source = str_replace(source, \"meta\", \"difference\")) |&gt;\n  mutate(source = fct_relevel(source, \"study1\", \"study2\", \"difference\"))\n\n\np_effect_size &lt;- ggplot(simulation_summary, aes(source, mean_cohens_d)) +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  geom_linerange(aes(ymin = mean_cohens_d_ci_lower, ymax = mean_cohens_d_ci_upper)) +\n  geom_point() +\n  scale_y_continuous(breaks = breaks_pretty(n = 10),\n                     name = \"Mean Cohen's d\") +\n  xlab(\"\") +\n  theme_linedraw() +\n  theme(panel.grid.minor = element_blank())\n\np_proportion_significant &lt;- ggplot(simulation_summary, aes(source, proportion_significant)) +\n  geom_hline(yintercept = 0.05, linetype = \"dashed\") +\n  geom_hline(yintercept = 0.80, linetype = \"dashed\") +\n  geom_bar(stat = \"identity\") +\n  scale_y_continuous(breaks = breaks_pretty(n = 10), limits = c(0, 1), name = \"Proportion of significant p values\") +\n  xlab(\"\") +\n  # geom_text(aes(x = \"study1\", y = 1.1, label = paste(\"Population\\nCohen's d =\", study_1_population_cohens_d))) +\n  # geom_text(aes(x = \"study2\", y = 1.1, label = paste(\"Population\\nCohen's d =\", study_2_population_cohens_d))) +\n  # geom_text(aes(x = \"difference\", y = 1.1, label = paste(\"Population\\nCohen's d =\", study_2_population_cohens_d - study_1_population_cohens_d))) +\n  theme_linedraw() +\n  theme(panel.grid.minor = element_blank())\n\np_effect_size + p_proportion_significant + plot_layout(ncol = 1)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>The differences between significant and non-significant is not itself significant</span>"
    ]
  },
  {
    "objectID": "chapters/standardized_effect_sizes_and_range_restriction.html",
    "href": "chapters/standardized_effect_sizes_and_range_restriction.html",
    "title": "16  Standardized effect sizes and range restriction",
    "section": "",
    "text": "16.1 Dependencies\nCode\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(sn)\nlibrary(janitor)\nlibrary(effsize)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(faux)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Standardized effect sizes and range restriction</span>"
    ]
  },
  {
    "objectID": "chapters/standardized_effect_sizes_and_range_restriction.html#load-data",
    "href": "chapters/standardized_effect_sizes_and_range_restriction.html#load-data",
    "title": "16  Standardized effect sizes and range restriction",
    "section": "16.2 Load data",
    "text": "16.2 Load data\nReal BDI-II data is taken from Cataldo et al. (2022) Abnormal Evidence Accumulation Underlies the Positive Memory Deficit in Depression, doi: 10.1037/xge0001268.\n\n\nCode\ndata_bdi &lt;- read_csv(\"../data/bdi_data.csv\")",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Standardized effect sizes and range restriction</span>"
    ]
  },
  {
    "objectID": "chapters/standardized_effect_sizes_and_range_restriction.html#why-standardize",
    "href": "chapters/standardized_effect_sizes_and_range_restriction.html#why-standardize",
    "title": "16  Standardized effect sizes and range restriction",
    "section": "16.3 Why standardize?",
    "text": "16.3 Why standardize?\nThey have different possible ranges, different population means (\\(\\mu\\)), and different population SDs (\\(\\sigma\\)).\nEven if had perfect that a given therapy has a (population) efficacy of lowering BDI-II depression scores by 6 points, without knowing a lot about the relationships between the BDI-II and other scores, we know little about how many points the same therapy would affect depression scores on the MADRS or the HAM-D.\n(surprisingly, very little work is ever done to collect information on the relationship between different scores so that we could know this)\nImagine three different published RCTs, each of which studied the efficacy of the same form of cognitive behavioral therapy for depression:\n\nRCT 1 found that it lowered depression scores on the BDI-II by 6 points on average\nRCT 2 found that it lowered depression scores on the MADRS by 8 points on average\nRCT 3 found that it lowered depression scores on the HAM-D by 4 points on average\n\nWhat is the efficacy of the intervention for depression scores on the PHQ-9? This is impossible to answer without knowing a lot about the details of the different scales (e.g., their min/max scores), the distribution of each scale’s scores in the population (eg population \\(\\mu\\) and \\(\\sigma\\)), and the relationship between different depression scales in the population. A one-point-change on one scale likely has a very different meaning to a one-point-change on another scale.\nWhat is the efficacy of the intervention for depression in general? This too is impossible to answer as there is no common scale between them.\n‘Standardized’ effect sizes are useful here as they provide common units. Instead of points on the self-report scale (i.e., sum scores), which differ between scales, standardized effect sizes generally use Standard Deviations as their units. For example, Cohen’s d = 0.2 means that there are 0.2 Standard Deviations of difference between the two groups.\nIn principle, standardized effect sizes are extremely useful as they allow us to draw comparisons between studies using very different outcome measures, or indeed to synthesise results between such studies (i.e., meta-analysis).\n\n16.3.1 Visualise\nSemi-realistic depression scores on different scales.\n\n\nCode\nN &lt;- 10000\n\ngenerated_data &lt;- \n  bind_rows(\n    tibble(measure = \"BDI-II\",\n           score = rsn(n = N, \n                       xi = 2,  # location\n                       omega = 15, # scale\n                       alpha = 16),\n           max_score = 63), # skew\n    tibble(measure = \"HAM-D\",\n           score = rsn(n = N, \n                       xi = 33,  # location\n                       omega = 7, # scale\n                       alpha = -1),\n           max_score = 52), # skew\n    tibble(measure = \"MADRS\",\n           score = rsn(n = N, \n                       xi = 7,  # location\n                       omega = 7, # scale\n                       alpha = 9),\n           max_score = 60) # skew\n  ) |&gt;\n  mutate(score = case_when(score &lt; 0 ~ 0,\n                           score &gt; max_score ~ max_score,\n                           TRUE ~ score))\n\n\nggplot(generated_data, aes(score)) +\n  geom_vline(aes(xintercept = 0), linetype = \"dotted\") +\n  geom_vline(aes(xintercept = max_score), linetype = \"dotted\") +\n  geom_histogram(boundary = 0) +\n  scale_x_continuous(breaks = breaks_pretty(n = 10)) +\n  facet_wrap(~ measure, ncol = 1, scales = \"free_y\") +\n  theme_linedraw() +\n  ylab(\"Frequency\") +\n  xlab(\"Sum score\")\n\n\n\n\n\n\n\n\n\nFor the moment, let’s pretend like these scales produce continuous normal data that only differ in their population location (\\(\\mu\\)) and scale (\\(\\sigma\\)):\n\n\nCode\ngenerated_data &lt;- \n  bind_rows(\n    tibble(measure = \"BDI-II\",\n           score = rnorm(n = N, mean = 7, sd = 9),\n           max_score = 63),\n    tibble(measure = \"HAM-D\",\n           score = rnorm(n = N, mean = 12, sd = 4),\n           max_score = 52),\n    tibble(measure = \"MADRS\",\n           score = rnorm(n = N, mean = 10, sd = 8),\n           max_score = 60)\n  ) \n\nggplot(generated_data, aes(score)) +\n  geom_histogram() +\n  scale_x_continuous(breaks = breaks_pretty(n = 10)) +\n  facet_wrap(~ measure, ncol = 1) +\n  theme_linedraw() +\n  ylab(\"Frequency\") +\n  xlab(\"Sum score\")\n\n\n\n\n\n\n\n\n\nA one-point change on the BDI-II still means something very different to a one-point change on the MADRS or HAM-D.\nData for a single sample can be standardized by taking each participant’s score, deducting the mean score (the sample estimate of \\(\\mu\\)), and then dividing by the SD of scores (the sample estimate of \\(\\sigma\\)). Now, all scales have a mean of 0 and an SD of 1. A one-point change on any scale has the same interpretation: a one-standard deviation change on that scale’s scores:\n\n\nCode\ngenerated_data &lt;- \n  bind_rows(\n    tibble(measure = \"BDI-II\",\n           score = rnorm(n = N, mean = 0, sd = 1),\n           max_score = 63),\n    tibble(measure = \"HAM-D\",\n           score = rnorm(n = N, mean = 0, sd = 1),\n           max_score = 52),\n    tibble(measure = \"MADRS\",\n           score = rnorm(n = N, mean = 0, sd = 1),\n           max_score = 60)\n  ) \n\nggplot(generated_data, aes(score)) +\n  geom_histogram() +\n  scale_x_continuous(breaks = breaks_pretty(n = 10)) +\n  facet_wrap(~ measure, ncol = 1) +\n  theme_linedraw() +\n  ylab(\"Frequency\") +\n  xlab(\"Standaridized scores\\n(score - mean)/SD\")\n\n\n\n\n\n\n\n\n\nYay, now we have scores that can be compared between scales, e.g., in a meta-analysis.\nHow can this go wrong?",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Standardized effect sizes and range restriction</span>"
    ]
  },
  {
    "objectID": "chapters/standardized_effect_sizes_and_range_restriction.html#influence-of-preselection-on-cohens-d",
    "href": "chapters/standardized_effect_sizes_and_range_restriction.html#influence-of-preselection-on-cohens-d",
    "title": "16  Standardized effect sizes and range restriction",
    "section": "16.4 Influence of preselection on Cohen’s d",
    "text": "16.4 Influence of preselection on Cohen’s d\nNote that in the below, only data at pre is real BDI-II data. Data at post is modified data (i.e., offset by known amounts).\n\n16.4.1 Example 1\n\n16.4.1.1 Wrangle/simulate\n\n\nCode\nset.seed(42)\n\nsubset_no_preselection &lt;- data_bdi |&gt;\n  rename(control = bdi_score) |&gt;\n  # simulate a 'intervention' score that is 5 points lower than pre\n  mutate(intervention = control - 5) |&gt;\n  # sample 100 participants from the real data \n  slice_sample(n = 100) |&gt;\n  mutate(recruitment = \"General population\") |&gt;\n  # reshape\n  pivot_longer(cols = c(control, intervention),\n               names_to = \"condition\",\n               values_to = \"bdi_score\") |&gt;\n  mutate(condition = fct_relevel(condition, \"control\", \"intervention\"))\n\n\nsubset_preselection_for_severe &lt;- data_bdi |&gt;\n  rename(control = bdi_score) |&gt;\n  # simulate recruitment into the study requiring a score of 29 or more at pre (\"severe\" depression according to the BDI-II manual)\n  filter(control &gt;= 29) |&gt;\n  # simulate a 'intervention' score that is 5 points lower than pre\n  mutate(intervention = control - 5) |&gt;\n  # sample 100 participants from the real data \n  slice_sample(n = 100) |&gt;\n  mutate(recruitment = \"'Severe' depression\") |&gt;\n  # reshape\n  pivot_longer(cols = c(control, intervention),\n               names_to = \"condition\",\n               values_to = \"bdi_score\") |&gt;\n  mutate(condition = fct_relevel(condition, \"control\", \"intervention\"))\n\n\n\n\n16.4.1.2 Plot\n\n\nCode\nbind_rows(subset_no_preselection,\n          subset_preselection_for_severe) |&gt;\n  mutate(recruitment = fct_relevel(recruitment, \"General population\", \"'Severe' depression\")) |&gt;\n  ## plot\n  ggplot(aes(bdi_score)) +\n  geom_histogram(boundary = 0, bins = 21) +\n  scale_fill_viridis_d(begin = 0.3, end = 0.7) +\n  theme_linedraw() +\n  coord_cartesian(xlim = c(-5, 63)) +\n  facet_grid(condition ~ recruitment) +\n  xlab(\"BDI-II sum score\") +\n  ylab(\"Frequency\")\n\n\n\n\n\n\n\n\n\n\n\n16.4.1.3 Analyze\nExercise:\nFor each of the two datasets, please calculate:\n\nThe unstandardized difference in means between the groups. To do this, calculate the mean BDI-II score in each condition (control vs intervention) and then the difference between the two means.\n\nThe standardized mean difference (Cohen’s d) between the two groups (e.g., using effsize::cohen.d()).\n\nDoes the intervention work? Think about the simulated population effect.\n\n\nCode\n# datasets:\nsubset_no_preselection\n\n\n# A tibble: 200 × 4\n        id recruitment        condition    bdi_score\n     &lt;dbl&gt; &lt;chr&gt;              &lt;fct&gt;            &lt;dbl&gt;\n 1 2527472 General population control              2\n 2 2527472 General population intervention        -3\n 3 2516002 General population control              9\n 4 2516002 General population intervention         4\n 5 2553222 General population control             33\n 6 2553222 General population intervention        28\n 7 2551678 General population control             26\n 8 2551678 General population intervention        21\n 9 2555281 General population control             13\n10 2555281 General population intervention         8\n# ℹ 190 more rows\n\n\nCode\nsubset_preselection_for_severe\n\n\n# A tibble: 200 × 4\n        id recruitment         condition    bdi_score\n     &lt;dbl&gt; &lt;chr&gt;               &lt;fct&gt;            &lt;dbl&gt;\n 1 2519655 'Severe' depression control             41\n 2 2519655 'Severe' depression intervention        36\n 3 2551157 'Severe' depression control             32\n 4 2551157 'Severe' depression intervention        27\n 5 2514051 'Severe' depression control             40\n 6 2514051 'Severe' depression intervention        35\n 7 2513857 'Severe' depression control             42\n 8 2513857 'Severe' depression intervention        37\n 9 2559761 'Severe' depression control             38\n10 2559761 'Severe' depression intervention        33\n# ℹ 190 more rows\n\n\nSolution\n\n\nCode\nsubset_no_preselection |&gt;\n  group_by(condition) |&gt;\n  summarize(mean_bdi_score = mean(bdi_score)) |&gt;\n  pivot_wider(names_from = condition,\n              values_from = mean_bdi_score) |&gt;\n  mutate(mean_diff = intervention - control)\n\n\n# A tibble: 1 × 3\n  control intervention mean_diff\n    &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1    15.6         10.6        -5\n\n\nCode\nsubset_preselection_for_severe |&gt;\n  group_by(condition) |&gt;\n  summarize(mean_bdi_score = mean(bdi_score)) |&gt;\n  pivot_wider(names_from = condition,\n              values_from = mean_bdi_score) |&gt;\n  mutate(mean_diff = intervention - control)\n\n\n# A tibble: 1 × 3\n  control intervention mean_diff\n    &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1    37.0         32.0     -5.00\n\n\nCode\neffsize::cohen.d(formula = bdi_score ~ condition,\n                 data = subset_no_preselection)$estimate |&gt;\n  round_half_up(2)\n\n\n[1] 0.4\n\n\nCode\neffsize::cohen.d(formula = bdi_score ~ condition,\n                 data = subset_preselection_for_severe)$estimate |&gt;\n  round_half_up(2)\n\n\n[1] 0.71\n\n\nEquivalent change in means, different change in Cohen’s d\nWe know for a fact that the true difference in means is the same in both studies, because we create the data to be this way (i.e., scores at post are exactly pre - 5). The unstandardized effect sizes (pre-post difference in means) are the same, by definition.\nDespite this, the two studies produce the different Cohen’s d values. The standardized effect sizes are the different, despite exactly the same pre-post differences between the studies.\nIf the point of standardized effect sizes is to be able to compare them between studies on a common scale, and they don’t do this, what is their point?\n\n\n\n16.4.2 Example 2\nThe only difference here is a) the true difference in means and b) the seed.\n\n16.4.2.1 Wrangle/simulate\n\n\nCode\nset.seed(46)\n\nsubset_no_preselection &lt;- data_bdi |&gt;\n  rename(control = bdi_score) |&gt;\n  # simulate a 'intervention' score that is 5 points lower than pre\n  mutate(intervention = control - 5) |&gt;\n  # sample 100 participants from the real data \n  slice_sample(n = 100) |&gt;\n  mutate(recruitment = \"General population\") |&gt;\n  # reshape\n  pivot_longer(cols = c(control, intervention),\n               names_to = \"condition\",\n               values_to = \"bdi_score\") |&gt;\n  mutate(condition = fct_relevel(condition, \"control\", \"intervention\"))\n\n\nsubset_preselection_for_severe &lt;- data_bdi |&gt;\n  rename(control = bdi_score) |&gt;\n  # simulate recruitment into the study requiring a score of 29 or more at pre (\"severe\" depression according to the BDI-II manual)\n  filter(control &gt;= 29) |&gt;\n  # simulate a 'intervention' score that is 5 points lower than pre\n  mutate(intervention = control - 3) |&gt;\n  # sample 100 participants from the real data \n  slice_sample(n = 100) |&gt;\n  mutate(recruitment = \"'Severe' depression\") |&gt;\n  # reshape\n  pivot_longer(cols = c(control, intervention),\n               names_to = \"condition\",\n               values_to = \"bdi_score\") |&gt;\n  mutate(condition = fct_relevel(condition, \"control\", \"intervention\"))\n\n\n\n\n16.4.2.2 Plot\n\n\nCode\nbind_rows(subset_no_preselection,\n          subset_preselection_for_severe) |&gt;\n  mutate(recruitment = fct_relevel(recruitment, \"General population\", \"'Severe' depression\")) |&gt;\n  ## plot\n  ggplot(aes(bdi_score)) +\n  geom_histogram(boundary = 0, bins = 21) +\n  scale_fill_viridis_d(begin = 0.3, end = 0.7) +\n  theme_linedraw() +\n  coord_cartesian(xlim = c(-5, 63)) +\n  facet_grid(condition ~ recruitment) +\n  xlab(\"BDI-II sum score\") +\n  ylab(\"Frequency\")\n\n\n\n\n\n\n\n\n\n\n\n16.4.2.3 Analyze\nExercise:\nAgain, for each of the two datasets, please calculate:\n\nThis is the unstandaridzied difference in means between the groups. To do this, calculate the mean BDI-II score in each condition (control vs intervention) and then the difference between the two means.\n\nThe standardized mean difference (Cohen’s d) between the two groups (e.g., using effsize::cohen.d()).\n\nDoes the intervention work? Think about the simulated population effect.\n\n\nCode\n# datasets:\nsubset_no_preselection\n\n\n# A tibble: 200 × 4\n        id recruitment        condition    bdi_score\n     &lt;dbl&gt; &lt;chr&gt;              &lt;fct&gt;            &lt;dbl&gt;\n 1 2506327 General population control             42\n 2 2506327 General population intervention        37\n 3 2548703 General population control             11\n 4 2548703 General population intervention         6\n 5 2551646 General population control             17\n 6 2551646 General population intervention        12\n 7 2512827 General population control              6\n 8 2512827 General population intervention         1\n 9 2544327 General population control              0\n10 2544327 General population intervention        -5\n# ℹ 190 more rows\n\n\nCode\nsubset_preselection_for_severe\n\n\n# A tibble: 200 × 4\n        id recruitment         condition    bdi_score\n     &lt;dbl&gt; &lt;chr&gt;               &lt;fct&gt;            &lt;dbl&gt;\n 1 2518232 'Severe' depression control             38\n 2 2518232 'Severe' depression intervention        35\n 3 2512966 'Severe' depression control             29\n 4 2512966 'Severe' depression intervention        26\n 5 2550823 'Severe' depression control             37\n 6 2550823 'Severe' depression intervention        34\n 7 2519655 'Severe' depression control             41\n 8 2519655 'Severe' depression intervention        38\n 9 2543945 'Severe' depression control             31\n10 2543945 'Severe' depression intervention        28\n# ℹ 190 more rows\n\n\nSolution\n\n\nCode\nsubset_no_preselection |&gt;\n  group_by(condition) |&gt;\n  summarize(mean_bdi_score = mean(bdi_score)) |&gt;\n  pivot_wider(names_from = condition,\n              values_from = mean_bdi_score) |&gt;\n  mutate(mean_diff = intervention - control)\n\n\n# A tibble: 1 × 3\n  control intervention mean_diff\n    &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1    14.2         9.18        -5\n\n\nCode\nsubset_preselection_for_severe |&gt;\n  group_by(condition) |&gt;\n  summarize(mean_bdi_score = mean(bdi_score)) |&gt;\n  pivot_wider(names_from = condition,\n              values_from = mean_bdi_score) |&gt;\n  mutate(mean_diff = intervention - control)\n\n\n# A tibble: 1 × 3\n  control intervention mean_diff\n    &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1    36.2         33.2        -3\n\n\nCode\neffsize::cohen.d(formula = bdi_score ~ condition,\n                 data = subset_no_preselection)$estimate |&gt;\n  round_half_up(2)\n\n\n[1] 0.45\n\n\nCode\neffsize::cohen.d(formula = bdi_score ~ condition,\n                 data = subset_preselection_for_severe)$estimate |&gt;\n  round_half_up(2)\n\n\n[1] 0.45\n\n\nWe know for a fact that the true difference in means is different, because we create the data to be this way (i.e., pre-post difference is -5 in the no preselection study and -3 in the severe depression preselection study). The unstandardized effect sizes (pre-post difference in means) are different, by definition.\nDespite this, the two studies produce the same Cohen’s d value. The standardized effect sizes are the same, despite genuine differences in the pre-post changes between the two studies.\nIf the same standardized effect size estimate (Cohen’s d) can represent different real changes in means, how can a Cohen’s d of .2, for example, represent “small” effects? That is, if “small” effects on standardized effect sizes can represent unstandardized effect sizes of different sizes, how are standardized effect sizes ‘standardized’ at all?",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Standardized effect sizes and range restriction</span>"
    ]
  },
  {
    "objectID": "chapters/standardized_effect_sizes_and_range_restriction.html#explanation",
    "href": "chapters/standardized_effect_sizes_and_range_restriction.html#explanation",
    "title": "16  Standardized effect sizes and range restriction",
    "section": "16.5 Explanation",
    "text": "16.5 Explanation\nThe above results - where the same unstandarized effect sizes have different standardized effect sizes, or vice-versa - are due to the fact that standardized effect sizes involve dividing, in one way or another, unstandardized effect sizes by standard deviations.\nE.g., for Cohen’s \\(d\\):\n\\(d = \\frac{M_{intervention} - M_{control}}{SD_{pooled}}\\)\nMost researchers are far more interested in the numerator than the denominator.\n\nResearchers often care about how much the means differ between the intervention and control groups. Differences in the means determine whether the intervention ‘worked’ or not.\nThey usually care very little about what the SD, except perhaps if they’re assessing statistical assumptions (homogeneity of variances).\n\nDespite this, the value of the SDs heavily influences the standardized effect size.\nIn the above examples, the range restriction in the ‘severe’ depression condition produces a narrower range of scores, and therefore smaller smaller SDs. Dividing the same difference in means by a smaller value of SD produces a different Cohen’s d estimate.\nRange restrictions like these are extremely common in psychology research, where studies can differ in their inclusion/exclusion strategies. This means makes it far harder to compare ‘standardized’ effect sizes between studies than you might think.\n\n16.5.1 Standardized effect sizes require estimating multiple parameters\nCohen’s d (usually) involves having to create a sample estimate of the means in each group. Researchers are usually more interested in differences between means.\nBut it also involves having to estimate the SDs. This can be a little a little confusing the first time you encounter it: we often intuitively think of SD as the amount of noise around the signal we’re interested in (the mean). We are somewhat more used to thinking about the fact that estimated means have error round them: the standard error of the mean (SEM) is used to calculate confidence intervals around means, and the SEM is actually just the SD of the mean (as opposed to normal SD, which is SD of the data).\nWe are relatively less familiar with thinking about the fact that estimates of standard deviation also are estimated with error, e.g., the standard error of the SD, which is the SD of the SD. Confused yet?\nWe can understand this more easily with a simulation. We generate data for a single sample with a population mean (\\(\\mu\\)) = 0 and population SD (\\(\\sigma\\)) = 1.\nAcross lots of iterations, we can see that the average sample mean is close to the population mean (\\(\\mu\\)), and the average sample SD is close to the population (\\(\\sigma\\)):\n\n\nCode\n# set the seed ----\n# for the pseudo random number generator to make results reproducible\nset.seed(123)\n\n\n# define data generating function ----\ngenerate_data &lt;- function(n,\n                          mean,\n                          sd) {\n  \n  data &lt;- tibble(score = rnorm(n = n, mean = mean, sd = sd))\n  \n  return(data)\n}\n\n\n# define data analysis function ----\nanalyse_data &lt;- function(data) {\n  \n  res &lt;- data |&gt;\n    summarize(sample_mean = mean(score),\n              sample_sd = sd(score))\n  \n  return(res)\n}\n\n\n# define experiment parameters ----\nexperiment_parameters_grid &lt;- expand_grid(\n  n = c(50, 100, 150),\n  mean = 0,\n  sd = 1,\n  iteration = 1:1000\n)\n\n\n# run simulation ----\nsimulation &lt;- \n  # using the experiment parameters\n  experiment_parameters_grid |&gt;\n  \n  # generate data using the data generating function and the parameters relevant to data generation\n  mutate(generated_data = pmap(list(n,\n                                    mean,\n                                    sd),\n                               generate_data)) |&gt;\n  \n  # apply the analysis function to the generated data using the parameters relevant to analysis\n  mutate(analysis_results = pmap(list(generated_data),\n                                 analyse_data))\n  \n\n# summarise simulation results over the iterations ----\nsimulation_summary &lt;- simulation |&gt;\n  unnest(analysis_results) \n\nsimulation_summary |&gt;\n  group_by(n) |&gt;\n  summarize(average_sample_means = mean(sample_mean),\n            average_sample_sds = mean(sample_sd)) |&gt;\n  mutate_if(is.numeric, janitor::round_half_up, digits = 2) |&gt;\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\nn\naverage_sample_means\naverage_sample_sds\n\n\n\n\n50\n0\n1\n\n\n100\n0\n1\n\n\n150\n0\n1\n\n\n\n\n\nBut the estimated means in individual samples (i.e., individual iterations) vary around this true value (\\(\\mu\\) = 0). The smaller the sample size, the more deviation there is from the population value:\n\n\nCode\nsimulation_summary |&gt;\n  mutate(n_string = paste(\"N =\", n),\n         n_string = fct_relevel(n_string, \"N = 50\", \"N = 100\", \"N = 150\")) |&gt;\n  ggplot(aes(sample_mean)) +\n  geom_histogram(boundary = 0) +\n  theme_linedraw() +\n  ylab(\"Frequency\") +\n  xlab(\"Means found in different samples\\n(where population mu = 0)\") +\n  facet_wrap(~ n_string)\n\n\n\n\n\n\n\n\n\nThe same applies to the estimated SDs in individual samples (i.e., individual iterations), which also vary around this true value (\\(\\sigma\\) = 1). The smaller the sample size, the more deviation there is from the population value:\n\n\nCode\nsimulation_summary |&gt;\n  mutate(n_string = paste(\"N =\", n),\n         n_string = fct_relevel(n_string, \"N = 50\", \"N = 100\", \"N = 150\")) |&gt;\n  ggplot(aes(sample_sd)) +\n  geom_histogram(boundary = 0) +\n  theme_linedraw() +\n  ylab(\"Frequency\") +\n  xlab(\"SDs found in different samples\\n(where population sigma = 1)\") +\n  facet_wrap(~ n_string)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Standardized effect sizes and range restriction</span>"
    ]
  },
  {
    "objectID": "chapters/standardized_effect_sizes_and_range_restriction.html#solutions-to-this-problem",
    "href": "chapters/standardized_effect_sizes_and_range_restriction.html#solutions-to-this-problem",
    "title": "16  Standardized effect sizes and range restriction",
    "section": "16.6 Solutions to this problem",
    "text": "16.6 Solutions to this problem\nThere are solutions to this, to make “standardized” effect sizes actually standard between studies. But almost no one does them.\n\nThe when calculating standardized effect sizes, use a well established population norm estimate of the measure’s SD rather than the sample SD. E.g., always set the BDI’s SD to 12 (or whatever your best estimate is). Note that no implementations of Cohen’s d in commonly used R packages recommend this, and only a few can directly handle it (e.g., {esci}).\nUse math/R packages to correct your standardised effect size estimate for range restriction (see Wiernik & Dahlke, 2020, doi: 10.1177/2515245919885611).",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Standardized effect sizes and range restriction</span>"
    ]
  },
  {
    "objectID": "chapters/standardized_effect_sizes_and_range_restriction.html#is-this-issue-limited-to-cohens-d",
    "href": "chapters/standardized_effect_sizes_and_range_restriction.html#is-this-issue-limited-to-cohens-d",
    "title": "16  Standardized effect sizes and range restriction",
    "section": "16.7 Is this issue limited to Cohen’s d?",
    "text": "16.7 Is this issue limited to Cohen’s d?\nNo, it affects other forms of standardized effect sizes too, including correlations.\nE.g., there is a perennial debate in the US about whether standardized university entrance tests like the SAT are useful or not, or indeed are biased or not (e.g., between gender and race/ethnicity), because straightforward analyses suggest that SAT scores (used to get a place at university) are poorly predictive of grades at university.\nHowever, this poor predictive validity may be due in part to range restriction: because the SAT scores are used to determine who goes to university, data on university grades is only obtained from those individuals who already scored highly on the SAT. That is, there is a fairly narrow range of SAT scores among university students. Correlations, like Cohen’s d, include SD in their denominator (i.e., \\(r = covariance_{xy}/(SD_x*SD_y)\\)), and therefore range restriction also distorts correlations.\nIt is therefore possible - indeed, likely - that SAT scores are usefully predictive of grades at university. The below short simulation demonstrates attentuation in correlations due to range constraint.\n\n\nCode\n# Set seed for reproducibility\nset.seed(42)\n\n# Parameters\nn &lt;- 10000  # number of observations\nrho &lt;- 0.6  # correlation between x and y\n\n# Generate correlated data using the faux package\nsimulated_data &lt;- rnorm_multi(n = n, \n                              mu = c(0, 0), \n                              sd = c(1, 1), \n                              r = matrix(c(1, rho, \n                                           rho, 1), nrow = 2),\n                              varnames = c(\"x\", \"y\"))\n\n# Calculate correlation in full data\nfull_correlation &lt;- cor(simulated_data$x, simulated_data$y)\ncat(\"Correlation in full data:\", janitor::round_half_up(full_correlation, digits = 2), \"\\n\")\n\n\nCorrelation in full data: 0.6 \n\n\nCode\n# Introduce range restriction (e.g., keep only x &gt; -0.5 and x &lt; 0.5)\nsimulated_data_range_restricted &lt;- simulated_data |&gt;\n  filter(x &gt; qnorm(0.75)) # top 25% of a normal population corresponds to SD &gt; qnorm(0.75), ie 0.6744898\n\n# Calculate correlation in restricted data\nrestricted_correlation &lt;- cor(simulated_data_range_restricted$x, simulated_data_range_restricted$y)\ncat(\"Correlation in restricted data:\", janitor::round_half_up(restricted_correlation, digits = 2), \"\\n\")\n\n\nCorrelation in restricted data: 0.35 \n\n\nCode\n# Plot full data with correlation annotation\nggplot(simulated_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.4) +\n  #geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  ggtitle(\"Correlation in Full Data\") +\n  theme_linedraw() +\n  annotate(\"text\", x = -2, y = 2, label = paste(\"r =\", round(full_correlation, 2)), \n           hjust = 0.5, vjust = 0.5, size = 6, color = \"blue\") +\n  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3))\n\n\n\n\n\n\n\n\n\nCode\n# Plot restricted data with correlation annotation\nggplot(simulated_data_range_restricted, aes(x = x, y = y)) +\n  geom_point(alpha = 0.4) +\n  #geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  ggtitle(\"Correlation in Range Restricted Data\") +\n  theme_linedraw() +\n  annotate(\"text\", x = -2, y = 2, label = paste(\"r =\", round(restricted_correlation, 2)), \n           hjust = 0.5, vjust = 0.5, size = 6, color = \"red\") +\n  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3))\n\n\n\n\n\n\n\n\n\nNote that the observed correlations which have been distorted due to range restriction can be ‘de-attentuated’ or corrected if normative data is available to know what the unrestricted range looks like. However, this is very rarely done in studies and meta-analyses.\n\n\nCode\n# Calculate the variance ratios as an estimate of the range restriction factor\nvariance_ratio &lt;- var(simulated_data_range_restricted$x) / var(simulated_data$x)\n\n# Deattenuate the observed correlation\ncorrected_correlation &lt;- restricted_correlation / sqrt(variance_ratio)\n\n# Output results\ncat(\"Observed Correlation (Restricted):\", janitor::round_half_up(restricted_correlation, 2), \"\\n\")\n\n\nObserved Correlation (Restricted): 0.35 \n\n\nCode\ncat(\"Variance Ratio (Range Restriction Factor):\", janitor::round_half_up(variance_ratio, 2), \"\\n\")\n\n\nVariance Ratio (Range Restriction Factor): 0.25 \n\n\nCode\ncat(\"Corrected Correlation (Deattenuated):\", janitor::round_half_up(corrected_correlation, 2), \"\\n\")\n\n\nCorrected Correlation (Deattenuated): 0.69 \n\n\nNote that the corrected correlation is much closer to the original one.\n\n\nCode\nsessionInfo()\n\n\nR version 4.5.0 (2025-04-11)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.6\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Zurich\ntzcode source: internal\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] faux_1.2.2       kableExtra_1.4.0 knitr_1.50       effsize_0.8.1   \n [5] janitor_2.2.1    sn_2.1.1         scales_1.4.0     lubridate_1.9.4 \n [9] forcats_1.0.1    stringr_1.6.0    dplyr_1.1.4      purrr_1.2.0     \n[13] readr_2.1.5      tidyr_1.3.1      tibble_3.3.0     ggplot2_4.0.1   \n[17] tidyverse_2.0.0 \n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.6          generics_0.1.4      xml2_1.4.0         \n [4] stringi_1.8.7       hms_1.1.3           digest_0.6.39      \n [7] magrittr_2.0.4      evaluate_1.0.5      grid_4.5.0         \n[10] timechange_0.3.0    RColorBrewer_1.1-3  fastmap_1.2.0      \n[13] jsonlite_2.0.0      viridisLite_0.4.2   numDeriv_2016.8-1.1\n[16] textshaping_1.0.3   mnormt_2.1.1        cli_3.6.5          \n[19] crayon_1.5.3        rlang_1.1.6         bit64_4.6.0-1      \n[22] withr_3.0.2         parallel_4.5.0      tools_4.5.0        \n[25] tzdb_0.5.0          vctrs_0.6.5         R6_2.6.1           \n[28] lifecycle_1.0.4     snakecase_0.11.1    bit_4.6.0          \n[31] htmlwidgets_1.6.4   vroom_1.6.6         archive_1.1.12.1   \n[34] pkgconfig_2.0.3     pillar_1.11.1       gtable_0.3.6       \n[37] glue_1.8.0          systemfonts_1.2.3   xfun_0.54          \n[40] tidyselect_1.2.1    rstudioapi_0.17.1   farver_2.1.2       \n[43] htmltools_0.5.9     labeling_0.4.3      rmarkdown_2.30     \n[46] svglite_2.2.1       compiler_4.5.0      S7_0.2.1",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Standardized effect sizes and range restriction</span>"
    ]
  },
  {
    "objectID": "chapters/impact_of_careless_responding.html",
    "href": "chapters/impact_of_careless_responding.html",
    "title": "17  The impact of one form of careless responding on power and the false positive rate",
    "section": "",
    "text": "17.1 Assignment\nBackground/Rationale of the exercise:\nMany surveys that use a Likert scale or a slider have a default response. E.g., when you load the page all answers already have a default answer of “0” on a -3 to +3 scale. Careless or lazy responding is common. Some participants simply leave the default answers and click “next” in the survey. Many researchers don’t use attention checks in their surveys, or don’t use good ones, and these careless or lazy responses are not excluded. This stimulation seeks to quantify the impact of this type of responding on the results. Please note there any many other forms of careless responding - this is just one example and doesn’t provide a full answer to this question.\nExercise:\nWrite R code from scratch, but using our established workflow, that does the following:",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The impact of one form of careless responding on power and the false positive rate</span>"
    ]
  },
  {
    "objectID": "chapters/impact_of_careless_responding.html#assignment",
    "href": "chapters/impact_of_careless_responding.html#assignment",
    "title": "17  The impact of one form of careless responding on power and the false positive rate",
    "section": "",
    "text": "Data generation function\n\nSimulate two independent groups, control and intervention, drawn from a normal distribution. The mean and SD of both conditions should be variables.\n\nCorrupt data function\n\nYou can use the corrupt data function I provide you with below. This replaces a proportion of the whole dataset’s ‘score’ column with a default value (in this case zero). You should use the usual mutate() and pmap() workflow to create a new column, corrupted_data, from an existing column named generated_data.\n\nAnalyze data function\n\nFit a Student’s t-test and extract the p value in a tidy tibble.\n\nAn expand grid call using:\n\nn per condition = 100\nmean = 0 for the control group\nmean = 0 or 0.50 for the intervention group (two scenarios, population effect exists or does not)\nSD = 1\nproportion of straight line responders = 0 or 0.1\n1000 iterations\nusing set.seed(42)\n\nSummarize across iterations\n\nSummarise the proportion of significant p values in all simulated conditions in a table or plot\nProvide a description and interpretation of the results: How does this form of straight line responding affect the false positive rate? How does it affect power? (briefly, in two or three sentences)",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The impact of one form of careless responding on power and the false positive rate</span>"
    ]
  },
  {
    "objectID": "chapters/impact_of_careless_responding.html#dependencies",
    "href": "chapters/impact_of_careless_responding.html#dependencies",
    "title": "17  The impact of one form of careless responding on power and the false positive rate",
    "section": "17.2 Dependencies",
    "text": "17.2 Dependencies\n\n\nCode\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(sn)\nlibrary(janitor)\nlibrary(effsize)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(faux)",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The impact of one form of careless responding on power and the false positive rate</span>"
    ]
  },
  {
    "objectID": "chapters/impact_of_careless_responding.html#generate-data-function",
    "href": "chapters/impact_of_careless_responding.html#generate-data-function",
    "title": "17  The impact of one form of careless responding on power and the false positive rate",
    "section": "17.3 Generate data function",
    "text": "17.3 Generate data function",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The impact of one form of careless responding on power and the false positive rate</span>"
    ]
  },
  {
    "objectID": "chapters/impact_of_careless_responding.html#contaminate-data-function",
    "href": "chapters/impact_of_careless_responding.html#contaminate-data-function",
    "title": "17  The impact of one form of careless responding on power and the false positive rate",
    "section": "17.4 Contaminate data function",
    "text": "17.4 Contaminate data function\n\n\nCode\ncontaminate_data &lt;- function(data, proportion_straightline_responder, value = 0) {\n  data %&gt;% \n    mutate(is_straightline_responder = runif(n()) &lt; proportion_straightline_responder,    # Bernoulli(proportion)\n           score       = if_else(is_straightline_responder, value, score)) %&gt;% \n    ungroup()\n}",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The impact of one form of careless responding on power and the false positive rate</span>"
    ]
  },
  {
    "objectID": "chapters/impact_of_careless_responding.html#analyze-data-function",
    "href": "chapters/impact_of_careless_responding.html#analyze-data-function",
    "title": "17  The impact of one form of careless responding on power and the false positive rate",
    "section": "17.5 Analyze data function",
    "text": "17.5 Analyze data function",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The impact of one form of careless responding on power and the false positive rate</span>"
    ]
  },
  {
    "objectID": "chapters/impact_of_careless_responding.html#simulation-parameters",
    "href": "chapters/impact_of_careless_responding.html#simulation-parameters",
    "title": "17  The impact of one form of careless responding on power and the false positive rate",
    "section": "17.6 Simulation parameters",
    "text": "17.6 Simulation parameters",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The impact of one form of careless responding on power and the false positive rate</span>"
    ]
  },
  {
    "objectID": "chapters/impact_of_careless_responding.html#run-simulation",
    "href": "chapters/impact_of_careless_responding.html#run-simulation",
    "title": "17  The impact of one form of careless responding on power and the false positive rate",
    "section": "17.7 Run simulation",
    "text": "17.7 Run simulation\n\n\nCode\nset.seed(42)",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The impact of one form of careless responding on power and the false positive rate</span>"
    ]
  },
  {
    "objectID": "chapters/impact_of_careless_responding.html#summarize-results-across-iterations",
    "href": "chapters/impact_of_careless_responding.html#summarize-results-across-iterations",
    "title": "17  The impact of one form of careless responding on power and the false positive rate",
    "section": "17.8 Summarize results across iterations",
    "text": "17.8 Summarize results across iterations\n[written description and interpretation of results here]\nRemember that this is just one narrow simulation of the impact of one type of lazy/careless responding on one type of analysis - other forms and other analyses can be affected very differently.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>The impact of one form of careless responding on power and the false positive rate</span>"
    ]
  },
  {
    "objectID": "chapters/meta_analysis_and_publication_bias.html",
    "href": "chapters/meta_analysis_and_publication_bias.html",
    "title": "18  Meta-analysis and publication bias",
    "section": "",
    "text": "18.1 Dependencies\nCode\n# dependencies ----\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(readr)\nlibrary(purrr)\nlibrary(furrr)\nlibrary(ggplot2)\nlibrary(effsize)\nlibrary(janitor)\nlibrary(tibble)\n#library(sn)\nlibrary(metafor)\nlibrary(parameters)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(pwr)\nlibrary(ggstance)\n\n# set the seed ----\n# for the pseudo random number generator to make results reproducible\nset.seed(123)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/meta_analysis_and_publication_bias.html#what-is-a-meta-analysis",
    "href": "chapters/meta_analysis_and_publication_bias.html#what-is-a-meta-analysis",
    "title": "18  Meta-analysis and publication bias",
    "section": "18.2 What is a meta-analysis?",
    "text": "18.2 What is a meta-analysis?\nLet’s start with some imagined summary statistics, and convert them to Cohen’s d effect sizes.\nyi refers to the effect size, and vi refers to its variance (note that Standard Error = sqrt(variance)).\n\n\nCode\nmean_intervention &lt;- c(0.68, 0.97, 0.40, 0.48, 0.56, 0.10, -0.10, 0.03)\nmean_control      &lt;- c(   0,    0,    0,    0,    0,    0,     0,    0)\nsd_intervention   &lt;- c(   1,    1,    1,    1,    1,    1,     1,    1)\nsd_control        &lt;- c(   1,    1,    1,    1,    1,    1,     1,    1)\nn_intervention    &lt;- c(  20,   10,  100,   37,   50,  450,    50, 1000)\nn_control         &lt;- c(  20,   10,  100,   37,   50,  450,    50, 1000)\n\nes &lt;- escalc(measure = \"SMD\", \n             m1i  = mean_intervention, \n             m2i  = mean_control, \n             sd1i = sd_intervention,\n             sd2i = sd_control,\n             n1i  = n_intervention,\n             n2i  = n_control)\n\nes |&gt;\n  as_tibble() |&gt;\n  mutate_all(round_half_up, digits = 2) |&gt;\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\nyi\nvi\n\n\n\n\n0.67\n0.11\n\n\n0.93\n0.22\n\n\n0.40\n0.02\n\n\n0.47\n0.06\n\n\n0.56\n0.04\n\n\n0.10\n0.00\n\n\n-0.10\n0.04\n\n\n0.03\n0.00\n\n\n\n\n\n\n18.2.1 Fixed-effects meta-analysis\nAka Common-Effects or Equal-Effects.\nThe simplest form of meta-analysis - although it would not be acceptable to use anywhere these days - is simply the mean effect size.\n\n\nCode\nes |&gt;\n  summarize(mean_effect_size = round_half_up(mean(yi), digits = 2)) |&gt;\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\nmean_effect_size\n\n\n\n\n0.38\n\n\n\n\n\nNote that calculating the mean is equivalent to fitting an intercept only fixed-effects model (ie linear regression) with the effect sizes as the DV. The estimate of the intercept is equivalent to the mean effect size.\n\n\nCode\nlm(yi ~ 1,\n   data = es) |&gt;\n  model_parameters() \n\n\nParameter   | Coefficient |   SE |       95% CI | t(7) |     p\n--------------------------------------------------------------\n(Intercept) |        0.38 | 0.12 | [0.09, 0.67] | 3.09 | 0.018\n\n\nWhy is this not acceptable? Because it ignores the error associated with each effect size. A very simple meta-analysis might use weighted-mean effect sizes and weight them by the total sample size of each study:\n\n\nCode\nweighted.mean(x = es$yi, w = n_intervention + n_control) |&gt;\n  round_half_up(digits = 2) \n\n\n[1] 0.1\n\n\nNote that the weighted mean effect size is equivalent to an intercept only fixed-effects model (linear regression) with the effect sizes as the DV and the sample sizes as weights. The estimate of the intercept is equivalent to the weighted mean effect size.\n\n\nCode\nlm(yi ~ 1,\n   weights = n_intervention + n_control,\n   data = es) |&gt;\n  model_parameters()\n\n\nParameter   | Coefficient |   SE |        95% CI | t(7) |     p\n---------------------------------------------------------------\n(Intercept) |        0.10 | 0.06 | [-0.04, 0.25] | 1.70 | 0.133\n\n\nQuite early in the development of meta-analysis methods, people started to weight not by N but by inverse variance of the effect size, on the basis that things other than N can affect the precision of estimation of the effect size. This can be implemented as follows:\n\n\nCode\nlm(yi ~ 1,\n   weights = 1/vi,\n   data = es) |&gt;\n  model_parameters()\n\n\nParameter   | Coefficient |   SE |        95% CI | t(7) |     p\n---------------------------------------------------------------\n(Intercept) |        0.10 | 0.06 | [-0.04, 0.24] | 1.70 | 0.133\n\n\nThe above linear regression produces comparable results as when you fit a ‘proper’ fixed-effects meta-analysis using the {metafor} package. There are some small differences in the effect size and its 95% CIs that aren’t important to understand here. The ‘Overall’ row reports the meta-analysis results.\n\n\nCode\nrma(yi = yi, \n    vi = vi, \n    method = \"FE\", # fixed effect model\n    data = es) |&gt;\n  model_parameters()\n\n\nMeta-analysis using 'metafor'\n\nParameter | Coefficient |   SE |        95% CI |     z |     p | Weight\n-----------------------------------------------------------------------\nStudy 1   |        0.67 | 0.32 | [ 0.03, 1.30] |  2.05 | 0.040 |   9.47\nStudy 2   |        0.93 | 0.47 | [ 0.01, 1.85] |  1.97 | 0.048 |   4.51\nStudy 3   |        0.40 | 0.14 | [ 0.12, 0.68] |  2.79 | 0.005 |  49.03\nStudy 4   |        0.47 | 0.24 | [ 0.01, 0.94] |  2.01 | 0.044 |  17.99\nStudy 5   |        0.56 | 0.20 | [ 0.16, 0.96] |  2.73 | 0.006 |  24.07\nStudy 6   |        0.10 | 0.07 | [-0.03, 0.23] |  1.50 | 0.134 | 224.72\nStudy 7   |       -0.10 | 0.20 | [-0.49, 0.29] | -0.50 | 0.620 |  24.97\nStudy 8   |        0.03 | 0.04 | [-0.06, 0.12] |  0.67 | 0.503 | 499.94\nOverall   |        0.10 | 0.03 | [ 0.03, 0.17] |  2.97 | 0.003 |       \n\n\n\n\n18.2.2 Random-effects meta-analysis\nThere is a debate about whether Fixed-Effects vs. Random-Effects models should be employed in meta-analysis. Most recommendations come down on the side of Random-Effects, sometimes people recommend reporting the results of both. Briefly: FE models have less plausible assumptions about the differences between studies, but RE models suffer from putting less weight on the sample sizes of individual large studies.\nWithout getting into Random-Effects models conceptually, its useful to know that Random-Effects meta-analyses can also easily be fitted in {metafor}, and indeed are the default.\n\n\nCode\nfit &lt;- \n  rma(yi = yi, \n      vi = vi, \n      method = \"REML\", # default random effects model\n      data = es)\n\nmodel_parameters(fit)\n\n\nMeta-analysis using 'metafor'\n\nParameter | Coefficient |   SE |        95% CI |     z |     p | Weight\n-----------------------------------------------------------------------\nStudy 1   |        0.67 | 0.32 | [ 0.03, 1.30] |  2.05 | 0.040 |   9.47\nStudy 2   |        0.93 | 0.47 | [ 0.01, 1.85] |  1.97 | 0.048 |   4.51\nStudy 3   |        0.40 | 0.14 | [ 0.12, 0.68] |  2.79 | 0.005 |  49.03\nStudy 4   |        0.47 | 0.24 | [ 0.01, 0.94] |  2.01 | 0.044 |  17.99\nStudy 5   |        0.56 | 0.20 | [ 0.16, 0.96] |  2.73 | 0.006 |  24.07\nStudy 6   |        0.10 | 0.07 | [-0.03, 0.23] |  1.50 | 0.134 | 224.72\nStudy 7   |       -0.10 | 0.20 | [-0.49, 0.29] | -0.50 | 0.620 |  24.97\nStudy 8   |        0.03 | 0.04 | [-0.06, 0.12] |  0.67 | 0.503 | 499.94\nOverall   |        0.27 | 0.10 | [ 0.07, 0.47] |  2.64 | 0.008 |       \n\n\nMany meta-analyses also report forest plots, which list the studies, plot the individual and meta-analysis effect sizes and their 95% CIs, and report them numerically too for precision.\n\n\nCode\nforest(fit, header = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/meta_analysis_and_publication_bias.html#why-you-cant-just-count-the-proportion-of-significant-p-values",
    "href": "chapters/meta_analysis_and_publication_bias.html#why-you-cant-just-count-the-proportion-of-significant-p-values",
    "title": "18  Meta-analysis and publication bias",
    "section": "18.3 Why you can’t just count the proportion of significant p values",
    "text": "18.3 Why you can’t just count the proportion of significant p values\nStudies have different power, so each tests the hypothesis with a different probability of detecting the effect assuming its true. Mixed results can therefore be found even when all have studied the same true hypothesis, even if they happened to all find exactly the same effect size.\n\nCounting p values: Only 2/8 studies found significant results [reject H1]\nObserved effect sizes: All studies found Cohen’s = 0.2 [accept H1]\nMeta-analysis: Cohen’s d = 0.20, 95% CI [0.13, 0.27] [accept H1]\n\n\n\nCode\nes &lt;- escalc(measure = \"SMD\", \n             m1i  = c( 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,  0.2), \n             m2i  = c(   0,   0,   0,   0,   0,   0,   0,    0), \n             sd1i = c(   1,   1,   1,   1,   1,   1,   1,    1),\n             sd2i = c(   1,   1,   1,   1,   1,   1,   1,    1),\n             n1i  = c(  20,  10, 100,  37,  50, 450,  50, 1000),\n             n2i  = c(  20,  10, 100,  37,  50, 450,  50, 1000))\n\nrma(yi     = yi, \n    vi     = vi, \n    data   = es,\n    method = \"REML\") |&gt;\n  forest(header = TRUE)\n\n\n\n\n\n\n\n\n\nEqually, the population effect might be zero, but some studies might still detect effects. Why might this happen?\n\n\nCode\nes &lt;- escalc(measure = \"SMD\", \n             m1i  = c( 0.18, 0.43, -0.30, -0.08, 0.06, 0.10, -0.10, 0.03), \n             m2i  = c(    0,    0,     0,     0,    0,    0,     0,    0), \n             sd1i = c(    1,    1,     1,     1,    1,    1,     1,    1),\n             sd2i = c(    1,    1,     1,     1,    1,    1,     1,    1),\n             n1i  = c(   70,   50,   100,    37,   50,  350,    50,  400),\n             n2i  = c(   70,   50,   100,    37,   50,  350,    50,  400))\n\nrma(yi     = yi, \n    vi     = vi, \n    data   = es,\n    method = \"REML\") |&gt;\n  forest(header = TRUE)\n\n\n\n\n\n\n\n\n\nOf course, the majority of studies might produce significant results and the meta-analysis also produce a significant effect, and we might still have doubts about whether the effect really exists or not. Why might this be?\n\n\nCode\nes &lt;- escalc(measure = \"SMD\", \n             m1i  = c( 0.68, 0.97, 0.40, 0.48, 0.56, 0.10, -0.10, 0.03), \n             m2i  = c(    0,    0,    0,    0,    0,    0,     0,    0), \n             sd1i = c(    1,    1,    1,    1,    1,    1,     1,    1),\n             sd2i = c(    1,    1,    1,    1,    1,    1,     1,    1),\n             n1i  = c(   20,   10,  100,   37,   50,  450,    50, 1000),\n             n2i  = c(   20,   10,  100,   37,   50,  450,    50, 1000))\n\nres &lt;- \n  rma(yi     = yi, \n      vi     = vi, \n      data   = es,\n      method = \"REML\") \n\nforest(res, header = TRUE)\n\n\n\n\n\n\n\n\n\nCode\nfunnel(res)\n\n\n\n\n\n\n\n\n\nCode\n# funnel(res, level = c(90, 95, 99), refline = 0, legend = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/meta_analysis_and_publication_bias.html#why-we-cant-have-nice-things",
    "href": "chapters/meta_analysis_and_publication_bias.html#why-we-cant-have-nice-things",
    "title": "18  Meta-analysis and publication bias",
    "section": "18.4 Why we can’t have nice things",
    "text": "18.4 Why we can’t have nice things\n\n18.4.1 Confusing SD and SE when extracting summary statistics\nEven articles published in the most prestigious journals and on topics that will impact patient care are highly susceptible to this, e.g., Metaxa & Clarke (2024) “Efficacy of psilocybin for treating symptoms of depression: systematic review and meta-analysis” was highlighted as doing this. It’s not even down to unclear labelling in the orignal study: even when they state “numerical data show means (SEM)”, as in this case, its often extracted as the SD.\nBecause SEs are MUCH smaller than SDs, incorrectly using SE causes Cohen’s d to be inflated - usually by a lot.\nThe below demonstrates this. The same summary statistics are used as in previous examples above. Effect sizes, their 95% CIs, and their SEM are then calculated. For two of the studies, the SDs are replaced with the SEs. The meta-analysis then shows this distortion on the individual effect sizes and the meta-analytic effect size.\n\n\nCode\n# summary stats\nmean_intervention &lt;- c(0.68, 0.97, 0.40, 0.48, 0.56, 0.10, -0.10, 0.03)\nmean_control      &lt;- c(   0,    0,    0,    0,    0,    0,     0,    0)\nsd_intervention   &lt;- c(   1,    1,    1,    1,    1,    1,     1,    1)\nsd_control        &lt;- c(   1,    1,    1,    1,    1,    1,     1,    1)\nn_intervention    &lt;- c(  20,   10,  100,   37,   50,  450,    50, 1000)\nn_control         &lt;- c(  20,   10,  100,   37,   50,  450,    50, 1000)\n\ndat &lt;- \n  tibble(m1i  = mean_intervention, \n         m2i  = mean_control, \n         sd1i = sd_intervention,\n         sd2i = sd_control,\n         n1i  = n_intervention,\n         n2i  = n_control) |&gt;\n  rownames_to_column(var = \"study\") |&gt;\n  # calculate SEs\n  mutate(se1i = sd1i/sqrt(n1i),\n         se2i = sd2i/sqrt(n2i)) |&gt;\n  # replace SDs with SEs for two studies, studies 4 and 6\n  mutate(sd1i_error = ifelse(study %in% c(\"4\", \"6\"), se1i, sd1i),\n         sd2i_error = ifelse(study %in% c(\"4\", \"6\"), se2i, sd2i))\n\n# calculate effect sizes properly\nes_without_errors &lt;- \n  escalc(measure = \"SMD\", \n         m1i  = dat$m1i,\n         m2i  = dat$m2i,\n         sd1i = dat$sd1i,\n         sd2i = dat$sd2i,\n         n1i  = dat$n1i,\n         n2i  = dat$n2i) \n\n# calculate effect sizes with SE/SD errors\nes_with_errors &lt;- \n  escalc(measure = \"SMD\", \n         m1i  = dat$m1i,\n         m2i  = dat$m2i,\n         sd1i = dat$sd1i_error,\n         sd2i = dat$sd2i_error,\n         n1i  = dat$n1i,\n         n2i  = dat$n2i) \n\n# meta-analyze the correctly calculated effect sizes\nfit_correct &lt;- \n  rma(yi = yi, \n      vi = vi, \n      method = \"REML\",\n      data = es_without_errors)\n\nforest(fit_correct, \n       header = \"Correctly calculated effect sizes\")\n\n\n\n\n\n\n\n\n\nCode\n# meta-analyze the erroneously calculated effect sizes\nfit_errors &lt;- \n  rma(yi = yi, \n      vi = vi, \n      method = \"REML\",\n      data = es_with_errors) \n\nforest(fit_errors,\n       header = \"Erroneous effect sizes: SE used as SD for two studies\")\n\n\n\n\n\n\n\n\n\nMaking this error for 2 of 8 studies here inflates the effect sizes for those studies to be extremely and implausibly large - Cohen’s d &gt; 2. This also greatly increases the meta-analysis effect size.\n\nNote that this could be simulated more extensively as an end-of-course assignment. I.e., assuming different true effect sizes and prevalences of misinterpreting SE as SD, what is the proportionate distortion of of meta-effect sizes in the literature? For a given true effect size, what is the probability of observing a true effect size of X in a component study relative to it being a coding error? (e.g., if true effect size is 0.2, what proportion of observed effect sizes of 1.5 are erroneous?)\n\n\n\n18.4.2 Publication bias\nWhat proportion of studies that are conducted are actually published?\nGiven what we know about publication bias, perhaps we should instead ask: what proportion of studies with significant results are published? And what proportion with non-significant results are published?\nIt is very hard to know how to interpret and synthesize the published literature without knowing this, because we don’t know what is hidden from us.\nSeveral estimates of the prevalence of significant vs non-significant results in the literature exist.\n\nSterling (1959) found that 97% of psychology articles reported support for their hypothesis.\nSterling et al. (1989) later found that this result was nearly unchanged 30 years later (95%).\nAnother 20 years later, Fanelli (2010) found it was around 90% (albeit using different journals).\n\nHowever, all of these estimate estimate the opposite conditional probability: the probability of significance given being published: P(significant | published).\nWe actually need to know the opposite, the probability of being published given significance: P(published | significant), and the probability of being published given non-significance: P(published | nonsignificant).\nWorryingly, there is very little research on this. I only know of two studies that provide estimates of this (Franco et al. (2014; 2016):\n\nP(published | significant) = 57/93 = 0.61\nP(published | nonsignificant) = 11/49 = 0.22\n\nHowever, registered databases of approved studies are not typical in psychology, so these values are likely to be representative of psychology as a whole.\nWe can also look to other fields such as medical trials. Both the EU Clinical Trials Register (EUCTR) and the US Food and Drug Administration’s (FDA) ClinicalTrials.gov registries make it a legal requirement to publish clinical trials within 12 months of their completion. So, perhaps at least in some areas that really matter, and where there is a legal requirement to do so, null results don’t sit unpublished? Unfortunately:\n\nGoldacre et al. (2018) found that only 50% of 7274 EU trials were published within that time frame.\nDeVito, Bacon, & Goldacre (2020) found that only 41% of 4209 US trials were published within that time frame and 64% were published ever.\n\nMore extreme values for these conditional probabilities might therefore be more realistic for psychology. In my anecdotal experience, they are more like: P(published | significant) = 0.70 and P(published | nonsignificant) = 0.05.\nLet’s simulate the impact of this on rate of bias for a given literature. In this simulation, each iteration is a given study in the literature, so the number of iterations (25) is much smaller than a typical simulation.\n\n\nCode\n# remove all objects from environment ----\n#rm(list = ls())\n\n\n# dependencies ----\n# repeated here for the sake of completeness \n\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(forcats)\nlibrary(purrr) \nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(janitor)\nlibrary(metafor)\n\n\n# set the seed ----\n# for the pseudo random number generator to make results reproducible\nset.seed(46)\n\n\n# define data generating function ----\ngenerate_data &lt;- function(n_minimum,\n                          n_max,\n                          mean_control,\n                          mean_intervention,\n                          sd_control,\n                          sd_intervention) {\n  require(tibble)\n  require(dplyr)\n  require(forcats)\n  \n  n_per_condition &lt;- runif(n = 1, min = n_minimum, max = n_max)\n  \n  data_control &lt;- \n    tibble(condition = \"control\",\n           score = rnorm(n = n_per_condition, mean = mean_control, sd = sd_control))\n  \n  data_intervention &lt;- \n    tibble(condition = \"intervention\",\n           score = rnorm(n = n_per_condition, mean = mean_intervention, sd = sd_intervention))\n  \n  data &lt;- bind_rows(data_control,\n                    data_intervention) |&gt;\n    # control's factor levels must be ordered so that intervention is the first level and control is the second\n    # this ensures that positive cohen's d values refer to intervention &gt; control and not the other way around.\n    mutate(condition = fct_relevel(condition, \"intervention\", \"control\"))\n  \n  return(data)\n}\n\n\n# define data analysis function ----\nanalyse_data &lt;- function(data, probability_sig_published, probability_nonsig_published) {\n  require(effsize)\n  require(tibble)\n  \n  res_n &lt;- data |&gt;\n    count()\n  \n  res_t_test &lt;- t.test(formula = score ~ condition, \n                       data = data,\n                       var.equal = FALSE,\n                       alternative = \"two.sided\")\n  \n  res_cohens_d &lt;- effsize::cohen.d(formula = score ~ condition,  # new addition: also fit cohen's d\n                                   within = FALSE,\n                                   data = data)\n  \n  res &lt;- tibble(total_n = res_n$n,\n                p = res_t_test$p.value, \n                cohens_d_estimate = res_cohens_d$estimate,  # new addition: save cohen's d and its 95% CIs to the results tibble\n                cohens_d_ci_lower = res_cohens_d$conf.int[\"lower\"],\n                cohens_d_ci_upper = res_cohens_d$conf.int[\"upper\"]) |&gt;\n    mutate(cohens_d_se = (cohens_d_ci_upper - cohens_d_ci_lower)/(1.96*2),\n           cohens_d_variance = cohens_d_se^2) |&gt; # variance of effect size = its standard error squared\n    mutate(\n      # define result as (non)significant\n      significant = p &lt; .05,\n      # generate a random luck probability between 0 and 1\n      luck = runif(n = 1, min = 0, max = 1),\n      # decide if the result is published or not based on whether:\n      # (a) the result was significant and the luck variable is higher than the probability of significant results being published, or\n      # (b) the result was nonsignificant and the luck variable is higher than the probability of nonsignificant results being published\n      published = ifelse((significant & luck &gt;= (1 - probability_sig_published)) |\n                           (!significant & luck &gt;= (1 - probability_nonsig_published)), TRUE, FALSE)\n    )\n  \n  return(res)\n}\n\n\n# define experiment parameters ----\nexperiment_parameters_grid &lt;- expand_grid(\n  n_minimum = 10,\n  n_maximum = 100,\n  mean_control = 0,\n  mean_intervention = 0.25,  \n  sd_control = 1,\n  sd_intervention = 1,\n  probability_sig_published = 0.70, # 0.61 from Franco et al 2014, 2016\n  probability_nonsig_published = 0.05, # 0.22 from Franco et al 2014, 2016\n  iteration = 1:25 # here iterations are studies, so the number is small relative to a normal simulation\n)\n\n\n# run simulation ----\nsimulation &lt;- \n  # using the experiment parameters\n  experiment_parameters_grid |&gt;\n  \n  # generate data using the data generating function and the parameters relevant to data generation\n  mutate(generated_data = pmap(list(n_minimum,\n                                    n_maximum,\n                                    mean_control,\n                                    mean_intervention,\n                                    sd_control,\n                                    sd_intervention),\n                               generate_data)) |&gt;\n  \n  # apply the analysis function to the generated data using the parameters relevant to analysis\n  mutate(analysis_results = pmap(list(generated_data,\n                                      probability_sig_published,\n                                      probability_nonsig_published),\n                                 analyse_data))\n\n\n# summarise simulation results over the iterations ----\nsimulation_unnested &lt;- simulation |&gt;\n  unnest(analysis_results)\n\n\n\n\nCode\n# meta analysis and forest plot\nfit_all &lt;- \n  rma(yi     = cohens_d_estimate, \n      vi     = cohens_d_variance, \n      data   = simulation_unnested,\n      method = \"REML\")\n\nforest(fit_all, header = c(\"All studies conducted (unknowable)\", \"SMD [95% CI]\"), xlab = \"Standardized Mean Difference\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nfit_published &lt;- \n  rma(yi     = cohens_d_estimate, \n      vi     = cohens_d_variance, \n      data   = simulation_unnested |&gt; filter(published == TRUE),\n      method = \"REML\")\n\nforest(fit_published, header = c(\"Published studies\", \"SMD [95% CI]\"), xlab = \"Standardized Mean Difference\")\n\n\n\n\n\n\n\n\n\nNote that the non-overlap between the confidence intervals between the two meta-analyses imply that the published literature has a significantly higher effect size than the actual studies run.\nRemember, however, that (a) our values for the prior probability that (non)significant results are published are not based on any good evidence, and (b) that this only simulates a single literature. These results try to illustrate a point, they don’t comprehensively simulate the potential impact of publication bias across a range of conditions.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/meta_analysis_and_publication_bias.html#power-of-eggers-test-of-publication-bias",
    "href": "chapters/meta_analysis_and_publication_bias.html#power-of-eggers-test-of-publication-bias",
    "title": "18  Meta-analysis and publication bias",
    "section": "18.5 Power of Egger’s Test of publication bias",
    "text": "18.5 Power of Egger’s Test of publication bias\nHow would we simulate at two levels, studies and also meta-analyses of them?\nHow could we assess the power of a method like Egger’s test for funnel plot asymmetry, as implemented by regtest(res, model = \"lm\")?\n[needs work]\n\n\nCode\n# # remove all objects from environment ----\n# #rm(list = ls())\n# \n# \n# # run furrr:::future_map in parallel\n# plan(multisession)\n# \n# # dependencies ----\n# # repeated here for the sake of completeness \n# \n# library(tidyr)\n# library(dplyr)\n# library(tibble)\n# library(forcats)\n# library(purrr) \n# library(ggplot2)\n# library(knitr)\n# library(kableExtra)\n# library(janitor)\n# library(metafor)\n# \n# # define data generating function ----\n# generate_data &lt;- function(n_min,\n#                           n_max,\n#                           mean_control,\n#                           mean_intervention,\n#                           sd_control,\n#                           sd_intervention) {\n#   \n#   \n#   n_per_condition &lt;- runif(n = 1, min = n_min, max = n_max)\n#   \n#   data_control &lt;- \n#     tibble(condition = \"control\",\n#            score = rnorm(n = n_per_condition, mean = mean_control, sd = sd_control))\n#   \n#   data_intervention &lt;- \n#     tibble(condition = \"intervention\",\n#            score = rnorm(n = n_per_condition, mean = mean_intervention, sd = sd_intervention))\n#   \n#   data &lt;- bind_rows(data_control,\n#                     data_intervention) |&gt;\n#     # control's factor levels must be ordered so that intervention is the first level and control is the second\n#     # this ensures that positive cohen's d values refer to intervention &gt; control and not the other way around.\n#     mutate(condition = fct_relevel(condition, \"intervention\", \"control\"))\n#   \n#   return(data)\n# }\n# \n# \n# # define data analysis function ----\n# analyse_study &lt;- function(data, probability_sig_published, probability_nonsig_published) {\n#   \n#   res_n &lt;- data |&gt;\n#     count()\n#   \n#   res_t_test &lt;- t.test(formula = score ~ condition, \n#                        data = data,\n#                        var.equal = FALSE,\n#                        alternative = \"two.sided\")\n#   \n#   res_cohens_d &lt;- effsize::cohen.d(formula = score ~ condition,  # new addition: also fit cohen's d\n#                                    within = FALSE,\n#                                    data = data)\n#   \n#   res &lt;- tibble(total_n = res_n$n,\n#                 p = res_t_test$p.value, \n#                 cohens_d_estimate = res_cohens_d$estimate,  # new addition: save cohen's d and its 95% CIs to the results tibble\n#                 cohens_d_ci_lower = res_cohens_d$conf.int[\"lower\"],\n#                 cohens_d_ci_upper = res_cohens_d$conf.int[\"upper\"]) |&gt;\n#     mutate(cohens_d_se = (cohens_d_ci_upper - cohens_d_ci_lower)/(1.96*2),\n#            cohens_d_variance = cohens_d_se^2) |&gt; # variance of effect size = its standard error squared\n#     mutate(\n#       # define result as (non)significant\n#       significant = p &lt; .05,\n#       # generate a random luck probability between 0 and 1\n#       luck = runif(n = 1, min = 0, max = 1),\n#       # decide if the result is published or not based on whether:\n#       # (a) the result was significant and the luck variable is higher than the probability of significant results being published, or\n#       # (b) the result was nonsignificant and the luck variable is higher than the probability of nonsignificant results being published\n#       published = ifelse((significant & luck &gt;= (1 - probability_sig_published)) |\n#                            (!significant & luck &gt;= (1 - probability_nonsig_published)), TRUE, FALSE)\n#     )\n#   \n#   return(res)\n# }\n# \n# analyse_meta &lt;- function(data){\n#   \n#   data_for_meta &lt;- data |&gt; \n#     filter(published == TRUE)\n#   \n#   if(nrow(data_for_meta) &gt; 0){\n#     fit_meta &lt;- \n#       rma(yi     = cohens_d_estimate, \n#           vi     = cohens_d_variance, \n#           data   = data_for_meta,\n#           method = \"REML\")\n#   } else {\n#     fit_meta &lt;- NULL\n#   }\n#   \n#   return(fit_meta)\n# }\n# \n# results_meta &lt;- function(fit_meta){\n#   results &lt;- tibble(meta_es = as.numeric(fit_meta$b[,1]))\n#   return(results)\n# }\n# \n# analyse_meta_publication_bias &lt;- function(fit_meta) {\n#   # sometimes 0 studies are published, so we need ways to handle this absence of data which would cause an error\n#   safe_regtest &lt;- possibly(\n#     function(fit) {\n#       fit_egger &lt;- regtest(fit, model = \"lm\")\n#       tibble(\n#         egger_p = ifelse(!is.nan(fit_egger$pval), fit_egger$pval, NA_real_),\n#         egger_corrected_es = ifelse(!is.nan(fit_egger$est), fit_egger$est, NA_real_)\n#       )\n#     },\n#     otherwise = tibble(egger_p = NA_real_, egger_corrected_es = NA_real_)\n#   )\n#   \n#   if (is.null(fit_meta)) {\n#     return(tibble(egger_p = NA_real_, egger_corrected_es = NA_real_))\n#   }\n#   \n#   safe_regtest(fit_meta)\n# }\n# \n# \n# # define experiment parameters ----\n# experiment_parameters_grid &lt;- bind_rows(\n#   expand_grid(\n#     n_min = 10,\n#     n_max = 100,\n#     mean_control = 0,\n#     mean_intervention = c(0, 0.2, 0.5, 0.8),  \n#     sd_control = 1,\n#     sd_intervention = 1,\n#     probability_sig_published = 0.70, # 0.61 from Franco et al 2014, 2016\n#     probability_nonsig_published = 0.05, # 0.22 from Franco et al 2014, 2016\n#     iteration_meta = 1:1000, # here iterations are meta analyses\n#     k_studies = 10,  # for reference later, number must match max iterations below\n#     iteration_study = 1:10 # here iterations are studies\n#   ),\n#   expand_grid(\n#     n_min = 10,\n#     n_max = 100,\n#     mean_control = 0,\n#     mean_intervention = c(0, 0.2, 0.5, 0.8),  \n#     sd_control = 1,\n#     sd_intervention = 1,\n#     probability_sig_published = 0.70, # 0.61 from Franco et al 2014, 2016\n#     probability_nonsig_published = 0.05, # 0.22 from Franco et al 2014, 2016\n#     iteration_meta = 1:1000, # here iterations are meta analyses\n#     k_studies = 20,  # for reference later, number must match max iterations below\n#     iteration_study = 1:20 # here iterations are studies\n#   ),\n#   expand_grid(\n#     n_min = 10,\n#     n_max = 100,\n#     mean_control = 0,\n#     mean_intervention = c(0, 0.2, 0.5, 0.8),  \n#     sd_control = 1,\n#     sd_intervention = 1,\n#     probability_sig_published = 0.70, # 0.61 from Franco et al 2014, 2016\n#     probability_nonsig_published = 0.05, # 0.22 from Franco et al 2014, 2016\n#     iteration_meta = 1:1000, # here iterations are meta analyses\n#     k_studies = 30,  # for reference later, number must match max iterations below\n#     iteration_study = 1:30 # here iterations are studies\n#   )\n# )\n# \n# if(file.exists(\"simulation_summary.rds\")) {\n#   simulation_summary &lt;- read_rds(\"../data/simulation_summary_12.rds\")\n# } else {\n#   \n#   # set the seed ----\n#   # for the pseudo random number generator to make results reproducible\n#   set.seed(46)\n#   \n#   # run study level simulation ----\n#   simulation_studies &lt;- \n#     # using the experiment parameters\n#     experiment_parameters_grid |&gt;\n#     \n#     # generate data using the data generating function and the parameters relevant to data generation\n#     mutate(generated_data = future_pmap(list(n_min = n_min,\n#                                              n_max = n_max,\n#                                              mean_control = mean_control,\n#                                              mean_intervention = mean_intervention,\n#                                              sd_control = sd_control,\n#                                              sd_intervention = sd_intervention),\n#                                         generate_data,\n#                                         .progress = TRUE,\n#                                         .options = furrr_options(seed = TRUE))) |&gt;\n#     \n#     # apply the analysis function to the generated data using the parameters relevant to analysis\n#     mutate(analysis_results_study = future_pmap(list(data = generated_data,\n#                                                      probability_sig_published = probability_sig_published,\n#                                                      probability_nonsig_published = probability_nonsig_published),\n#                                                 analyse_study,\n#                                                 .progress = TRUE,\n#                                                 .options = furrr_options(seed = TRUE)))\n#   \n#   \n#   # run meta-analysis simulation ----\n#   simulation_meta &lt;- simulation_studies |&gt;\n#     unnest(analysis_results_study) |&gt;\n#     select(-generated_data) |&gt;\n#     group_by(n_min, n_max, mean_control, mean_intervention, sd_control, sd_intervention, probability_sig_published, probability_nonsig_published, iteration_meta, k_studies) |&gt;\n#     nest(.key = \"meta_data\") |&gt;\n#     ungroup() |&gt;\n#     # at this point we have the equivalent of the usual steps up to and including generate data. next analyze that data\n#     mutate(analysis_meta = future_pmap(list(data = meta_data),\n#                                        analyse_meta,\n#                                        .progress = TRUE,\n#                                        .options = furrr_options(seed = TRUE))) |&gt;\n#     # extract meta ES\n#     mutate(meta_es = future_pmap(list(fit_meta = analysis_meta),\n#                                  results_meta,\n#                                  .progress = TRUE,\n#                                  .options = furrr_options(seed = TRUE)),\n#            .progress = TRUE,\n#            seed = TRUE) |&gt;\n#     unnest(meta_es) |&gt;\n#     # run eggers test\n#     mutate(analysis_meta_publication_bias = future_pmap(list(fit_meta = analysis_meta),\n#                                                         analyse_meta_publication_bias,\n#                                                         .progress = TRUE,\n#                                                         .options = furrr_options(seed = TRUE)))\n#   \n#   # summarize across meta iterations\n#   simulation_summary &lt;- simulation_meta |&gt;\n#     unnest(analysis_meta_publication_bias) |&gt;\n#     rename(population_es = mean_intervention) |&gt;\n#     group_by(population_es, probability_sig_published, probability_nonsig_published, k_studies) |&gt;\n#     summarize(mean_meta_es = mean(meta_es), \n#               egger_proportion_significant = mean(egger_p &lt; .05, na.rm = TRUE),\n#               mean_egger_corrected_es = mean(egger_corrected_es, na.rm = TRUE))\n#   \n#   write_rds(simulation_summary, \"../data/simulation_summary_12.rds\")\n# }\n# \n# # print table\n# simulation_summary |&gt;\n#   mutate_if(is.numeric, round_half_up, digits = 2) |&gt;\n#   kable() |&gt;\n#   kable_classic(full_width = FALSE)\n\n# Error in `mutate()`:\n# ℹ In argument: `analysis_meta = future_pmap(...)`.\n# Caused by error:\n# ℹ In index: 384.\n# Caused by error in `rma()`:\n# ! Fisher scoring algorithm did not converge. See 'help(rma)' for possible remedies.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/causality.html",
    "href": "chapters/causality.html",
    "title": "19  Regression assumes causality",
    "section": "",
    "text": "19.1 Dependencies\nCode\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(purrr) \nlibrary(parameters)\nlibrary(lavaan)\nlibrary(semPlot)\nlibrary(knitr)\nlibrary(kableExtra)",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Regression assumes causality</span>"
    ]
  },
  {
    "objectID": "chapters/causality.html#functions",
    "href": "chapters/causality.html#functions",
    "title": "19  Regression assumes causality",
    "section": "19.2 Functions",
    "text": "19.2 Functions\n\n\nCode\ngenerate_data &lt;- function(n, population_model) {\n  \n  data &lt;- lavaan::simulateData(model = population_model, sample.nobs = n) \n  \n  return(data)\n}\n\nanalyse &lt;- function(data, model) {\n  \n  # specify and fit model\n  fit &lt;- sem(model = model, data = data)\n  #fit &lt;- lm(formula = mode, data = data)\n  \n  # extract regression beta estimates \n  results &lt;- parameters::model_parameters(fit, standardize = FALSE) |&gt;\n    filter(To == \"Y\" & From == \"X\") |&gt;  # this corresponds to the Y ~ X effect\n    select(beta = Coefficient,\n           ci_lower = CI_low,\n           ci_upper = CI_high,\n           p) \n  \n  return(results)\n}\n\n plots_causal &lt;- function(model){\n  generate_data(n = 300, population_model = model) %&gt;%\n    sem(model = model, data = .) |&gt;\n    semPaths(whatLabels = \"diagram\", \n             layout = layout_matrix, \n             residuals = FALSE,\n             edge.label.cex = 1.2, \n             sizeMan = 10)\n}",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Regression assumes causality</span>"
    ]
  },
  {
    "objectID": "chapters/causality.html#does-regression-assume-causality",
    "href": "chapters/causality.html#does-regression-assume-causality",
    "title": "19  Regression assumes causality",
    "section": "19.3 Does regression assume causality?",
    "text": "19.3 Does regression assume causality?\n\n19.3.1 Plots\nSimple regression: X causes Y\n\n\nCode\n# simple regression\nlayout_matrix &lt;- matrix(c( 1,  0,\n                           -1,  0), \n                        ncol = 2, \n                        byrow = TRUE)\n\nplots_causal(\"Y ~ 0.5*X\")\n\n\n\n\n\n\n\n\n\nSimple regression: Y causes X\n\n\nCode\n# simple regression\nlayout_matrix &lt;- matrix(c(-1,  0,\n                           1,  0), \n                        ncol = 2, \n                        byrow = TRUE)\n\nplots_causal(\"X ~ 0.5*Y\")\n\n\n\n\n\n\n\n\n\n\n\n19.3.2 Run simulation\n\n\nCode\nexperiment_parameters_grid &lt;- expand_grid(\n  n = 200,\n  population_model = c(\"Y ~ 0.5*X\",\n                       \"X ~ 0.5*Y\"),\n  analyse_model = \"Y ~ X\",\n  iteration = 1:1000\n) \n\nset.seed(42)\n\nsimulation &lt;- \n  # using the experiment parameters...\n  experiment_parameters_grid |&gt;\n  \n  # ...generate data... \n  mutate(generated_data = pmap(list(n = n,\n                                    population_model = population_model),\n                               generate_data)) |&gt;\n  # ...analyze data \n  mutate(results = pmap(list(data = generated_data,\n                             model = analyse_model),\n                        analyse))\n\n\n\n\n19.3.3 Summarize results\n\n\nCode\nsimulation_summary &lt;- simulation |&gt;\n  unnest(results) |&gt;\n  group_by(n,\n           population_model,\n           analyse_model) |&gt;\n  summarize(mean_beta = mean(beta),\n            proportion_signficiant = mean(p &lt; .05))\n\nsimulation_summary |&gt;\n  mutate_if(is.numeric, janitor::round_half_up, digits = 2) |&gt;\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\nn\npopulation_model\nanalyse_model\nmean_beta\nproportion_signficiant\n\n\n\n\n200\nX ~ 0.5*Y\nY ~ X\n0.4\n1\n\n\n200\nY ~ 0.5*X\nY ~ X\n0.5\n1",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Regression assumes causality</span>"
    ]
  },
  {
    "objectID": "chapters/causality.html#collider",
    "href": "chapters/causality.html#collider",
    "title": "19  Regression assumes causality",
    "section": "19.4 Collider",
    "text": "19.4 Collider\n\n19.4.1 Plots\nCovariate is a confounder:\n\n\nCode\nlayout_matrix &lt;- matrix(c( 1,  0,\n                           -1,  0,\n                           0,  1), \n                        ncol = 2, \n                        byrow = TRUE)\n\nplots_causal(\"Y ~ 0.0*X + 0.5*C; X ~ 0.5*C\")\n\n\n\n\n\n\n\n\n\nConfounder is a collider:\n\n\nCode\nlayout_matrix &lt;- matrix(c( 0,  1,\n                           1,  0,\n                           -1,  0), \n                        ncol = 2, \n                        byrow = TRUE)\n\nplots_causal(\"C ~ 0.5*X + 0.5*Y; Y ~ 0.0*X\")\n\n\n\n\n\n\n\n\n\n\n\n19.4.2 Run simulation\n\n\nCode\nexperiment_parameters_grid &lt;- expand_grid(\n  n = 200,\n  population_model = c(\"Y ~ 0.5*X + 0.0*C\",\n                       \"C ~ 0.5*X + 0.5*Y; Y ~~ 0.0*X\"),\n  analyse_model = \"Y ~ X + C\",\n  iteration = 1:1000\n)\n\nset.seed(42)\n\nsimulation &lt;- \n  # using the experiment parameters...\n  experiment_parameters_grid |&gt;\n  \n  # ...generate data...\n  mutate(generated_data = pmap(list(n = n,\n                                    population_model = population_model),\n                               generate_data)) |&gt;\n  # ...analyze data\n  mutate(results = pmap(list(data = generated_data,\n                             model = analyse_model),\n                        analyse))\n\n\n\n\n19.4.3 Summarize results\n\n\nCode\nsimulation_summary &lt;- simulation |&gt;\n  unnest(results) |&gt;\n  group_by(n,\n           population_model,\n           analyse_model) |&gt;\n  summarize(mean_beta = mean(beta),\n            proportion_signficiant = mean(p &lt; .05))\n\nsimulation_summary |&gt;\n  mutate_if(is.numeric, janitor::round_half_up, digits = 2) |&gt;\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\nn\npopulation_model\nanalyse_model\nmean_beta\nproportion_signficiant\n\n\n\n\n200\nC ~ 0.5*X + 0.5*Y; Y ~~ 0.0*X\nY ~ X + C\n-0.2\n0.82\n\n\n200\nY ~ 0.5*X + 0.0*C\nY ~ X + C\n0.5\n1.00",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Regression assumes causality</span>"
    ]
  },
  {
    "objectID": "chapters/license.html",
    "href": "chapters/license.html",
    "title": "20  License and citation",
    "section": "",
    "text": "© Ian Hussey (2025)\nText and figures are licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) license.\nCode is licensed under the MIT License.\nYou are free to copy, share, adapt, and reuse the contents of this book — text, figures, and code — for any purpose, including commercial use, provided you cite it.\nCitation:\nHussey, I. (2025) Improving your statistical inferences using Monte Carlo simulation studies in tidyverse. github.com/ianhussey/improving-your-statistical-inferences-through-monte-carlo-simulation-studies",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>License and citation</span>"
    ]
  }
]