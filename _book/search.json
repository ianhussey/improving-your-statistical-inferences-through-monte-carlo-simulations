[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Improving your statistical inferences through Monte Carlo simulations",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/1_foundational_concepts.html",
    "href": "chapters/1_foundational_concepts.html",
    "title": "1  Foundation concepts",
    "section": "",
    "text": "1.1 One line Monte Carlo simulation\nAnswering the question “what is the distribution of p-values under the null hypothesis?”\nCode\nset.seed(42)\n\nhist(replicate(100000, t.test(rnorm(n = 50, m = 0, sd = 1), rnorm(n = 50, m = 0, sd = 1))$p.value))\nThis is a complete Monte Carlo simulation. But how is it constructed? Let’s built it up more slowly.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundation concepts</span>"
    ]
  },
  {
    "objectID": "chapters/1_foundational_concepts.html#generate-normally-distributed-data",
    "href": "chapters/1_foundational_concepts.html#generate-normally-distributed-data",
    "title": "1  Foundation concepts",
    "section": "1.2 Generate normally distributed data",
    "text": "1.2 Generate normally distributed data\nDraw data from a normally distributed population and plot it using rnorm()\n\n\nCode\nset.seed(42)\n\nrnorm(n = 50, m = 0, sd = 1) |&gt; \n  hist()",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundation concepts</span>"
    ]
  },
  {
    "objectID": "chapters/1_foundational_concepts.html#generate-two-normally-distributed-data-sets-and-fit-a-t-test",
    "href": "chapters/1_foundational_concepts.html#generate-two-normally-distributed-data-sets-and-fit-a-t-test",
    "title": "1  Foundation concepts",
    "section": "1.3 Generate two normally distributed data sets and fit a t-test",
    "text": "1.3 Generate two normally distributed data sets and fit a t-test\nNote the population means are equivalent. What does a t-test test for?\n\n\nCode\nset.seed(42)\n\nt.test(rnorm(n = 50, m = 0, sd = 1), \n       rnorm(n = 50, m = 0, sd = 1))\n\n\n\n    Welch Two Sample t-test\n\ndata:  rnorm(n = 50, m = 0, sd = 1) and rnorm(n = 50, m = 0, sd = 1)\nt = -0.65289, df = 93.647, p-value = 0.5154\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.5511248  0.2783784\nsample estimates:\n  mean of x   mean of y \n-0.03567178  0.10070141",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundation concepts</span>"
    ]
  },
  {
    "objectID": "chapters/1_foundational_concepts.html#generate-two-normally-distributed-data-sets-and-fit-a-t-test-and-extract-the-p-value",
    "href": "chapters/1_foundational_concepts.html#generate-two-normally-distributed-data-sets-and-fit-a-t-test-and-extract-the-p-value",
    "title": "1  Foundation concepts",
    "section": "1.4 Generate two normally distributed data sets and fit a t-test and extract the p-value",
    "text": "1.4 Generate two normally distributed data sets and fit a t-test and extract the p-value\n\n\nCode\nset.seed(42)\n\nt.test(rnorm(n = 50, m = 0, sd = 1), rnorm(n = 50, m = 0, sd = 1))$p.value\n\n\n[1] 0.5154295",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundation concepts</span>"
    ]
  },
  {
    "objectID": "chapters/1_foundational_concepts.html#do-this-generate-analyze-extract-many-times",
    "href": "chapters/1_foundational_concepts.html#do-this-generate-analyze-extract-many-times",
    "title": "1  Foundation concepts",
    "section": "1.5 Do this generate-analyze-extract many times",
    "text": "1.5 Do this generate-analyze-extract many times\nUsing the replicate() function, and plot the p-values using hist().\n\n\nCode\nset.seed(42)\n\nhist(replicate(1000, t.test(rnorm(n = 50, m = 0, sd = 1), rnorm(n = 50, m = 0, sd = 1))$p.value))",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundation concepts</span>"
    ]
  },
  {
    "objectID": "chapters/1_foundational_concepts.html#increase-the-number-of-iterations",
    "href": "chapters/1_foundational_concepts.html#increase-the-number-of-iterations",
    "title": "1  Foundation concepts",
    "section": "1.6 Increase the number of iterations",
    "text": "1.6 Increase the number of iterations\nWhy is the distribution more uniform?\n\n\nCode\nset.seed(42)\n\nhist(replicate(100000, t.test(rnorm(n = 50, m = 0, sd = 1), rnorm(n = 50, m = 0, sd = 1))$p.value))",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundation concepts</span>"
    ]
  },
  {
    "objectID": "chapters/1_foundational_concepts.html#compare-with-distribution-of-p-values-when-population-means-are-not-equal",
    "href": "chapters/1_foundational_concepts.html#compare-with-distribution-of-p-values-when-population-means-are-not-equal",
    "title": "1  Foundation concepts",
    "section": "1.7 Compare with distribution of p-values when population means are not equal",
    "text": "1.7 Compare with distribution of p-values when population means are not equal\n‘Small’, ‘medium’ and ‘large’ differences in population means.\n\n\nCode\nset.seed(42)\n\nhist(replicate(100000, t.test(rnorm(n = 50, m = 0, sd = 1), rnorm(n = 50, m = 0.2, sd = 1))$p.value))\n\n\n\n\n\n\n\n\n\nCode\nhist(replicate(100000, t.test(rnorm(n = 50, m = 0, sd = 1), rnorm(n = 50, m = 0.5, sd = 1))$p.value))\n\n\n\n\n\n\n\n\n\nCode\nhist(replicate(100000, t.test(rnorm(n = 50, m = 0, sd = 1), rnorm(n = 50, m = 0.8, sd = 1))$p.value))",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundation concepts</span>"
    ]
  },
  {
    "objectID": "chapters/1_foundational_concepts.html#check-your-learning",
    "href": "chapters/1_foundational_concepts.html#check-your-learning",
    "title": "1  Foundation concepts",
    "section": "1.8 Check your learning",
    "text": "1.8 Check your learning\n\nWhat are the five core components of a Monte Carlo simulation?\nCan you mentally match these five components onto the above code (sometimes in different chunks!) to understand how, together, they answer the question “what is the distribution of p-values under the null vs. alternative hypothesis?”",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundation concepts</span>"
    ]
  },
  {
    "objectID": "chapters/2_writing_functions.html",
    "href": "chapters/2_writing_functions.html",
    "title": "2  Practice writing R functions",
    "section": "",
    "text": "2.1 Overview\nTwo of the key steps in a simulation study (generate data and analyze data) require us to know how to write functions. This R Markdown lesson practices this.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Practice writing R functions</span>"
    ]
  },
  {
    "objectID": "chapters/2_writing_functions.html#dependencies",
    "href": "chapters/2_writing_functions.html#dependencies",
    "title": "2  Practice writing R functions",
    "section": "2.2 Dependencies",
    "text": "2.2 Dependencies\n\n\nCode\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(forcats)\nlibrary(report)\nlibrary(janitor)\nlibrary(faux)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Practice writing R functions</span>"
    ]
  },
  {
    "objectID": "chapters/2_writing_functions.html#primer-on-functions",
    "href": "chapters/2_writing_functions.html#primer-on-functions",
    "title": "2  Practice writing R functions",
    "section": "2.3 Primer on functions",
    "text": "2.3 Primer on functions\nMost code we use are functions, e.g., mean(), setwd() and library().\nThese functions were written by others, but we can write our own.\n“It’s functions all the down”: you will use existing functions to write new ones. For example:\n\n\nCode\nvalues &lt;- c(4, 2, 6, 2, NA, 4, 3, 1, NA, 7, 5)\n\nmean(values) # returns NA \n\n\n[1] NA\n\n\nCode\nmean(values, na.rm = TRUE) # returns the mean after dropping NA\n\n\n[1] 3.777778\n\n\nCode\n# tired of writing 'na.rm = TRUE' repeatedly? write your own function to do it automatically\nmean_na_rm &lt;- function(x){\n  mean(x, na.rm = TRUE)\n}\n\nmean_na_rm(values) # returns the mean after dropping NA\n\n\n[1] 3.777778\n\n\nWhat if we usually want to round() to two decimal places, and we’re tired of writing digits = 2 every time?\n\n\nCode\nmean_of_values &lt;- mean_na_rm(values)\n\nround(mean_of_values, digits = 2)\n\n\n[1] 3.78\n\n\nCode\n# write a function to always round to two decimal places\nround_2 &lt;- function(x){\n  round(x, digits = 2)\n}\n\nround_2(mean_of_values)\n\n\n[1] 3.78\n\n\n\n2.3.1 Mini lesson: round() probably doesn’t do what you think it does\nround() uses “banker’s rounding” rather than the round-half-up method we’re used to\n\n\nCode\nround(0.5)\n\n\n[1] 0\n\n\nCode\nround(1.5)\n\n\n[1] 2\n\n\nCode\nround(2.5)\n\n\n[1] 2\n\n\nCode\nround(3.5)\n\n\n[1] 4\n\n\nCode\nround(4.5)\n\n\n[1] 4\n\n\nCode\nround(5.5)\n\n\n[1] 6\n\n\n\n\nCode\njanitor::round_half_up(0.5)\n\n\n[1] 1\n\n\nCode\njanitor::round_half_up(1.5)\n\n\n[1] 2\n\n\nCode\njanitor::round_half_up(2.5)\n\n\n[1] 3\n\n\nCode\njanitor::round_half_up(3.5)\n\n\n[1] 4\n\n\nCode\njanitor::round_half_up(4.5)\n\n\n[1] 5\n\n\nCode\njanitor::round_half_up(5.5)\n\n\n[1] 6\n\n\n\n\n2.3.2 General structure of a function\nFunctions (usually) have ‘inputs’, they have code that they run (‘do stuff’), and they (almost always) return ‘outputs’. The often specify their requirements and include checks that their inputs are correctly formatted.\nNote that this is pseudo-code only: chunk is set not to run (eval=FALSE).\n\n\nCode\n# define function\nfunction_name &lt;- function(argument_1, # first argument is often the data, if the function takes a data frame as an argument\n                          argument_2 = \"default\", # arguments can have defaults\n                          argument_3) {\n  # required packages\n  require(dplyr)\n  \n  # checks\n  # well written functions contain checks. \n  # e.g., if the function assumes that argument_1 is a data frame, check that this is the case.\n  # note that it is more useful to write the function first and add checks later.\n  if(!is.data.frame(argument_1)){\n    stop(\"argument_1 must be a data frame\")\n  }\n  \n  # code that does things\n  object_to_be_returned &lt;- input_data_frame |&gt;\n    # do things\n    mutate(value = value + 1)\n  \n  # object to be returned\n  return(object_to_be_returned)\n}\n\n\n\n\n2.3.3 Example function: t-test p-value\n\n\nCode\n# data to be analyzed using the analysis function\ndata_simulated_intervention &lt;- \n  tibble(condition = \"intervention\", \n         score = rnorm(n = 50, mean = 0, sd = 1))\n\ndata_simulated_control &lt;- \n  tibble(condition = \"control\", \n         score = rnorm(n = 50, mean = 0, sd = 1))\n\ndata_simulated &lt;- \n  bind_rows(data_simulated_intervention,\n            data_simulated_control)\n\n\n\n\nCode\n# define function\nt_test_p_value &lt;- function(data) {\n\n  res &lt;- t.test(formula = score ~ condition, \n                data = data)\n  \n  return(res$p.value)\n}\n\n# call function\nt_test_p_value(data_simulated)\n\n\n[1] 0.9314455\n\n\nHow would I build this from scratch? What’s the first thing I would type?\n\n\n2.3.4 General things to remember when writing functions\n\nIf you can’t immediately write the code, write pseudo-code first!\nBuild the ‘do stuff’ part outside of a function first!\nWrap the ‘do stuff’ with input and output after you have ‘do stuff’ working. Why: so you don’t have to fight variable scoping.\nThe function must be present in your environment to be usable, and must be called to be used\nCheck that your function actually works as you expect, not just that it runs. Give it lots of different input values that should and should not work, and check you get the correct outputs.\nDon’t try to abstract more than you need. One function should do one thing. Elaborate the function only as needed.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Practice writing R functions</span>"
    ]
  },
  {
    "objectID": "chapters/2_writing_functions.html#in-class-and-at-home-exercise-practice-writing-functions",
    "href": "chapters/2_writing_functions.html#in-class-and-at-home-exercise-practice-writing-functions",
    "title": "2  Practice writing R functions",
    "section": "2.4 In class and at home exercise: Practice writing functions",
    "text": "2.4 In class and at home exercise: Practice writing functions\nWrite functions below that can be applied to the following data sets, i.e., use these data sets to guide how you write the functions and test that they work.\nTry and use chatGPT as little as possible.\nIf your existing knowledge of data processing/wrangling with {tidyverse}/{dplyr}/{tidyr} isn’t good, start with\n\n\nCode\nset.seed(42)\n\n# data for t tests\ndata_intervention &lt;- \n  tibble(condition = \"intervention\", \n         score = rnorm(n = 50, mean = 0, sd = 1))\n\ndata_control &lt;- \n  tibble(condition = \"control\", \n         score = rnorm(n = 50, mean = 0, sd = 1))\n\ndata_combined_ttest &lt;- \n  bind_rows(data_intervention,\n            data_control) |&gt;\n  # control's factor levels must be ordered so that intervention is the first level and control is the second\n  # this ensures that positive Cohen's d values refer to intervention &gt; control and not the other way around.\n  mutate(condition = fct_relevel(condition, \"intervention\", \"control\"))\n\n\n# data for correlations\ndata_correlation &lt;- rnorm_multi(n = 100, \n                                vars = 2, \n                                mu = 0, \n                                sd = 1, \n                                r = 0.5, \n                                varnames = c(\"X\", \"Y\"))\n\n\n\n2.4.1 Calculate mean\n\nUse dplyr::summarize().\nUse the data_intervention data set.\nReturn results in a data frame.\n\n\n\n2.4.2 Calculate SD\n\nUse dplyr::summarize().\nUse the data_intervention data set.\nReturn results in a data frame.\n\n\n\n2.4.3 Calculate mean for each condition\n\nUse dplyr::summarize() and group_by().\nUse the data_combined_ttest data set.\nReturn results in a data frame.\n\n\n\n2.4.4 Calculate mean and SD for each condition\n\nUse dplyr::summarize() and group_by().\nUse the data_combined_ttest data set.\nReturn results in a data frame.\n\n\n\n2.4.5 Cohen’s d and its 95% Confidence Intervals\n\nCalculate Cohen’s d using effsize::cohen.d() and extract the Cohen’s d estimate.\nUse the data_combined_ttest data set.\nReturn results in a data frame.\n\n\n\n2.4.6 t-test’s p-value, Cohen’s d and its 95% Confidence Intervals\n\nCalculate Cohen’s d using effsize::cohen.d() and extract the estimate.\nAlso fit a Student’s t-test and extract its p value.\nUse the data_combined_ttest data set.\nReturn results (d and p) in a data frame.\n\n\n\n2.4.7 Pearson’s r from correlation test\n\nFit a correlation test using cor.test() and extract the correlation estimate.\nUse the data_correlation data set.\nReturn results in a data frame.\n\n\n\n2.4.8 p-value from correlation test\n\nFit a correlation test using cor.test() and extract the p value.\nUse the data_correlation data set.\nReturn results in a data frame.\n\n\n\n2.4.9 Pearson’s r and its p-value from cor.test()\n\nFit a correlation test using cor.test() and extract the p value and correlation.\nUse the data_correlation data set.\nReturn results in a data frame.\n\n\n\n2.4.10 Generate data for a between groups design\nRather than writing a data analysis function, this time write a data generation function. In the previous chunks we’ve used this code to generate a single data set with intervention and control conditions and simulated normally distributed data. Rewrite this as a function so that we can generate such a data set with one line of code using the new function generate_data(). Unlike your previous functions, this one has no inputs, i.e., you can write function() &lt;-.\n\n\nCode\n# # code to convert into a function\n# data_simulated_intervention &lt;- tibble(condition = \"intervention\", \n#                                       score = rnorm(n = 50, mean = 0, sd = 1))\n# \n# data_simulated_control &lt;- tibble(condition = \"control\", \n#                                  score = rnorm(n = 50, mean = 0, sd = 1))\n# \n# data_simulated &lt;- \n#   bind_rows(data_simulated_intervention,\n#             data_simulated_control)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Practice writing R functions</span>"
    ]
  },
  {
    "objectID": "chapters/2_writing_functions.html#further-reading",
    "href": "chapters/2_writing_functions.html#further-reading",
    "title": "2  Practice writing R functions",
    "section": "2.5 Further reading",
    "text": "2.5 Further reading\nAlthough we have practiced writing custom functions to extract statistical results / model parameters, it is worth knowing that the {easystats} family of packages includes {parameters} package, which does a very good job of extracting model parameters from a very wide range of models including base R functions, {lavaan}, {psych}, and other packages. If you want to extract values from a model, consider using {parameters} to do a lot of the work for you when writing your function.\nSeparately, the {report} package will fully report the results of many common analyses for you. e.g.:\n\n\nCode\ndata_simulated_intervention &lt;- \n  tibble(condition = \"intervention\", \n         score = rnorm(n = 50, mean = 0, sd = 1))\n\ndata_simulated_control &lt;- \n  tibble(condition = \"control\", \n         score = rnorm(n = 50, mean = 0, sd = 1))\n\ndata_simulated &lt;- \n  bind_rows(data_simulated_intervention,\n            data_simulated_control)\n\nt.test(score ~ condition, data = data_simulated) |&gt;\n  report::report()\n\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Welch Two Sample t-test testing the difference of score by condition (mean\nin group control = 0.13, mean in group intervention = -0.06) suggests that the\neffect is positive, statistically not significant, and small (difference =\n0.19, 95% CI [-0.16, 0.54], t(95.08) = 1.08, p = 0.283; Cohen's d = 0.22, 95%\nCI [-0.18, 0.62])\n\n\nThis lesson does not cover documenting your functions well, organizing them into an R package to make them easy to load and include help menus, or writing unit tests them. These are all very worth doing. Look into the {roxygen} package.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Practice writing R functions</span>"
    ]
  },
  {
    "objectID": "chapters/3_general_structure_of_a_simulation_1.html",
    "href": "chapters/3_general_structure_of_a_simulation_1.html",
    "title": "3  General structure of a simulation study",
    "section": "",
    "text": "3.1 Overview of tutorial\nThis tutorial teaches you about the essential components of a simulation study and the general steps involved in actually writing one. This lesson covers the steps of writing functions to generate data and analyze that data within the workflow that we will use throughout this course.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>General structure of a simulation study</span>"
    ]
  },
  {
    "objectID": "chapters/3_general_structure_of_a_simulation_1.html#essential-components-general-steps",
    "href": "chapters/3_general_structure_of_a_simulation_1.html#essential-components-general-steps",
    "title": "3  General structure of a simulation study",
    "section": "3.2 Essential components & general steps",
    "text": "3.2 Essential components & general steps\nThe essential components of a simulation are:\n\nGenerate pseudo-random data set with known properties\nAnalyse data with a statistical method\nRepeat 1 & 2 many times (‘iterations’)\nCollect and aggregate results across iterations\nMake it an experiment: Systematically vary parameters in Step 1 (between factor) and/or compare different ways to do Step 2 (within factor)\n\nHowever, the way you build a simulation is much more iterative than this final product. Each component must be both built and inspected by itself, and then checked to make sure that the components work together in the correct manner, like a fine Swiss watch.\nThe general steps for actually building one involve:\n\nGenerate a single pseudo-random data set with known properties, using hard-coded variables\nAnalyse this single data set with a statistical method, using hard-coded variables\nConvert the data generation code to a function, making it as abstract as necessary [Component 1]\nConvert the data analysis code to a function, making it as abstract as necessary [Component 2]\nEnsure that experiment parameters, data generation function, and data analysis code play together nicely (i.e., can pass values between one another in a workflow, and save values appropriately [Component 4]). Do this using a small number of parameters and just one or two iterations.\nIncrease the number of iterations and parameters and actually run the simulation as a whole [Component 4 and 5]\nCheck the assumptions of the simulation have been adequately met\nInterpret the results of the simulation\n\nThis tutorial focuses on the general practical steps, but always keep the essential components in mind as the guiding principles of why we’re doing something.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>General structure of a simulation study</span>"
    ]
  },
  {
    "objectID": "chapters/3_general_structure_of_a_simulation_1.html#dependencies",
    "href": "chapters/3_general_structure_of_a_simulation_1.html#dependencies",
    "title": "3  General structure of a simulation study",
    "section": "3.3 Dependencies",
    "text": "3.3 Dependencies\n\n\nCode\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(readr)\nlibrary(purrr) \nlibrary(ggplot2)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>General structure of a simulation study</span>"
    ]
  },
  {
    "objectID": "chapters/3_general_structure_of_a_simulation_1.html#generate-data-and-apply-and-analysis-once-using-hard-coded-arguments",
    "href": "chapters/3_general_structure_of_a_simulation_1.html#generate-data-and-apply-and-analysis-once-using-hard-coded-arguments",
    "title": "3  General structure of a simulation study",
    "section": "3.4 Generate data and apply and analysis once using hard coded arguments",
    "text": "3.4 Generate data and apply and analysis once using hard coded arguments\n\n3.4.1 Generate some data for an independent t-test\n\ncondition: factor with two levels (control, intervention)\nscore: numeric of normally distributed data (within each condition), with different means, and SD = 1.\ngiven that cohen’s d = (m2 - m1)/SD_pooled, simply setting SDs to 1 and mean_control to 0 lets us control the cohen’s d = mean_intervention\nuse the R function rnorm\n\nTo sample data from a normally distributed population, we use the pseudo-random number generator for normal data built into R: rnorm(). Note that many packages exist to help you generate different types of data more easily, including {MASS}, {faux}, {simstudy}, and {lavaan}. These can be very helpful, but here we’ll do it manually for the moment.\n\n3.4.1.1 Try it yourself first\nFor each ‘try it yourself first’ exercise in this lesson, work with the person next to you to try to write the code from scratch. Don’t look at the solution below until you’re done.\nThe point is to practice not just receptive language (understanding my code, or chatGPT’s code) but to practice productive language yourself. After you make an attempt yourself, we’ll work as a group to do it together.\n\n\n3.4.1.2 Solution\n\n\nCode\ndata_control &lt;- \n  tibble(condition = \"control\",\n         score = rnorm(n = 50, mean = 0, sd = 1))\n\ndata_intervention &lt;- \n  tibble(condition = \"intervention\",\n         score = rnorm(n = 50, mean = 0.50, sd = 1))\n\ndata_combined &lt;- bind_rows(data_control,\n                           data_intervention)\n\nView(data_combined)\n\n\nNote that the above code is equivalent to the below. The above is easier to start with when you’re learning, the below is faster to write when you’re experienced.\n\n\nCode\ndata_combined &lt;- bind_rows(\n  tibble(condition = \"control\",\n         score = rnorm(n = 50, mean = 0, sd = 1)),\n  tibble(condition = \"intervention\",\n         score = rnorm(n = 50, mean = 0.50, sd = 1))\n)\n\n\n\n\n\n3.4.2 Fit a Student’s independent t-test\nTo the data we just generated.\nUse t.test()\n\n3.4.2.1 Try it yourself\n\n\n3.4.2.2 Solution\n\n\nCode\nt.test(formula = score ~ condition, \n       data = data_combined,\n       var.equal = TRUE,\n       alternative = \"two.sided\")\n\n\n\n    Two Sample t-test\n\ndata:  score by condition\nt = -3.0498, df = 98, p-value = 0.002945\nalternative hypothesis: true difference in means between group control and group intervention is not equal to 0\n95 percent confidence interval:\n -1.0502797 -0.2222606\nsample estimates:\n     mean in group control mean in group intervention \n                -0.1176359                  0.5186342",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>General structure of a simulation study</span>"
    ]
  },
  {
    "objectID": "chapters/3_general_structure_of_a_simulation_1.html#create-a-data-generating-function",
    "href": "chapters/3_general_structure_of_a_simulation_1.html#create-a-data-generating-function",
    "title": "3  General structure of a simulation study",
    "section": "3.5 Create a data generating function",
    "text": "3.5 Create a data generating function\n\n3.5.1 Make the existing code more abstract\nMove the values used as parameters in the rnorm() call to variables.\n\n3.5.1.1 Try it yourself\n\n\n3.5.1.2 Solution\n\n\nCode\nn_per_condition &lt;- 50\nmean_control &lt;- 0\nmean_intervention &lt;- 0.5\nsd &lt;- 1\n\ndata_control &lt;- \n  tibble(condition = \"control\",\n         score = rnorm(n = n_per_condition, mean = mean_control, sd = sd))\n\ndata_intervention &lt;- \n  tibble(condition = \"intervention\",\n         score = rnorm(n = n_per_condition, mean = mean_intervention, sd = sd))\n\ndata_combined &lt;- bind_rows(data_control,\n                           data_intervention)\n\n# view the generated data\nView(data_combined)\n\n\n\n\n\n3.5.2 Convert this to a function\nAbstracting code into functions is a learned skill. For the moment, see if you can follow the logic of the function and where it came from in the hard coded original version.\n\n3.5.2.1 Try it yourself\n\n\n3.5.2.2 Solution\n\n\nCode\n# define function\ngenerate_data &lt;- function(n_per_condition, # the parameters are now function arguments\n                          mean_control,\n                          mean_intervention,\n                          sd) {\n  \n  data_control &lt;- \n    tibble(condition = \"control\",\n           score = rnorm(n = n_per_condition, mean = mean_control, sd = sd))\n  \n  data_intervention &lt;- \n    tibble(condition = \"intervention\",\n           score = rnorm(n = n_per_condition, mean = mean_intervention, sd = sd))\n  \n  data_combined &lt;- bind_rows(data_control,\n                             data_intervention)\n  \n  return(data_combined)\n}\n\n# call the function with example arguments\ngenerated_data &lt;- generate_data(n_per_condition = 50,\n                                mean_control = 0,\n                                mean_intervention = 0.5,\n                                sd = 1)\n\n# view the generated data\nView(generated_data)\n\n\n\nNow change the above function to use default values for SDs, with SDs = 1. Check that the function works even when you don’t specify SDs when calling the function.\n\n\n\n\n3.5.3 Check the generated data still works with the t-test\nIn general, you should inspect the data thoroughly, check the data types, plot it, conduct other sanity checks, etc. For the moment, we apply just the most basic test: check that the analysis can be fit to data generated by the data generation function. For the moment, the specific results don’t matter as much as whether it can accept the data. That is, your data generation will have to be aligned with your data analysis code (e.g., both make use of the variables score [continuous] and condition [factor with two levels]).\n\n\nCode\nt.test(formula = score ~ condition, \n       data = generated_data, # using the data generated in the previous chunk\n       var.equal = TRUE,\n       alternative = \"two.sided\")\n\n\n\n    Two Sample t-test\n\ndata:  score by condition\nt = -2.6405, df = 98, p-value = 0.009634\nalternative hypothesis: true difference in means between group control and group intervention is not equal to 0\n95 percent confidence interval:\n -1.0107467 -0.1433692\nsample estimates:\n     mean in group control mean in group intervention \n                -0.1765645                  0.4004935",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>General structure of a simulation study</span>"
    ]
  },
  {
    "objectID": "chapters/3_general_structure_of_a_simulation_1.html#create-a-data-analysis-function",
    "href": "chapters/3_general_structure_of_a_simulation_1.html#create-a-data-analysis-function",
    "title": "3  General structure of a simulation study",
    "section": "3.6 Create a data analysis function",
    "text": "3.6 Create a data analysis function\n\n3.6.1 Make the existing code more abstract\nNow do the same abstraction for the analysis.\nRather than just printing all the results of the t test to the console, extract the p value specifically as a column in a tibble. In general, you usually want to extract the results of analyses in tidy data format. Note that the {parameters} and {broom} packages can be very helpful for doing this for you for many common types of analysis, but here we’re do it manually.\n\n3.6.1.1 Try it yourself\n\n\n3.6.1.2 Solution\n\n\nCode\nres_t_test &lt;- t.test(formula = score ~ condition, \n                     data = generated_data,\n                     var.equal = TRUE,\n                     alternative = \"two.sided\")\n\nres &lt;- tibble(p = res_t_test$p.value)\n\nres\n\n\n# A tibble: 1 × 1\n        p\n    &lt;dbl&gt;\n1 0.00963\n\n\n\n\n\n3.6.2 Convert this to a function\n\n3.6.2.1 Try it yourself\n\n\n3.6.2.2 Solution\n\n\nCode\nanalyze &lt;- function(data) {\n  \n  res_t_test &lt;- t.test(formula = score ~ condition, \n                       data = data,\n                       var.equal = TRUE,\n                       alternative = \"two.sided\")\n  \n  res &lt;- tibble(p = res_t_test$p.value)\n  \n  return(res)\n}\n\n\n\nNow modify the function to also extract the t values and degrees of freedom.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>General structure of a simulation study</span>"
    ]
  },
  {
    "objectID": "chapters/3_general_structure_of_a_simulation_1.html#compare-original-hard-coded-version-with-functionalised-version",
    "href": "chapters/3_general_structure_of_a_simulation_1.html#compare-original-hard-coded-version-with-functionalised-version",
    "title": "3  General structure of a simulation study",
    "section": "3.7 Compare original hard-coded version with functionalised version",
    "text": "3.7 Compare original hard-coded version with functionalised version\n\n3.7.1 Original manual code, copied from above\n\n\nCode\nset.seed(42)\n\ndata_control &lt;- \n  tibble(condition = \"control\",\n         score = rnorm(n = 50, mean = 0, sd = 1))\n\ndata_intervention &lt;- \n  tibble(condition = \"intervention\",\n         score = rnorm(n = 50, mean = 0.5, sd = 1))\n\ndata_combined &lt;- bind_rows(data_control,\n                           data_intervention)\n\nt.test(formula = score ~ condition, \n       data = data_combined,\n       var.equal = TRUE,\n       alternative = \"two.sided\")\n\n\n\n    Two Sample t-test\n\ndata:  score by condition\nt = -3.0466, df = 98, p-value = 0.002974\nalternative hypothesis: true difference in means between group control and group intervention is not equal to 0\n95 percent confidence interval:\n -1.0508839 -0.2218625\nsample estimates:\n     mean in group control mean in group intervention \n               -0.03567178                 0.60070141 \n\n\n\n\n3.7.2 New code using functions, copied from above\n\n\nCode\nset.seed(42)\n\ngenerated_data &lt;- generate_data(n_per_condition = 50,\n                                mean_control = 0,\n                                mean_intervention = 0.5,\n                                sd = 1)\n\nresults &lt;- analyze(generated_data)\n\nresults\n\n\n# A tibble: 1 × 1\n        p\n    &lt;dbl&gt;\n1 0.00297\n\n\nNote that this can also be done using the pipe:\n\n\nCode\nset.seed(42)\n\nresults &lt;- \n  generate_data(n_per_condition = 50,\n                mean_control = 0,\n                mean_intervention = 0.5,\n                sd = 1) |&gt;\n  analyze()\n\nresults\n\n\n# A tibble: 1 × 1\n        p\n    &lt;dbl&gt;\n1 0.00297",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>General structure of a simulation study</span>"
    ]
  },
  {
    "objectID": "chapters/3_general_structure_of_a_simulation_1.html#to-do-at-home-this-week",
    "href": "chapters/3_general_structure_of_a_simulation_1.html#to-do-at-home-this-week",
    "title": "3  General structure of a simulation study",
    "section": "3.8 To do at home this week",
    "text": "3.8 To do at home this week\nRead:\n\nSiepe et al. (2024) Simulation Studies for Methodological Research in Psychology: A Standardized Template for Planning, Preregistration, and Reporting. Psychological Methods. https://doi.org/10.1037/met0000695\n\nOptional book-length extra reading you could do over the next several weeks if you choose. However, bear in mind that they employ different simulation workflows than we do.\n\nPustejovsky & Miratrix (2023) Designing Monte Carlo Simulations in R. https://jepusto.github.io/Designing-Simulations-in-R/",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>General structure of a simulation study</span>"
    ]
  },
  {
    "objectID": "chapters/4_general_structure_of_a_simulation_2.html",
    "href": "chapters/4_general_structure_of_a_simulation_2.html",
    "title": "4  General structure of a simulation study",
    "section": "",
    "text": "4.1 Overview of tutorial\nThis lesson covers the steps of writing functions for the remaining essential components of a simulation study within the workflow we will use throughout this course: doing the generate and analyze steps many times, summarizing results across iterations, and making it an experiment.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>General structure of a simulation study</span>"
    ]
  },
  {
    "objectID": "chapters/4_general_structure_of_a_simulation_2.html#dependencies",
    "href": "chapters/4_general_structure_of_a_simulation_2.html#dependencies",
    "title": "4  General structure of a simulation study",
    "section": "4.2 Dependencies",
    "text": "4.2 Dependencies\n\n\nCode\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(readr)\nlibrary(purrr) \nlibrary(furrr)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# set up parallelization\nplan(multisession)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>General structure of a simulation study</span>"
    ]
  },
  {
    "objectID": "chapters/4_general_structure_of_a_simulation_2.html#generate-data-and-analyze-functions-from-last-lesson",
    "href": "chapters/4_general_structure_of_a_simulation_2.html#generate-data-and-analyze-functions-from-last-lesson",
    "title": "4  General structure of a simulation study",
    "section": "4.3 Generate data and analyze functions from last lesson",
    "text": "4.3 Generate data and analyze functions from last lesson\n\n\nCode\ngenerate_data &lt;- function(n_per_condition,\n                          mean_control,\n                          mean_intervention,\n                          sd) {\n  \n  data_control &lt;- \n    tibble(condition = \"control\",\n           score = rnorm(n = n_per_condition, mean = mean_control, sd = sd))\n  \n  data_intervention &lt;- \n    tibble(condition = \"intervention\",\n           score = rnorm(n = n_per_condition, mean = mean_intervention, sd = sd))\n  \n  data_combined &lt;- bind_rows(data_control,\n                             data_intervention)\n  \n  return(data_combined)\n}\n\n\nanalyze &lt;- function(data) {\n  \n  res_t_test &lt;- t.test(formula = score ~ condition, \n                       data = data,\n                       var.equal = TRUE,\n                       alternative = \"two.sided\")\n  \n  res &lt;- tibble(p = res_t_test$p.value)\n  \n  return(res)\n}",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>General structure of a simulation study</span>"
    ]
  },
  {
    "objectID": "chapters/4_general_structure_of_a_simulation_2.html#do-it-once",
    "href": "chapters/4_general_structure_of_a_simulation_2.html#do-it-once",
    "title": "4  General structure of a simulation study",
    "section": "4.4 Do it once",
    "text": "4.4 Do it once\n\n\nCode\nresults &lt;- \n  generate_data(n_per_condition = 50,\n                mean_control = 0,\n                mean_intervention = 0.5,\n                sd = 1) |&gt;\n  analyze()\n\n\nNote that this is equivalent to the below, which just reformats the code:\n\n\nCode\nresults &lt;- generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>General structure of a simulation study</span>"
    ]
  },
  {
    "objectID": "chapters/4_general_structure_of_a_simulation_2.html#do-it-lots-of-times-and-make-it-an-experiment-bad",
    "href": "chapters/4_general_structure_of_a_simulation_2.html#do-it-lots-of-times-and-make-it-an-experiment-bad",
    "title": "4  General structure of a simulation study",
    "section": "4.5 Do it lots of times and make it an experiment [bad]",
    "text": "4.5 Do it lots of times and make it an experiment [bad]\nLet’s use our functions to understand the same question we asked in the first lesson: what is the distribution of p values under the null vs. alternative hypothesis.\nTo do this we need to a) do it a lot of times and b) make it an experiment.\nWe’ll start by doing this in a stupid way. 25 iterations of the simulation by repeating our functions 25 times.\n\n\nCode\nresults_null &lt;- \n  bind_rows(\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0, sd = 1) |&gt; analyze()\n  )\n\nresults_alternative &lt;- \n  bind_rows(\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze(),\n    generate_data(n_per_condition = 50, mean_control = 0, mean_intervention = 0.5, sd = 1) |&gt; analyze()\n  )\n\n# plots\nggplot(results_null, aes(p)) +\n  geom_histogram() +\n  ggtitle(\"Distribution of p values under the null hypothesis\\n(Population Cohen's d = 0.0)\")\n\n\n\n\n\n\n\n\n\nCode\nggplot(results_alternative, aes(p)) +\n  geom_histogram() +\n  ggtitle(\"Distribution of p values under the alternative hypothesis\\n(Population Cohen's d = 0.5)\")\n\n\n\n\n\n\n\n\n\nCode\n# proportion of significant p values\nmean(results_null$p &lt;.05)\n\n\n[1] 0.04\n\n\nCode\nmean(results_alternative$p &lt;.05)\n\n\n[1] 0.71",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>General structure of a simulation study</span>"
    ]
  },
  {
    "objectID": "chapters/4_general_structure_of_a_simulation_2.html#do-it-lots-of-times-and-make-it-an-experiment-good",
    "href": "chapters/4_general_structure_of_a_simulation_2.html#do-it-lots-of-times-and-make-it-an-experiment-good",
    "title": "4  General structure of a simulation study",
    "section": "4.6 Do it lots of times and make it an experiment [good]",
    "text": "4.6 Do it lots of times and make it an experiment [good]\n\n4.6.1 Part 1 of the solution: {tidyr}’s expand_grid()\nexpand_grid() is extremely useful because it creates all possible permutations of all variables you provide it with.\nIt is used to create the set of all conditions and iterations you want to simulate using your data generation and analysis functions. For the moment, we will only create the conditions and iterations.\n\n\nCode\nexpand_grid(\n  time_point = c(\"pre\", \"post\"),\n  condition = c(\"control\", \"intervention\") \n) \n\n\n# A tibble: 4 × 2\n  time_point condition   \n  &lt;chr&gt;      &lt;chr&gt;       \n1 pre        control     \n2 pre        intervention\n3 post       control     \n4 post       intervention\n\n\nIt’s even more useful when you want to repeat the same information on a lot of rows:\n\n\nCode\n# create the sequence from 1 to 100\niterations &lt;- seq(from = 1, to = 100, by = 1)\n\n# check\niterations\n\n\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100\n\n\nCode\n# add these iterations to the expand_grid() call\ngrid &lt;- expand_grid(\n  time_point = c(\"pre\", \"post\"),\n  condition = c(\"control\", \"intervention\"),\n  iteration = iterations\n) \n\nView(grid)\n\n\nNote that I can more simply specify the iterations seq(from = 1, to = 100, by = 1) as 1:100.\n\n\nCode\n# add these iterations to the expand_grid() call\ngrid &lt;- expand_grid(\n  time_point = c(\"pre\", \"post\"),\n  condition = c(\"control\", \"intervention\"),\n  iteration = 1:100 # note this is a series not an integer, i.e., \"1:100\" not \"100\", as \"100\" would mean just one iteration called \"100\".\n) \n\nView(grid)\n\n\n\n4.6.1.1 Use expand_grid() to create a tibble\nWith:\n\n100 iterations of:\nn per condition = 50\nmean control of 0\nmean intervention of 0.5\nSD of 1\n\n\n4.6.1.1.1 Try it yourself first\n\n\nShow solution\nexperiment_parameters &lt;- expand_grid(\n  n_per_condition = 50,\n  mean_control = 0,\n  mean_intervention = 0.5,\n  sd = 1,\n  iteration = 1:100 \n) \n\n\n\n\n\n4.6.1.2 How would we make this an experiment?\nModify the expand_grid() so that it can be used to simulate not only mean_intervention = 0.5 but also 0.\n\n4.6.1.2.1 Try it yourself first\n\n\n4.6.1.2.2 Solution\n\n\nShow solution\nexperiment_parameters &lt;- expand_grid(\n  n_per_condition = 50,\n  mean_control = 0,\n  mean_intervention = c(0, 0.5),\n  sd = 1,\n  iteration = 1:100 \n) \n\n\n\nNote that expand_grid() effectively creates a full-factorial simulation, following Siepe et al.’s (2024) definition.\n\n\n\n\n\n4.6.2 Part 2 of the solution: {furrr}’s future_pmap()\n\n4.6.2.1 Use future_pmap() to map a function and output a nested data frame\npurrr::pmap() aka ‘parallel map’ can be used to map an arbitrary number of inputs from columns in a tibble onto a function. It returns a data frame. It also plays nice with mutate() and other tidyverse functions.\nfurrr::future_pmap() is just a version of purrr::pmap() that takes advantage of parallel processing on your computer’s CPU to run faster. So, future_pmap is parallel to totally distinct ways: mapping multiple inputs onto a function, and running this code on multiple cores of your CPU.\nHowever, it complicates things by introducing the concept of nested data frames. Run the chunk below and then view the tibble to see what I mean.\n\n\nCode\nsimulation &lt;- experiment_parameters |&gt;\n  mutate(generated_data = future_pmap(.l = list(n_per_condition = n_per_condition, \n                                                mean_control = mean_control,\n                                                mean_intervention = mean_intervention,\n                                                sd = sd),\n                                      .f = generate_data,\n                                      .progress = TRUE,\n                                      .options = furrr_options(seed = TRUE)))\n\nView(simulation)\n\n\nWe are used to data frames where the value of a given cell (i.e., a given row of a given column) is a numeric, character, or logical value - i.e. a number, a string of letters, or maybe TRUE/FALSE.\nData frames can technically contain anything though, even other data frames. Nested data frames are data frames whose cells contain other data frames.\n„Gott im Himmel!“, „Hopp de Bäse!“ I hear you cry. Why would we want to do this??\nNote: stop here and show slides on tidy nested data\n\n\n4.6.2.2 Viewing and extracting nested data frames\nIn the data viewer, you can click on a cell containing a data frame to view its contents in a new viewer window.\nYou can also access these data frames using containing_data_frame$nested_data_frame_column[[rownumber]]:\n\n\nCode\n# extract a single data set and assign it to its own object\ndata_set_iteration_1 &lt;- simulation$generated_data[[1]]\n\n# use this extracted data frame in the analyze function\nanalyze(data_set_iteration_1)\n\n\n# A tibble: 1 × 1\n      p\n  &lt;dbl&gt;\n1 0.177\n\n\n\n\n4.6.2.3 Use pmap() to use a nested data frame as the input for a a function\nYou can also use nested data frames as inputs to other functions, not just outputs. This is also done via pmap().\nBeing able to use them as both inputs and outputs makes them very powerful.\n\n\nCode\nsimulation &lt;- experiment_parameters |&gt;\n  mutate(generated_data = future_pmap(.l = list(n_per_condition, \n                                                mean_control,\n                                                mean_intervention,\n                                                sd),\n                                      .f = generate_data,\n                                      .progress = TRUE,\n                                      .options = furrr_options(seed = TRUE))) |&gt;\n  mutate(results = future_pmap(.l = list(generated_data),\n                               .f = analyze,\n                               .progress = TRUE,\n                               .options = furrr_options(seed = TRUE))) \n\nView(simulation)\n\n\n\n\n4.6.2.4 Transparency vs. data size\n\n\nCode\nsimulation |&gt;\n  object.size() |&gt;\n  format(units = \"auto\")\n\n\n[1] \"711.8 Kb\"\n\n\nCode\nsimulation |&gt;\n  select(-generated_data) |&gt;\n  object.size() |&gt;\n  format(units = \"auto\")\n\n\n[1] \"180.4 Kb\"\n\n\n\n\n\n4.6.3 Flatten / unnest a nested data frames\nFlatten / unnest a nested data frames into the containing data frame.\n\n\nCode\nsimulation_summary &lt;- simulation |&gt;\n  unnest(results)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>General structure of a simulation study</span>"
    ]
  },
  {
    "objectID": "chapters/4_general_structure_of_a_simulation_2.html#summarize-across-iterations",
    "href": "chapters/4_general_structure_of_a_simulation_2.html#summarize-across-iterations",
    "title": "4  General structure of a simulation study",
    "section": "4.7 Summarize across iterations",
    "text": "4.7 Summarize across iterations\nNote that this step will be much less standardized between simulations. You will have to adjust this code quite a lot to do what you need it to compared to the relatively standardized workflow for generate data and analyze data. Summarizing the data across iterations depends heavily what your simulation’s experimental conditions and research questions are.\n\n\nCode\n# summarize across iterations\nsimulation_summary &lt;- simulation |&gt;\n  unnest(results) |&gt;\n  # because mean_control is 0 and both SDs are 1, mean_intervention is really a proxy for Cohen's d \n  # because d = [mean_intervention - mean_control]/SD_pooled\n  # for clarity, let's just make a variable called population_effect_size\n  mutate(population_effect_size = paste0(\"Cohen's d = \", mean_intervention))|&gt; \n  # for each level of mean_intervention... \n  group_by(population_effect_size) |&gt;\n  # ... calculate proportion of iterations where significant results were found\n  mutate(significant = p &lt; .05) |&gt;\n  summarize(power = mean(significant))\n\n# plot results\nggplot(simulation_summary, aes(population_effect_size, power)) +\n  geom_col() +\n  theme_linedraw() +\n  scale_y_continuous(breaks = breaks_pretty(n = 10),\n                     limits = c(0,1),\n                     name = \"Proportion of significant p-values\") +\n  scale_x_discrete(name = \"Population effect size\")\n\n\n\n\n\n\n\n\n\n\nComprehension question: What do these two columns represent? They are well known statistical properties that you have just simulated.\n\n\n4.7.1 Common mistake\nYou must group by the things you manipulated in the expand grid! I can’t give you an example for this yet, but remember this as an essential step. If you set up an simulation experiment manipulating two variables but then summarize the results over just one of them the results will be meaningless or at least very hard to interpret.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>General structure of a simulation study</span>"
    ]
  },
  {
    "objectID": "chapters/4_general_structure_of_a_simulation_2.html#putting-it-all-together",
    "href": "chapters/4_general_structure_of_a_simulation_2.html#putting-it-all-together",
    "title": "4  General structure of a simulation study",
    "section": "4.8 Putting it all together",
    "text": "4.8 Putting it all together\n\n\nCode\n# remove all objects from environment to ensure you're starting from a blank page\nrm(list = ls())\n\n# set seed\nset.seed(42)\n\n# functions for simulation\ngenerate_data &lt;- function(n_per_condition,\n                          mean_control,\n                          mean_intervention,\n                          sd) {\n  \n  data_control &lt;- \n    tibble(condition = \"control\",\n           score = rnorm(n = n_per_condition, mean = mean_control, sd = sd))\n  \n  data_intervention &lt;- \n    tibble(condition = \"intervention\",\n           score = rnorm(n = n_per_condition, mean = mean_intervention, sd = sd))\n  \n  data_combined &lt;- bind_rows(data_control,\n                             data_intervention)\n  \n  return(data_combined)\n}\n\nanalyze &lt;- function(data) {\n  \n  res_t_test &lt;- t.test(formula = score ~ condition, \n                       data = data,\n                       var.equal = TRUE,\n                       alternative = \"two.sided\")\n  \n  res &lt;- tibble(p = res_t_test$p.value)\n  \n  return(res)\n}\n\n# simulation parameters\nexperiment_parameters &lt;- expand_grid(\n  n_per_condition = 50,\n  mean_control = 0,\n  mean_intervention = c(0, 0.5),\n  sd = 1,\n  iteration = 1:10000 # note that number of iterations has been increased to provide more stable estimates / lower monte-carlo error. \n) |&gt;\n  # because mean_control is 0 and both SDs are 1, mean_intervention is really a proxy for Cohen's d\n  # because d = [mean_intervention - mean_control]/SD_pooled\n  # for clarity, let's just make a variable called population_effect_size\n  mutate(population_effect_size = paste0(\"Cohen's d = \", mean_intervention)) \n\n# run simulation\nsimulation &lt;- experiment_parameters |&gt;\n  mutate(generated_data = future_pmap(.l = list(n_per_condition, \n                                                mean_control,\n                                                mean_intervention,\n                                                sd),\n                                      .f = generate_data,\n                                      .progress = TRUE,\n                                      .options = furrr_options(seed = TRUE))) |&gt;\n  mutate(results = future_pmap(.l = list(generated_data),\n                               .f = analyze,\n                               .progress = TRUE,\n                               .options = furrr_options(seed = TRUE)))\n\n# summarize across iterations\nsimulation_summary &lt;- simulation |&gt;\n  # unnest results\n  unnest(results) |&gt;\n  # for each level of mean_intervention... \n  group_by(population_effect_size) |&gt;\n  # ... calculate proportion of iterations where significant results were found\n  mutate(significant = p &lt; .05) |&gt;\n  summarize(proportion_of_significant_p_values = mean(significant))\n\n# table of results\nsimulation_summary |&gt;\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\npopulation_effect_size\nproportion_of_significant_p_values\n\n\n\n\nCohen's d = 0\n0.0513\n\n\nCohen's d = 0.5\n0.6874\n\n\n\n\n\nCode\n# plot results\nggplot(simulation_summary, aes(population_effect_size, proportion_of_significant_p_values)) +\n  geom_col() +\n  theme_linedraw() +\n  scale_y_continuous(breaks = breaks_pretty(n = 10),\n                     limits = c(0,1),\n                     name = \"Proportion of significant p-values\") +\n  scale_x_discrete(name = \"Population effect size\")\n\n\n\n\n\n\n\n\n\n\nThe false-positive rate of a Student’s t-test (proportion of significant results when population effect is zero) is 5% when all assumptions of the test are met. This is as it should be: false positive rate should == the test’s alpha value.\nThe true-positive rate of a Student’s t-test (proportion of significant results when population effect is non-zero) is c.67% when all assumptions of the test are met, the population effect size is Cohen’s d = 0.5, and there are 50 participants per group. In other words, the statistical power of the test (the probability of detecting effects that exist) is about .67 under these conditions.\n\n\n4.8.1 Check your own learning\n\nHow do I know that all the test’s assumptions have been met?\nWhat are a Student’s t-test’s assumptions?\n\n…What happens if these assumptions aren’t met? We will cover this in the next class.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>General structure of a simulation study</span>"
    ]
  },
  {
    "objectID": "chapters/4_general_structure_of_a_simulation_2.html#distribution-of-p-values",
    "href": "chapters/4_general_structure_of_a_simulation_2.html#distribution-of-p-values",
    "title": "4  General structure of a simulation study",
    "section": "4.9 Distribution of p values",
    "text": "4.9 Distribution of p values\nLet’s return to the question we answered in the first lecture on using simulation to understand the distribution of p-values under the alternative vs. null hypothesis, but this time using the results of the tidy simulation above.\nRemember: this just plots the data in a different way to the above, but expresses the same information (the false-positive and true-positive rate). Plotting it the below way helps illustrate why the false-positive and false-negative rates are why they are.\n\n\nCode\n# function to extract and plot p values\nplot_p_values &lt;- function(data){ # assumes that data is a data frame with a column \"p\"\n  data |&gt;\n    mutate(decision = ifelse(p &lt; .05, \"significant\", \"non-significant\")) |&gt;\n    ggplot(aes(p, fill = decision)) +\n    geom_histogram(binwidth = 0.05, boundary = 0) +\n    scale_fill_viridis_d(option = \"mako\", begin = 0.3, end = 0.7, direction = -1) +\n    scale_x_continuous(labels = c(0, 0.05, 0.25, 0.50, 0.75, 1.0),\n                       breaks = c(0, 0.05, 0.25, 0.50, 0.75, 1.0), \n                       limits = c(0, 1)) +\n    theme_linedraw() +\n    ylab(\"Frequency\") +\n    xlab(\"p value\")\n}\n\nsimulation |&gt;\n  unnest(results) |&gt;\n  filter(population_effect_size == \"Cohen's d = 0\") |&gt;\n  plot_p_values() +\n  ggtitle(\"Distribution of p values under the null hypothesis\")\n\n\n\n\n\n\n\n\n\nCode\nsimulation |&gt;\n  unnest(results) |&gt;\n  filter(population_effect_size == \"Cohen's d = 0.5\") |&gt;\n  plot_p_values() +\n  ggtitle(\"Distribution of p values under the alternative hypothesis\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>General structure of a simulation study</span>"
    ]
  },
  {
    "objectID": "chapters/5_violating_assumptions.html",
    "href": "chapters/5_violating_assumptions.html",
    "title": "5  Assessing the impact of violating the assumption of normality",
    "section": "",
    "text": "5.1 Dependencies\nCode\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(purrr) \nlibrary(furrr)\nlibrary(forcats)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(sn)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(janitor)\nlibrary(effsize)\n\n# set up parallelization\nplan(multisession)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Assessing the impact of violating the assumption of normality</span>"
    ]
  },
  {
    "objectID": "chapters/5_violating_assumptions.html#skew-normal-distributions",
    "href": "chapters/5_violating_assumptions.html#skew-normal-distributions",
    "title": "5  Assessing the impact of violating the assumption of normality",
    "section": "5.2 Skew-normal distributions",
    "text": "5.2 Skew-normal distributions\nIn order to violate normality, we will need to use a different non-normal distribution. For this example we’ll use the skew-normal distribution. Where the normal distribution of defined by two parameters, mean and standard deviation, other distributions are controlled by other parameters with different naming conventions, and often more than two parameters.\nThe skew-normal distribution is defined by:\n\n‘location’, akin to mean, is controlled via the parameter \\(\\xi\\), i.e., ‘xi’ in sn(). In fact, mean is referred to as ‘location’ in many distributions.\n‘scale’, akin to SD, is controlled via the parameter \\(\\omega\\), i.e., ‘omega’ in sn(). Likewise, ‘scale’ is a common way of referring to measures of dispersion like SD.\n‘slant’/‘skew’, is controlled via parameter \\(\\alpha\\), i.e., ‘alpha’ in sn().\n\nNote that when alpha = 0, skew-normal data is the same as normal data:\n\n\nCode\nmu    &lt;- 0    # population mean \nsigma &lt;- 1    # population standard deviation\nskew  &lt;- 12   # skewness parameter\n\n# convert m, sd, and skewness into skew-normal parameters scale, location, and alpha (skew)\ndelta_val &lt;- skew / sqrt(1 + skew^2)  # delta is an intermediate value\nscale_val &lt;- sigma / sqrt(1 - 2 * delta_val^2 / pi)  \nlocation_val &lt;- mu - scale_val * delta_val * sqrt(2 / pi) \n\n# generate data\nset.seed(42)  \n\nsample_size &lt;- 1000000\n\ndata_combined &lt;- tibble(\n  normal = rnorm(n = sample_size, \n                 mean = mu, \n                 sd = sigma),\n  skewnormal = rsn(n = sample_size, \n                   xi = location_val, \n                   omega = scale_val, \n                   alpha = skew)\n)\n\n# table\ndata_combined |&gt;\n  summarize(mean_normal = mean(normal),\n            mean_skewnormal = mean(skewnormal),\n            sd_normal = sd(normal),\n            sd_skewnormal = sd(skewnormal)) |&gt;\n  mutate_if(is.numeric, round_half_up, digits = 2) |&gt;\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\nmean_normal\nmean_skewnormal\nsd_normal\nsd_skewnormal\n\n\n\n\n0\n0\n1\n1\n\n\n\n\n\nCode\n# plot\nggplot(data_combined) +\n  geom_density(aes(x = normal)) +\n  geom_density(aes(x = skewnormal)) +\n  labs(x = \"Scores\",\n       y = \"Density\",\n       title = \"Normal vs. Skew-Normal Distributions with equal mean and SD\") +\n  scale_x_continuous(breaks = breaks_pretty(n = 9), limits = c(-4, 4)) +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\n\n5.2.1 Math for converting normal parameters + skew to skew-normal parameters\nYou don’t to understand the following math, but here is is for nerds.\nThe parameters that define normal data, i.e., population mean (\\(\\mu\\)) and population SD (\\(\\sigma\\)) plus a skew parameter (\\(\\alpha\\)) can be converted to the parameters that directly define a skew-normal distribution, location (\\(\\xi\\)), scale (\\(\\omega\\)), and skew (\\(\\alpha\\)).\nThe intermediate variable \\(\\delta\\) is calculated from skew (\\(\\alpha\\)):\n\\[\n\\delta = \\frac{\\alpha}{\\sqrt{\\,1 + \\alpha^2\\,}}\n\\]\nScale (\\(\\omega\\)) is calculated from \\(\\delta\\) and skew (\\(\\alpha\\)):\n\\[\n\\omega\n= \\frac{\\sigma}{\\sqrt{\\,1 \\;-\\; \\frac{2\\,\\delta^2}{\\pi}\\,}}\n\\]\nLocation (\\(\\xi\\)) is calculated from population mean (\\(\\mu\\)), scale (\\(\\omega\\)), and skew (\\(\\alpha\\)).\n\\[\n\\xi\n= \\mu\n\\;-\\;\n\\omega \\,\\delta \\,\\sqrt{\\frac{2}{\\pi}}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Assessing the impact of violating the assumption of normality</span>"
    ]
  },
  {
    "objectID": "chapters/5_violating_assumptions.html#impact-of-non-normality-on-the-t-tests-false-positive-rate",
    "href": "chapters/5_violating_assumptions.html#impact-of-non-normality-on-the-t-tests-false-positive-rate",
    "title": "5  Assessing the impact of violating the assumption of normality",
    "section": "5.3 1. Impact of non-normality on the t-test’s false-positive rate",
    "text": "5.3 1. Impact of non-normality on the t-test’s false-positive rate\n\n5.3.1 Data generation function\n\n\nCode\n# define data generating function ----\ngenerate_data &lt;- function(n_control,\n                          n_intervention,\n                          location_control, # location, akin to mean\n                          location_intervention,\n                          scale_control, # scale, akin to SD\n                          scale_intervention,\n                          skew_control, # slant/skew. When 0, produces normal/gaussian data\n                          skew_intervention) {\n  \n  data_control &lt;- \n    tibble(condition = \"control\",\n           score = rsn(n = n_control, \n                       xi = location_control, # location, akin to mean\n                       omega = scale_control, # scale, akin to SD\n                       alpha = skew_control)) # slant/skew. When 0, produces normal/gaussian data\n  \n  data_intervention &lt;- \n    tibble(condition = \"intervention\",\n           score = rsn(n = n_intervention, \n                       xi = location_intervention, # location, akin to mean\n                       omega = scale_intervention, # scale, akin to SD\n                       alpha = skew_intervention)) # slant/skew. When 0, produces normal/gaussian data\n  \n  data &lt;- \n    bind_rows(data_control,\n              data_intervention) |&gt;\n    # order the levels of condition correctly so that the direction of cohens d is correct\n    mutate(condition = fct_relevel(condition, \"intervention\", \"control\"))\n    \n  return(data)\n}\n\n\n\n\n5.3.2 Analysis function\n\n\nCode\n# define data analysis function ----\nanalyze &lt;- function(data) {\n\n  res_t_test &lt;- t.test(formula = score ~ condition, \n                       data = data,\n                       var.equal = TRUE,\n                       alternative = \"two.sided\")\n  \n  res_cohens_d &lt;- cohen.d(formula = score ~ condition,\n                          data = data,\n                          pooled = TRUE)\n  \n  res &lt;- tibble(p = res_t_test$p.value, \n                cohens_d = res_cohens_d$estimate)\n\n  return(res)\n}\n\n\n\n\n5.3.3 Simulation parameters\n\n5.3.3.1 Exercise - setting up the experiment correctly\nWe want to construct a simulation where the skew parameter is either 0 or 12 AND for the skew parameter to be the same in both conditions: both 0 or both 12.\nThis presents a problem for our expand_grid call. Why? That is, what is wrong with the below code? How would we fix it?\nThis is an important lesson in thinking carefully about how you set up a simulation and whether it does what you intend it to.\n\n\nCode\n# define experiment parameters ----\nexperiment_parameters &lt;- expand_grid(\n  n_control = 100,\n  n_intervention = 100,\n  mu_control = 0,\n  mu_intervention = 0, \n  sigma_control = 1,\n  sigma_intervention = 1,\n  skew_control = c(0, 12),\n  skew_intervention = c(0, 12),\n  iteration = 1:1000 \n) \n\n\n\n\n5.3.3.2 Solution\n\n\nCode\n# define experiment parameters ----\nexperiment_parameters &lt;- expand_grid(\n  n_control = 100,\n  n_intervention = 100,\n  mu_control = 0,\n  mu_intervention = 0, \n  sigma_control = 1,\n  sigma_intervention = 1,\n  skew_control = c(0, 12),\n  iteration = 1:1000 \n) |&gt;\n  mutate(skew_intervention = skew_control) \n\n\n# do some sanity checks on the experiment's grid\nexperiment_parameters |&gt;\n  count(skew_control, skew_intervention) |&gt;\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\nskew_control\nskew_intervention\nn\n\n\n\n\n0\n0\n1000\n\n\n12\n12\n1000\n\n\n\n\n\n\n\n5.3.3.3 Experiment parameters\nI have added the normal-to-skew-normal parameter conversions and the convenience variables.\n\n\nCode\n# define experiment parameters ----\nexperiment_parameters &lt;- expand_grid(\n  n_control = 100,\n  n_intervention = 100,\n  mu_control = 0,\n  mu_intervention = 0, \n  sigma_control = 1,\n  sigma_intervention = 1,\n  skew_control = c(0, 12),\n  iteration = 1:1000 \n) |&gt;\n  mutate(skew_intervention = skew_control) |&gt;\n  \n  # make an intuitive label for the conditions\n  mutate(distribution = case_when(skew_intervention == 0 ~ \"Normal data\",\n                                  skew_intervention == 12 ~ \"Skew-Normal data\")) |&gt;\n  \n  # calculate skew-normal parameters\n  # don't worry about the math, it's not important to understand\n  mutate(delta_control = skew_control / sqrt(1 + skew_control^2),\n         delta_intervention = skew_intervention / sqrt(1 + skew_intervention^2),\n         scale_control = sigma_control / sqrt(1 - 2 * delta_control^2 / pi),\n         scale_intervention = sigma_intervention / sqrt(1 - 2 * delta_intervention^2 / pi),\n         location_control = mu_control - scale_control * delta_control * sqrt(2 / pi),\n         location_intervention = mu_intervention - scale_intervention * delta_intervention * sqrt(2 / pi)) \n\n\n\n\n\n5.3.4 Run simulation\n\n\nCode\n# set the seed ----\nset.seed(42)\n\n# run simulation ----\nsimulation &lt;- \n  # using the experiment parameters\n  experiment_parameters |&gt;\n  \n  # generate data using the data generating function and the parameters relevant to data generation\n  mutate(generated_data = pmap(list(n_control,\n                                    n_intervention,\n                                    location_control,\n                                    location_intervention,\n                                    scale_control,\n                                    scale_intervention,\n                                    skew_control,\n                                    skew_intervention),\n                               generate_data)) |&gt;\n  \n  # apply the analysis function to the generated data using the parameters relevant to analysis\n  mutate(analysis_results = pmap(list(generated_data),\n                                 analyze))\n\n\n\n\n5.3.5 Summarize results\nSummarize the proportion of significant p-values, i.e., the false positive rate, since population difference in locations in zero.\nQuick check on your own learning: When would this proportion not represent the false positive rate?\n\n\nCode\n# summarise simulation results over the iterations ----\nsimulation_summary &lt;- simulation |&gt;\n  unnest(analysis_results) |&gt;\n  group_by(distribution) |&gt;\n  summarize(proportion_significant = mean(p &lt; .05)) \n\n# print table\nsimulation_summary |&gt;\n  mutate_if(is.numeric, janitor::round_half_up, digits = 2) |&gt; # note: only ever do rounding at the point of printing results!\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\ndistribution\nproportion_significant\n\n\n\n\nNormal data\n0.04\n\n\nSkew-Normal data\n0.04\n\n\n\n\n\n\nWhat does this tell us?\nWhat does this not tell us?\nIf we want to understand why violating the assumption of normality might be bad, what else can we do?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Assessing the impact of violating the assumption of normality</span>"
    ]
  },
  {
    "objectID": "chapters/5_violating_assumptions.html#impact-of-non-normality-on-the-t-tests-false-positive-and-true-positive-power-rates",
    "href": "chapters/5_violating_assumptions.html#impact-of-non-normality-on-the-t-tests-false-positive-and-true-positive-power-rates",
    "title": "5  Assessing the impact of violating the assumption of normality",
    "section": "5.4 2. Impact of non-normality on the t-test’s false-positive and true-positive (power) rates",
    "text": "5.4 2. Impact of non-normality on the t-test’s false-positive and true-positive (power) rates\n\n5.4.1 Simulation parameters\n\n5.4.1.1 Exercise\nHow would we modify the parameters to study not only the false-positive rate but also the true positive rate?\n\n\nCode\n# define experiment parameters ----\nexperiment_parameters &lt;- expand_grid(\n  n_control = 100,\n  n_intervention = 100,\n  mu_control = 0,\n  mu_intervention = 0,\n  sigma_control = 1,\n  sigma_intervention = 1,\n  skew_control = c(0, 12),\n  iteration = 1:1000 \n) |&gt;\n  mutate(skew_intervention = skew_control) |&gt;\n  \n  # make an intuitive label for the conditions\n  mutate(distribution = case_when(skew_intervention == 0 ~ \"Normal data\",\n                                  skew_intervention == 12 ~ \"Skew-Normal data\"),\n         population_effect_size = paste(\"Cohen's d =\", mu_intervention)) |&gt;\n  \n  # calculate skew-normal parameters\n  # don't worry about the math, it's not important to understand\n  mutate(delta_control = skew_control / sqrt(1 + skew_control^2),\n         delta_intervention = skew_intervention / sqrt(1 + skew_intervention^2),\n         scale_control = sigma_control / sqrt(1 - 2 * delta_control^2 / pi),\n         scale_intervention = sigma_intervention / sqrt(1 - 2 * delta_intervention^2 / pi),\n         location_control = mu_control - scale_control * delta_control * sqrt(2 / pi),\n         location_intervention = mu_intervention - scale_intervention * delta_intervention * sqrt(2 / pi)) \n\n\n\n\n5.4.1.2 Solution\n\n\nCode\n# define experiment parameters ----\nexperiment_parameters &lt;- expand_grid(\n  n_control = 100,\n  n_intervention = 100,\n  mu_control = 0,\n  mu_intervention = c(0, 0.5), # multiple location values\n  sigma_control = 1,\n  sigma_intervention = 1,\n  skew_control = c(0, 12),\n  iteration = 1:1000 \n) |&gt;\n  mutate(skew_intervention = skew_control) |&gt;\n  \n  # make an intuitive label for the conditions\n  mutate(distribution = case_when(skew_intervention == 0 ~ \"Normal data\",\n                                  skew_intervention == 12 ~ \"Skew-Normal data\"),\n         population_effect_size = paste(\"Cohen's d =\", mu_intervention)) |&gt;\n  \n  # calculate skew-normal parameters\n  # don't worry about the math, it's not important to understand\n  mutate(delta_control = skew_control / sqrt(1 + skew_control^2),\n         delta_intervention = skew_intervention / sqrt(1 + skew_intervention^2),\n         scale_control = sigma_control / sqrt(1 - 2 * delta_control^2 / pi),\n         scale_intervention = sigma_intervention / sqrt(1 - 2 * delta_intervention^2 / pi),\n         location_control = mu_control - scale_control * delta_control * sqrt(2 / pi),\n         location_intervention = mu_intervention - scale_intervention * delta_intervention * sqrt(2 / pi)) \n\n\n\n\n\n5.4.2 Run simulation\nExactly the same as last time, only parameters differ.\n\n\nCode\n# set the seed ----\nset.seed(42)\n\n# run simulation ----\nsimulation &lt;- \n  # using the experiment parameters\n  experiment_parameters |&gt;\n  \n  # generate data using the data generating function and the parameters relevant to data generation\n  mutate(generated_data = pmap(list(n_control,\n                                    n_intervention,\n                                    location_control,\n                                    location_intervention,\n                                    scale_control,\n                                    scale_intervention,\n                                    skew_control,\n                                    skew_intervention),\n                               generate_data)) |&gt;\n  \n  # apply the analysis function to the generated data using the parameters relevant to analysis\n  mutate(analysis_results = pmap(list(generated_data),\n                                 analyze))\n\n\n\n\n5.4.3 Summarize results\nSummarize the proportion of significant p-values, i.e., the false positive rate, since population difference in locations in zero.\n\n5.4.3.1 Exercise\nHow do we need to modify the summarization compared to last time? Modify the below code.\n\n\nCode\n# summarise simulation results over the iterations\nsimulation_summary &lt;- simulation |&gt;\n  unnest(analysis_results) |&gt;\n  group_by(distribution) |&gt;\n  summarize(proportion_significant = mean(p &lt; .05)) \n\nsimulation_summary |&gt;\n  mutate_if(is.numeric, janitor::round_half_up, digits = 2) |&gt; # note: only ever do rounding at the point of printing results!\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\ndistribution\nproportion_significant\n\n\n\n\nNormal data\n0.49\n\n\nSkew-Normal data\n0.49\n\n\n\n\n\n\n\n5.4.3.2 Solution\n\n\nShow solution\n# summarise simulation results over the iterations ----\nsimulation_summary &lt;- simulation |&gt;\n  unnest(analysis_results) |&gt;\n  # group_by ALL the manipulated factors\n  group_by(distribution, population_effect_size) |&gt; \n  summarize(proportion_significant = mean(p &lt; .05)) \n\nsimulation_summary |&gt;\n  mutate_if(is.numeric, janitor::round_half_up, digits = 2) |&gt; # note: only ever do rounding at the point of printing results!\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\ndistribution\npopulation_effect_size\nproportion_significant\n\n\n\n\nNormal data\nCohen's d = 0\n0.04\n\n\nNormal data\nCohen's d = 0.5\n0.94\n\n\nSkew-Normal data\nCohen's d = 0\n0.04\n\n\nSkew-Normal data\nCohen's d = 0.5\n0.93\n\n\n\n\n\n\nWhat does this tell us?\nWhat does this not tell us?\nIf we want to understand why violating the assumption of normality might be bad, what else can we do?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Assessing the impact of violating the assumption of normality</span>"
    ]
  },
  {
    "objectID": "chapters/5_violating_assumptions.html#impact-of-non-normality-on-the-t-tests-false-positive-and-true-positive-power-rates-and-estimates-of-cohenss-d",
    "href": "chapters/5_violating_assumptions.html#impact-of-non-normality-on-the-t-tests-false-positive-and-true-positive-power-rates-and-estimates-of-cohenss-d",
    "title": "5  Assessing the impact of violating the assumption of normality",
    "section": "5.5 3. Impact of non-normality on the t-test’s false-positive and true-positive (power) rates and estimates of Cohens’s d",
    "text": "5.5 3. Impact of non-normality on the t-test’s false-positive and true-positive (power) rates and estimates of Cohens’s d\nAs well as the false-positive and false-negative rate of the t-test’s p-values, we could also examine the estimates of standardized effect size.\n\n5.5.1 Summarize results\nWe can summarize the results of the previous simulation differently, by adding Cohen’s d too.\n\n\nCode\n# summarise simulation results over the iterations ----\nsimulation_summary &lt;- simulation |&gt;\n  unnest(analysis_results) |&gt;\n  # group by ALL the manipulated factors\n  group_by(distribution, \n           population_effect_size) |&gt;\n  summarize(proportion_significant = mean(p &lt; .05),\n            cohens_d = mean(cohens_d)) |&gt;\n  select(distribution, \n         population_effect_size,\n         cohens_d,\n         proportion_significant)\n\n# table\nsimulation_summary |&gt;\n  mutate_if(is.numeric, janitor::round_half_up, digits = 2) |&gt;\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\ndistribution\npopulation_effect_size\ncohens_d\nproportion_significant\n\n\n\n\nNormal data\nCohen's d = 0\n0.0\n0.04\n\n\nNormal data\nCohen's d = 0.5\n0.5\n0.94\n\n\nSkew-Normal data\nCohen's d = 0\n0.0\n0.04\n\n\nSkew-Normal data\nCohen's d = 0.5\n0.5\n0.93\n\n\n\n\n\n\nThe false-positive and false-negative rates of the t-test’s p-value are maintained when data is skewed.\nThe effect size recovery is maintained when data is skewed.\n\nSo, why do we care about the assumption of normality for a t-test if violating it in these ways doesn’t substantially change much?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Assessing the impact of violating the assumption of normality</span>"
    ]
  },
  {
    "objectID": "chapters/5_violating_assumptions.html#at-home-exercises",
    "href": "chapters/5_violating_assumptions.html#at-home-exercises",
    "title": "5  Assessing the impact of violating the assumption of normality",
    "section": "5.6 At-home exercises",
    "text": "5.6 At-home exercises\n\n5.6.1 Collate the final simulation above into a single code chunk with all the pieces to run the full simulation\n\n\nCode\n# remove all objects from environment to ensure you're starting from a blank page\nrm(list = ls())\n\n# paste necessary code in here\n\n\n\n\n5.6.2 Elaborate the simulation\nRight now the simulation only examines a fixed sample size. Perhaps the supposed negative impact of violating normality would be seen at other sample sizes? Vary this or indeed other things to understand when violating normality matters. If you wanted to get more complex, examine the impact of using another distribution other than skew normal that still has knowable population means and SDs, such as a bounded uniform distribution.\n\n\n5.6.3 Develop a simulation to examine the impact of violating other statistical assumptions\nThe Student’s t-test also assumes equal variances between the samples (var.equal = TRUE), whereas the Welches’ t-test does not (var.equal = FALSE). Using just normal data, and varying between experimental conditions whether the SDs are equal or unequal, how much does violating the Student’s t-test undermine its false-positive or false-negative (power) rates? Does using a Welches’ t-test resolve this? If so, what are the downsides of Welches’ t-test, i.e., why don’t we use it by default?\nNote that this isn’t a trivial exercise, it would take you some time to answer. People have published papers on these exact questions (Delacre, Lakens and Leys, 2017).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Assessing the impact of violating the assumption of normality</span>"
    ]
  },
  {
    "objectID": "chapters/6_testing_assumptions_and_conditional_tests.html",
    "href": "chapters/6_testing_assumptions_and_conditional_tests.html",
    "title": "6  The statistical power of assumptions tests and the conditional use of (non-)parameteric tests”",
    "section": "",
    "text": "6.1 Check your understanding\nWhat is the relationship between a linear regression, a Student’s t-test and a Wilcox rank-sum test? Or between linear regression and ANOVA?\nWhat is the difference between a hypothesis test like a Student’s t-test and a test of assumptions such as Shapiro-Wilk’s test (for normality) or Levene’s test (for homogeneity of variances)?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The statistical power of assumptions tests and the conditional use of (non-)parameteric tests\"</span>"
    ]
  },
  {
    "objectID": "chapters/6_testing_assumptions_and_conditional_tests.html#answer",
    "href": "chapters/6_testing_assumptions_and_conditional_tests.html#answer",
    "title": "6  The statistical power of assumptions tests and the conditional use of (non-)parameteric tests”",
    "section": "6.2 Answer",
    "text": "6.2 Answer\nAlmost everything is a linear model.\n\n\nCode\nknitr::include_graphics(\"../figs/common_statistical_tests_are_linear_models_cheat_sheet.png\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The statistical power of assumptions tests and the conditional use of (non-)parameteric tests\"</span>"
    ]
  },
  {
    "objectID": "chapters/6_testing_assumptions_and_conditional_tests.html#overview",
    "href": "chapters/6_testing_assumptions_and_conditional_tests.html#overview",
    "title": "6  The statistical power of assumptions tests and the conditional use of (non-)parameteric tests”",
    "section": "6.3 Overview",
    "text": "6.3 Overview\nWhat do most statistics textbooks tell you to do when trying to test if two groups’ means differ?\n\nCheck assumptions of an independent Student’s t-test are met, e.g., normality of data and homogeneity of variances.\nIf so, run an interpret an independent Student’s t-test.\nIf not, then perhaps perhaps either ‘interpret results with caution’ (which always feels vague) or run and interpret a non-parametric test instead.\n\nWhy? What benefits are there for doing so? Or what bad things happen if you don’t?\nIn a previous session, we observed that violations of the assumption of normality actually has very little impact on the false-positive and false-negative rates of a t-test, as long as the two conditions have similarly non-normal data, which is plausible in many situations. This lesson seeks to answer two related questions:\n\nJust like hypothesis tests, assumptions tests are just inferential tests of other properties (e.g., differences in SDs rather than differences in means), and as such they have false-positive rates and false-negative rates (statistical power). What is the power of these tests under different degrees of violations of assumptions? I.e. what proportion of the time do they get it wrong?\nWhat is the aggregate benefit of choosing a hypothesis test based on the results of assumption tests? This multi-step researcher behavior can itself be simulated.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The statistical power of assumptions tests and the conditional use of (non-)parameteric tests\"</span>"
    ]
  },
  {
    "objectID": "chapters/6_testing_assumptions_and_conditional_tests.html#dependencies",
    "href": "chapters/6_testing_assumptions_and_conditional_tests.html#dependencies",
    "title": "6  The statistical power of assumptions tests and the conditional use of (non-)parameteric tests”",
    "section": "6.4 Dependencies",
    "text": "6.4 Dependencies\n\n\nCode\n# dependencies\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(readr)\nlibrary(purrr) \nlibrary(furrr)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(sn)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# set up parallelization\nplan(multisession)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The statistical power of assumptions tests and the conditional use of (non-)parameteric tests\"</span>"
    ]
  },
  {
    "objectID": "chapters/6_testing_assumptions_and_conditional_tests.html#power-of-students-t-test",
    "href": "chapters/6_testing_assumptions_and_conditional_tests.html#power-of-students-t-test",
    "title": "6  The statistical power of assumptions tests and the conditional use of (non-)parameteric tests”",
    "section": "6.5 Power of Student’s t-test",
    "text": "6.5 Power of Student’s t-test\nCode taken from the previous lesson, with some simplifications.\n\n\nCode\n# remove all objects from environment ----\nrm(list = ls())\n\n# define data generating function ----\ngenerate_data &lt;- function(n,\n                          location_control, # location, akin to mean\n                          location_intervention,\n                          scale, # scale, akin to SD\n                          skew) { # slant/skew. When 0, produces normal/gaussian data\n  \n  data_control &lt;- \n    tibble(condition = \"control\",\n           score = rsn(n = n, \n                       xi = location_control, # location, akin to mean\n                       omega = scale, # scale, akin to SD\n                       alpha = skew)) # slant/skew. When 0, produces normal/gaussian data\n  \n  data_intervention &lt;- \n    tibble(condition = \"intervention\",\n           score = rsn(n = n, \n                       xi = location_intervention, # location, akin to mean\n                       omega = scale, # scale, akin to SD\n                       alpha = skew)) # slant/skew. When 0, produces normal/gaussian data\n  \n  data &lt;- bind_rows(data_control,\n                    data_intervention) \n  \n  return(data)\n}\n\n\n# define data analysis function ----\nanalyze_data_students_t &lt;- function(data) {\n  \n  hypothesis_test_students_t &lt;- t.test(formula = score ~ condition, \n                                       data = data,\n                                       var.equal = TRUE,\n                                       alternative = \"two.sided\")\n  \n  res &lt;- tibble(hypothesis_test_p_students_t = hypothesis_test_students_t$p.value) \n  \n  return(res)\n}\n\n\n# set the seed ----\n# for the pseudo random number generator to make results reproducible\nset.seed(42)\n\n\n# define experiment parameters ----\nexperiment_parameters &lt;- expand_grid(\n  n = c(10, 25, 50, 100, 200),\n  mu_control = 0,\n  mu_intervention = 0.5, \n  sigma = 1,\n  skew = c(0, 2, 4, 8, 12),     # slant/skew. When 0, produces normal/gaussian data\n  iteration = 1:1000 \n) |&gt;\n  # calculate skew-normal parameters\n  # don't worry about the math, it's not important to understand\n  mutate(delta = skew / sqrt(1 + skew^2),\n         scale = sigma / sqrt(1 - 2 * delta^2 / pi),\n         location_control = mu_control - scale * delta * sqrt(2 / pi),\n         location_intervention = mu_intervention - scale * delta * sqrt(2 / pi)) \n\n\n# run simulation ----\nsimulation &lt;- \n  # using the experiment parameters\n  experiment_parameters |&gt;\n  \n  # generate data using the data generating function and the parameters relevant to data generation\n  mutate(generated_data = future_pmap(.l = list(n = n,\n                                                location_control = location_control,\n                                                location_intervention = location_intervention,\n                                                scale = scale,\n                                                skew = skew),\n                                      .f = generate_data,\n                                      .progress = TRUE,\n                                      .options = furrr_options(seed = TRUE))) |&gt;\n  \n  # apply the analysis function to the generated data using the parameters relevant to analysis\n  mutate(analysis_results_students_t = future_pmap(.l = list(data = generated_data),\n                                                   .f = analyze_data_students_t,\n                                                   .progress = TRUE,\n                                                   .options = furrr_options(seed = TRUE)))\n\n# summarise simulation results over the iterations ----\n## ie what proportion of p values are significant (&lt; .05)\nsimulation_summary &lt;- simulation |&gt;\n  unnest(analysis_results_students_t) |&gt;\n  mutate(skew = as.factor(skew)) |&gt;\n  group_by(n, \n           location_control,\n           location_intervention,\n           scale, \n           skew) |&gt;\n  summarize(proportion_significant_students_t = mean(hypothesis_test_p_students_t &lt; .05),\n            .groups = \"drop\")\n\n# plot\nggplot(simulation_summary, aes(n*2, proportion_significant_students_t, color = skew)) +\n  geom_hline(yintercept = 0.80, linetype = \"dotted\") +\n  geom_hline(yintercept = 0.05, linetype = \"dotted\") +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = breaks_pretty(n = 9), name = \"Total sample size\") +\n  scale_y_continuous(limits = c(0, 1), breaks = breaks_pretty(n = 9), name = \"Proportion significant results\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.2, end = 0.8)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The statistical power of assumptions tests and the conditional use of (non-)parameteric tests\"</span>"
    ]
  },
  {
    "objectID": "chapters/6_testing_assumptions_and_conditional_tests.html#power-of-students-t-test-vs.-wilcoxon-rank-sum-test",
    "href": "chapters/6_testing_assumptions_and_conditional_tests.html#power-of-students-t-test-vs.-wilcoxon-rank-sum-test",
    "title": "6  The statistical power of assumptions tests and the conditional use of (non-)parameteric tests”",
    "section": "6.6 Power of Student’s t-test vs. Wilcoxon rank-sum test",
    "text": "6.6 Power of Student’s t-test vs. Wilcoxon rank-sum test\nAka Mann-Whitney U-test.\n\n6.6.1 Exercise\nWrite a function called analyze_data_wilcox() that uses wilcox.test() instead of t.test() to compare the differences in central tendency between the conditions.\n\n\n6.6.2 Answer\n\n\nShow solution\n# define data analysis function ----\nanalyze_data_wilcox &lt;- function(data) {\n  \n  hypothesis_test_wilcox &lt;- wilcox.test(formula = score ~ condition,  # added\n                                        data = data,\n                                        alternative = \"two.sided\")\n  \n  res &lt;- tibble(hypothesis_test_p_wilcox = hypothesis_test_wilcox$p.value)  # added \n  \n  return(res)\n}\n\n\n\n\n6.6.3 Simulation\n\n\nCode\n# run simulation ----\nsimulation_alt_analysis &lt;- \n  # using the experiment parameters\n  simulation |&gt;\n\n  # apply the analysis function to the generated data using the parameters relevant to analysis\n  mutate(analysis_results_wilcox = future_pmap(.l = list(data = generated_data),\n                                               .f = analyze_data_wilcox,\n                                               .progress = TRUE,\n                                               .options = furrr_options(seed = TRUE)))\n\n\n# summarise simulation results over the iterations ----\n## ie what proportion of p values are significant (&lt; .05)\nsimulation_summary &lt;- simulation_alt_analysis |&gt;\n  unnest(analysis_results_students_t) |&gt;\n  unnest(analysis_results_wilcox) |&gt;\n  mutate(skew = as.factor(skew)) |&gt;\n  group_by(n, \n           location_control,\n           location_intervention,\n           scale, \n           skew) |&gt;\n  summarize(proportion_significant_students_t = mean(hypothesis_test_p_students_t &lt; .05),\n            proportion_significant_wilcox = mean(hypothesis_test_p_wilcox &lt; .05),\n            .groups = \"drop\")\n\n# plots\nggplot(simulation_summary, aes(n*2, proportion_significant_students_t, color = skew)) +\n  geom_hline(yintercept = 0.80, linetype = \"dotted\") +\n  geom_hline(yintercept = 0.05, linetype = \"dotted\") +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = breaks_pretty(n = 9), name = \"Total sample size\") +\n  scale_y_continuous(limits = c(0, 1), breaks = breaks_pretty(n = 9), name = \"Proportion significant results\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.2, end = 0.8) +\n  ggtitle(\"Power of Student's t-test for Cohen's d = 0.5\")\n\n\n\n\n\n\n\n\n\nCode\nggplot(simulation_summary, aes(n*2, proportion_significant_wilcox, color = skew)) +\n  geom_hline(yintercept = 0.80, linetype = \"dotted\") +\n  geom_hline(yintercept = 0.05, linetype = \"dotted\") +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = breaks_pretty(n = 9), name = \"Total sample size\") +\n  scale_y_continuous(limits = c(0, 1), breaks = breaks_pretty(n = 9), name = \"Proportion significant results\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.2, end = 0.8) +\n  ggtitle(\"Power of Wilcoxon rank-sum test for Cohen's d = 0.5\")\n\n\n\n\n\n\n\n\n\n\nWhat’s the upside and downside of each?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The statistical power of assumptions tests and the conditional use of (non-)parameteric tests\"</span>"
    ]
  },
  {
    "objectID": "chapters/6_testing_assumptions_and_conditional_tests.html#power-of-shapiro-wilks-test-with-skew-normal-data",
    "href": "chapters/6_testing_assumptions_and_conditional_tests.html#power-of-shapiro-wilks-test-with-skew-normal-data",
    "title": "6  The statistical power of assumptions tests and the conditional use of (non-)parameteric tests”",
    "section": "6.7 Power of Shapiro-Wilk’s test with skew-normal data",
    "text": "6.7 Power of Shapiro-Wilk’s test with skew-normal data\n\n6.7.1 Exercise\nWrite a function called test_assumption_of_normality() that uses shapiro.test() to assess for non-normality in each condition (control and intervention). It should return a p value for both tests.\n\n\n6.7.2 Answer\n\n\nCode\n# define data analysis function ----\ntest_assumption_of_normality &lt;- function(data) {\n  \n  fit_intervention &lt;- data |&gt;\n    filter(condition == \"intervention\") |&gt;\n    pull(score) |&gt;\n    shapiro.test()\n  \n  fit_control &lt;- data |&gt;\n    filter(condition == \"control\") |&gt;\n    pull(score) |&gt;\n    shapiro.test()\n  \n  res &lt;- tibble(assumption_test_p_sharpirowilks_intervention = fit_intervention$p.value, \n                assumption_test_p_sharpirowilks_control = fit_control$p.value) \n  \n  return(res)\n}\n\n\n\n\n6.7.3 Simulation\nTest for normality in each of two samples, and return a decision of non-normality if it is found in either.\n\n\nCode\n# run simulation ----\nsimulation_normality &lt;- \n  # using the experiment parameters\n  simulation_alt_analysis |&gt;\n\n  # apply the analysis function to the generated data using the parameters relevant to analysis\n  mutate(analysis_results_normality = future_pmap(.l = list(generated_data),\n                                                  .f = test_assumption_of_normality,\n                                                  .progress = TRUE,\n                                                  .options = furrr_options(seed = TRUE)))\n\n# summarise simulation results over the iterations ----\n## ie what proportion of p values are significant (&lt; .05)\nsimulation_summary &lt;- simulation_normality |&gt;\n  unnest(analysis_results_students_t) |&gt;\n  unnest(analysis_results_wilcox) |&gt;\n  unnest(analysis_results_normality) |&gt;\n  mutate(skew = as.factor(skew)) |&gt;\n  # choose the lower of the two shapiro wilk's tests' p values\n  mutate(assumption_test_p_sharpirowilks_lower = ifelse(assumption_test_p_sharpirowilks_intervention &lt; assumption_test_p_sharpirowilks_control, \n                                                        assumption_test_p_sharpirowilks_intervention, \n                                                        assumption_test_p_sharpirowilks_control)) |&gt;\n  group_by(n, \n           location_control,\n           location_intervention,\n           scale, \n           skew) |&gt;\n  summarize(proportion_significant_students_t = mean(hypothesis_test_p_students_t &lt; .05),\n            proportion_significant_wilcox = mean(hypothesis_test_p_wilcox &lt; .05),\n            proportion_significant_shapirowilks = mean(assumption_test_p_sharpirowilks_lower &lt; .05),\n            .groups = \"drop\")\n\n# plot\nggplot(simulation_summary, aes(n*2, proportion_significant_shapirowilks, color = skew)) +\n  geom_hline(yintercept = 0.80, linetype = \"dotted\") +\n  geom_hline(yintercept = 0.05, linetype = \"dotted\") +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = breaks_pretty(n = 9), name = \"Total sample size\") +\n  scale_y_continuous(limits = c(0, 1), breaks = breaks_pretty(n = 9), name = \"Proportion significant results\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.2, end = 0.8) +\n  ggtitle(\"Power of Shapiro-Wilks test applied to each condition\")\n\n\n\n\n\n\n\n\n\n\nWhat does this tell us?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The statistical power of assumptions tests and the conditional use of (non-)parameteric tests\"</span>"
    ]
  },
  {
    "objectID": "chapters/6_testing_assumptions_and_conditional_tests.html#conditionally-running-a-students-t-test-or-a-wilcoxon-rank-sum-test-depending-on-shapiro-wilks-test-for-normality-in-both-samples",
    "href": "chapters/6_testing_assumptions_and_conditional_tests.html#conditionally-running-a-students-t-test-or-a-wilcoxon-rank-sum-test-depending-on-shapiro-wilks-test-for-normality-in-both-samples",
    "title": "6  The statistical power of assumptions tests and the conditional use of (non-)parameteric tests”",
    "section": "6.8 Conditionally running a Student’s t-test or a Wilcoxon rank-sum test depending on Shapiro-Wilk’s test for normality in both samples",
    "text": "6.8 Conditionally running a Student’s t-test or a Wilcoxon rank-sum test depending on Shapiro-Wilk’s test for normality in both samples\n\n6.8.1 Exercise\nModify the code below to implement condition logic between how the tests are interpreted.\nI.e., for each iteration, calculate a conditional p value: if either of the Shapiro-Wilks tests are significant (evidence of non-normality), use the p value from the Wilcox test; if not, use the Student’s t test’s p value.\nSummarize the power of the conditional p value vs. the Wilcox’s test’s p value. This compares the workflows where we a) use Shapiro-Wilk’s tests to choose which method to use to test the hypothesis vs. b) just use the non-parametric test by default.\n\n\nCode\n# summarise simulation results over the iterations ----\nsimulation_summary &lt;- simulation_normality |&gt;\n  unnest(analysis_results_students_t) |&gt;\n  unnest(analysis_results_wilcox) |&gt;\n  unnest(analysis_results_normality) |&gt;\n  # choose the lower of the two shapiro wilk's tests' p values\n  mutate(assumption_test_p_sharpirowilks_lower = ifelse(assumption_test_p_sharpirowilks_intervention &lt; assumption_test_p_sharpirowilks_control, \n                                                        assumption_test_p_sharpirowilks_intervention, \n                                                        assumption_test_p_sharpirowilks_control)) |&gt;\n  # condition logic\n  mutate(hypothesis_p_conditional = ifelse(assumption_test_p_sharpirowilks_lower &lt; .05, \n                                           hypothesis_test_p_wilcox,\n                                           hypothesis_test_p_students_t)) |&gt;\n  # summarize\n  mutate(skew = as.factor(skew)) |&gt;\n  group_by(n, \n           location_control,\n           location_intervention,\n           scale, \n           skew) |&gt;\n  summarize(proportion_significant_conditional = mean(hypothesis_p_conditional &lt; .05),\n            proportion_significant_wilcox = mean(hypothesis_test_p_wilcox &lt; .05),\n            .groups = \"drop\") |&gt;\n  mutate(power_diff = proportion_significant_conditional - proportion_significant_wilcox)\n\n\n\n\n6.8.2 Answer\nThis stimulation tests normality in both conditions’ data, as well as testing for differences in the central tendency using both parametric and non-parametric tests. Which test of the differences in central tendency is used for each simulated data set is determined by whether the assumption of normality is detectably violated.\n\n\nShow solution\n# summarise simulation results over the iterations ----\nsimulation_summary &lt;- simulation_normality |&gt;\n  unnest(analysis_results_students_t) |&gt;\n  unnest(analysis_results_wilcox) |&gt;\n  unnest(analysis_results_normality) |&gt;\n  # choose the lower of the two shapiro wilk's tests' p values\n  mutate(assumption_test_p_sharpirowilks_lower = ifelse(assumption_test_p_sharpirowilks_intervention &lt; assumption_test_p_sharpirowilks_control, \n                                                        assumption_test_p_sharpirowilks_intervention, \n                                                        assumption_test_p_sharpirowilks_control)) |&gt;\n  # condition logic\n  mutate(hypothesis_p_conditional = ifelse(assumption_test_p_sharpirowilks_lower &lt; .05, \n                                           hypothesis_test_p_wilcox,\n                                           hypothesis_test_p_students_t)) |&gt;\n  # summarize\n  mutate(skew = as.factor(skew)) |&gt;\n  group_by(n, \n           location_control,\n           location_intervention,\n           scale, \n           skew) |&gt;\n  summarize(proportion_significant_conditional = mean(hypothesis_p_conditional &lt; .05),\n            proportion_significant_wilcox = mean(hypothesis_test_p_wilcox &lt; .05),\n            .groups = \"drop\") |&gt;\n  mutate(power_diff = proportion_significant_conditional - proportion_significant_wilcox)\n\n# plots\nggplot(simulation_summary, aes(n*2, proportion_significant_conditional, color = skew)) +\n  geom_hline(yintercept = 0.80, linetype = \"dotted\") +\n  geom_hline(yintercept = 0.05, linetype = \"dotted\") +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = breaks_pretty(n = 9), name = \"Total sample size\") +\n  scale_y_continuous(limits = c(0, 1), breaks = breaks_pretty(n = 9), name = \"Proportion significant results\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.2, end = 0.8) +\n  ggtitle(\"Power of conditional Student's t-test vs. Wilcox test for Cohen's d = 0.5\")\n\n\n\n\n\n\n\n\n\nShow solution\n# ggsave(\"plots/conditional.png\",\n#        width = 7,\n#        height = 4)\n\nggplot(simulation_summary, aes(n*2, proportion_significant_wilcox, color = skew)) +\n  geom_hline(yintercept = 0.80, linetype = \"dotted\") +\n  geom_hline(yintercept = 0.05, linetype = \"dotted\") +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = breaks_pretty(n = 9), name = \"Total sample size\") +\n  scale_y_continuous(limits = c(0, 1), breaks = breaks_pretty(n = 9), name = \"Proportion significant results\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.2, end = 0.8) +\n  ggtitle(\"Power of Wilcoxon rank-sum test for Cohen's d = 0.5\")\n\n\n\n\n\n\n\n\n\nShow solution\n# ggsave(\"plots/nonparametric.png\",\n#        width = 7,\n#        height = 4)\n\nggplot(simulation_summary, aes(power_diff)) +\n  geom_histogram(binwidth = 0.01) +\n  theme_linedraw() +\n  xlab(\"Difference in power between conditional tests\\nvs. always running non-parametric\")\n\n\n\n\n\n\n\n\n\n\nWhat does this tell us about whether we should (a) test the assumption of normality and then run a (non)parameteric test conditionally or just run the non-parametric test by default?\nWhat interpretation issues might be an argument against doing this by default?\nAre you starting to see why statisticians usually reply with “it depends” when we ask them questions?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The statistical power of assumptions tests and the conditional use of (non-)parameteric tests\"</span>"
    ]
  },
  {
    "objectID": "chapters/6_testing_assumptions_and_conditional_tests.html#recap",
    "href": "chapters/6_testing_assumptions_and_conditional_tests.html#recap",
    "title": "6  The statistical power of assumptions tests and the conditional use of (non-)parameteric tests”",
    "section": "6.9 Recap",
    "text": "6.9 Recap\nWhat did you learn here?\n\nAbout statistical tests?\nAbout using Monte Carlo simulations to understand not just single tests but to understand workflows?\nAbout how to use this {purrr} simulation workflow to analyze data more than one way and make conditional decisions, in order to model the behavior of scientists?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The statistical power of assumptions tests and the conditional use of (non-)parameteric tests\"</span>"
    ]
  },
  {
    "objectID": "chapters/6_testing_assumptions_and_conditional_tests.html#at-home-exercises",
    "href": "chapters/6_testing_assumptions_and_conditional_tests.html#at-home-exercises",
    "title": "6  The statistical power of assumptions tests and the conditional use of (non-)parameteric tests”",
    "section": "6.10 At-home exercises",
    "text": "6.10 At-home exercises\n\n6.10.1 Collate the simulation above into a single code chunk with all the pieces to run the full simulation\n\n\n6.10.2 Elaborate the simulation\nOne relatively simpler extension would be to assess how much better or worse just using the Student’s t test by default is compared to a) using Student’s t-test vs. Wilcox conditionally based on the Shapiro Wilks tests or b) using the Wilcox test by default. I.e., try adding c) using Student’s t-test by default.\nRight now the simulation only examines a fixed effect size. Perhaps the supposed negative impact of violating normality would be seen at other effect sizes? Vary this or indeed other things to make an informed decision about how to choose which test(s) to run to make the inference about differences in means between conditions.\n\n\n6.10.3 Develop a simulation to examine the power of the conditional use of (non)parametric based on heteroscedasticity vs using non-parametric tests by default\nDecisions about which hypothesis test is used are also made on the basis of other tests of those assumptions. For example, heteroscedasticity (equal SDs) can be tested using leveneTest(). This has lower power than Bartlett’s test but does not assume normality.\nWrite a simulation that is analogous to the one above, but which compares the statistical power of a) the conditional use of Student’s t test or Wilcox test based on the result of Levene’s test vs. b) using Wilcox test by default.\nFor simplicity:\n\nUse a true effect population effect size of Cohen’s d = 0.5 in all simulations.\nUse only normal data.\nVary degree of heteroscedasticity between the groups, i.e., set it to 1, 1.5, or 2 in the intervention group. Work out what you’ll need to set it to in the control group and why.\nUse the same sample sizes as as in the above simulation (i.e., n = c(10, 25, 50, 100, 200)).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The statistical power of assumptions tests and the conditional use of (non-)parameteric tests\"</span>"
    ]
  },
  {
    "objectID": "chapters/7_p_hacking.html",
    "href": "chapters/7_p_hacking.html",
    "title": "7  p-hacking via different forms of selecting reporting",
    "section": "",
    "text": "7.1 Overview\np-hacking is increasing the false-positive rate through analytic choices. These choices are often a) flexible, in that many different options might be tried until significance is found (Gelman called this the “Garden of Forking Paths”), and b) undisclosed. However, it need not be either in a given instance: perhaps the very first analytic strategy tried produces the significant result (but if it hadn’t, other strategies would have been tried), or perhaps the author fully discloses the analytic choices but does not make clear to the reader how this undermines the severity of the test (i.e., it takes a very close reading to understand that the results present weak evidence, or the verbal conclusions oversell this).\np-hacking can occur because of the uniform distribution of p-values under the null hypothesis. Because all values are equally likely, you just have to keep rolling the dice to eventually find p &lt; .05.\nThis lesson illustrates a few different examples of one broad form of p-hacking called selective reporting.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>p-hacking via different forms of selecting reporting</span>"
    ]
  },
  {
    "objectID": "chapters/7_p_hacking.html#dependencies",
    "href": "chapters/7_p_hacking.html#dependencies",
    "title": "7  p-hacking via different forms of selecting reporting",
    "section": "7.2 Dependencies",
    "text": "7.2 Dependencies\n\n\nCode\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(tibble)\nlibrary(purrr) \nlibrary(furrr)\nlibrary(faux)\nlibrary(janitor)\n#library(afex)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# set up parallelization\nplan(multisession)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>p-hacking via different forms of selecting reporting</span>"
    ]
  },
  {
    "objectID": "chapters/7_p_hacking.html#selective-reporting-of-studies",
    "href": "chapters/7_p_hacking.html#selective-reporting-of-studies",
    "title": "7  p-hacking via different forms of selecting reporting",
    "section": "7.3 Selective reporting of studies",
    "text": "7.3 Selective reporting of studies\nIf I run two experiments, and only report the one that “works”, I will increase the false positive rate across studies. Perhaps it is because one really is better designed etc than the other. Or perhaps its just a false positive.\nWhat would you guess the false positive rate is for two studies?\nLet’s simulate it.\n\n7.3.1 Run simulation\n\n\nCode\ngenerate_data &lt;- function(n_per_condition,\n                          mean_control,\n                          mean_intervention,\n                          sd) {\n  \n  data_control &lt;- \n    tibble(condition = \"control\",\n           score = rnorm(n = n_per_condition, mean = mean_control, sd = sd))\n  \n  data_intervention &lt;- \n    tibble(condition = \"intervention\",\n           score = rnorm(n = n_per_condition, mean = mean_intervention, sd = sd))\n  \n  data_combined &lt;- bind_rows(data_control,\n                             data_intervention)\n  \n  return(data_combined)\n}\n\n\n# define data analysis function ----\nanalyze &lt;- function(data) {\n  \n  students_ttest &lt;- t.test(formula = score ~ condition,\n                           data = data,\n                           var.equal = TRUE,\n                           alternative = \"two.sided\")\n  \n  res &lt;- tibble(p = students_ttest$p.value)\n  \n  return(res)\n}\n\n\n# define experiment parameters ----\nexperiment_parameters &lt;- expand_grid(\n  n_per_condition = 100, \n  mean_control = 0,\n  mean_intervention = 0,\n  sd = 1,\n  iteration = 1:1000\n)\n\n\n# run simulation ----\nset.seed(42)\n\nsimulation &lt;- \n  # using the experiment parameters\n  experiment_parameters |&gt;\n  \n  # generate data using the data generating function and the parameters relevant to data generation\n  mutate(generated_data_study1 = future_pmap(.l = list(n_per_condition,\n                                                       mean_control,\n                                                       mean_intervention,\n                                                       sd),\n                                             .f = generate_data,\n                                             .progress = TRUE,\n                                             .options = furrr_options(seed = TRUE))) |&gt;\n  mutate(generated_data_study2 = future_pmap(.l = list(n_per_condition,\n                                                       mean_control,\n                                                       mean_intervention,\n                                                       sd),\n                                             .f = generate_data,\n                                             .progress = TRUE,\n                                             .options = furrr_options(seed = TRUE))) |&gt;\n  \n  # apply the analysis function to the generated data using the parameters relevant to analysis\n  mutate(results_study1 = future_pmap(.l = list(data = generated_data_study1),\n                                      .f = analyze,\n                                      .progress = TRUE,\n                                      .options = furrr_options(seed = TRUE))) |&gt;\n  mutate(results_study2 = future_pmap(.l = list(data = generated_data_study2),\n                                      .f = analyze,\n                                      .progress = TRUE,\n                                      .options = furrr_options(seed = TRUE)))\n\n\n# summarise simulation results over the iterations ----\nsimulation_summary &lt;- simulation |&gt;\n  # unnest and rename\n  unnest(results_study1) |&gt;\n  rename(p_study1 = p) |&gt;\n  unnest(results_study2) |&gt;\n  rename(p_study2 = p) |&gt;\n  # simulate flexible reporting\n  mutate(p_hacked = ifelse(p_study1 &lt; .05, p_study1, p_study2)) |&gt;\n  # summarize\n  summarize(prop_sig_study1 = mean(p_study1 &lt; .05),\n            prop_sig_study2 = mean(p_study2 &lt; .05),\n            prop_sig_hacked = mean(p_hacked &lt; .05),\n            .groups = \"drop\") |&gt;\n  pivot_longer(cols = everything(), \n               names_to = \"Source\",\n               values_to = \"proportion_significant\") |&gt;\n  mutate(Source = str_remove(Source, \"prop_sig_\"))\n\n\n\n\n7.3.2 Results\n\n\nCode\nsimulation_summary |&gt;\n  mutate_if(is.numeric, round_half_up, digits = 2) |&gt;\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\nSource\nproportion_significant\n\n\n\n\nstudy1\n0.04\n\n\nstudy2\n0.05\n\n\nhacked\n0.09\n\n\n\n\n\n\n\n7.3.3 Conclusions\nThe false positive rate for two studies is ~10%, because each study has a 5% false positive rate.\n\n\n7.3.4 Exercise: extend the simulation\nExtend the simulation so that four studies are run. What do you expect the false positive rate to be? What do you find?\n\n\n7.3.5 Solution\n\n\nCode\ngenerate_data &lt;- function(n_per_condition,\n                          mean_control,\n                          mean_intervention,\n                          sd) {\n  \n  data_control &lt;- \n    tibble(condition = \"control\",\n           score = rnorm(n = n_per_condition, mean = mean_control, sd = sd))\n  \n  data_intervention &lt;- \n    tibble(condition = \"intervention\",\n           score = rnorm(n = n_per_condition, mean = mean_intervention, sd = sd))\n  \n  data_combined &lt;- bind_rows(data_control,\n                             data_intervention)\n  \n  return(data_combined)\n}\n\n\n# define data analysis function ----\nanalyze &lt;- function(data) {\n  \n  students_ttest &lt;- t.test(formula = score ~ condition,\n                           data = data,\n                           var.equal = TRUE,\n                           alternative = \"two.sided\")\n  \n  res &lt;- tibble(p = students_ttest$p.value)\n  \n  return(res)\n}\n\n\n# define experiment parameters ----\nexperiment_parameters &lt;- expand_grid(\n  n_per_condition = 100, \n  mean_control = 0,\n  mean_intervention = 0,\n  sd = 1,\n  iteration = 1:1000\n)\n\n\n# run simulation ----\nset.seed(42)\n\nsimulation &lt;- \n  # using the experiment parameters\n  experiment_parameters |&gt;\n  \n  # generate data using the data generating function and the parameters relevant to data generation\n  mutate(generated_data_study1 = future_pmap(.l = list(n_per_condition = n_per_condition,\n                                                       mean_control = mean_control,\n                                                       mean_intervention = mean_intervention,\n                                                       sd = sd),\n                                             .f = generate_data,\n                                             .progress = TRUE,\n                                             .options = furrr_options(seed = TRUE))) |&gt;\n  mutate(generated_data_study2 = future_pmap(.l = list(n_per_condition = n_per_condition,\n                                                       mean_control = mean_control,\n                                                       mean_intervention = mean_intervention,\n                                                       sd = sd),\n                                             .f = generate_data,\n                                             .progress = TRUE,\n                                             .options = furrr_options(seed = TRUE))) |&gt;\n  mutate(generated_data_study3 = future_pmap(.l = list(n_per_condition = n_per_condition,\n                                                       mean_control = mean_control,\n                                                       mean_intervention = mean_intervention,\n                                                       sd = sd),\n                                             .f = generate_data,\n                                             .progress = TRUE,\n                                             .options = furrr_options(seed = TRUE))) |&gt;\n  mutate(generated_data_study4 = future_pmap(.l = list(n_per_condition = n_per_condition,\n                                                       mean_control = mean_control,\n                                                       mean_intervention = mean_intervention,\n                                                       sd = sd),\n                                             .f = generate_data,\n                                             .progress = TRUE,\n                                             .options = furrr_options(seed = TRUE))) |&gt;\n  \n  # apply the analysis function to the generated data using the parameters relevant to analysis\n  mutate(results_study1 = future_pmap(.l = list(data = generated_data_study1),\n                                      analyze,\n                                      .progress = TRUE,\n                                      .options = furrr_options(seed = TRUE))) |&gt;\n  mutate(results_study2 = future_pmap(.l = list(data = generated_data_study2),\n                                      analyze,\n                                      .progress = TRUE,\n                                      .options = furrr_options(seed = TRUE))) |&gt;\n  mutate(results_study3 = future_pmap(.l = list(data = generated_data_study3),\n                                      analyze,\n                                      .progress = TRUE,\n                                      .options = furrr_options(seed = TRUE))) |&gt;\n  mutate(results_study4 = future_pmap(.l = list(data = generated_data_study4),\n                                      analyze,\n                                      .progress = TRUE,\n                                      .options = furrr_options(seed = TRUE)))\n\n\n# summarise simulation results over the iterations ----\nsimulation_summary &lt;- simulation |&gt;\n  # unnest and rename\n  unnest(results_study1) |&gt;\n  rename(p_study1 = p) |&gt;\n  unnest(results_study2) |&gt;\n  rename(p_study2 = p) |&gt;\n  unnest(results_study3) |&gt;\n  rename(p_study3 = p) |&gt;\n  unnest(results_study4) |&gt;\n  rename(p_study4 = p) |&gt;\n  # simulate flexible reporting\n  mutate(p_hacked = case_when(p_study1 &lt; .05 ~ p_study1,\n                              p_study2 &lt; .05 ~ p_study2,\n                              p_study3 &lt; .05 ~ p_study3,\n                              TRUE ~ p_study4)) |&gt;\n  # summarize\n  summarize(prop_sig_study1 = mean(p_study1 &lt; .05),\n            prop_sig_study2 = mean(p_study2 &lt; .05),\n            prop_sig_study3 = mean(p_study3 &lt; .05),\n            prop_sig_study4 = mean(p_study4 &lt; .05),\n            prop_sig_hacked = mean(p_hacked &lt; .05),\n            .groups = \"drop\") |&gt;\n  pivot_longer(cols = everything(), \n               names_to = \"Source\",\n               values_to = \"proportion_significant\") |&gt;\n  mutate(Source = str_remove(Source, \"prop_sig_\"))\n\nsimulation_summary |&gt;\n  mutate_if(is.numeric, round_half_up, digits = 2) |&gt;\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\nSource\nproportion_significant\n\n\n\n\nstudy1\n0.04\n\n\nstudy2\n0.05\n\n\nstudy3\n0.04\n\n\nstudy4\n0.05\n\n\nhacked\n0.17",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>p-hacking via different forms of selecting reporting</span>"
    ]
  },
  {
    "objectID": "chapters/7_p_hacking.html#false-positive-rates",
    "href": "chapters/7_p_hacking.html#false-positive-rates",
    "title": "7  p-hacking via different forms of selecting reporting",
    "section": "7.4 False positive rates",
    "text": "7.4 False positive rates\nNote that these probabilities are merely additive. This might not be intuitive.\nWhile the false positive rate for any individual test is 5%, the probability of finding at least one significant results in exactly \\(k\\) number of tests follows the equation:\n\\[\nFPR = 1-(1-\\alpha)^k,\n\\]\nwhere \\(\\alpha\\) is the alpha value for the tests (e.g., 0.05) and \\(k\\) is the number of tests run.\nWe can write a function to calculate the FPR mathematically for different values of \\(\\alpha\\) and \\(k\\).\n\n\nCode\nfpr &lt;- function(k, alpha = 0.05){\n  fpr &lt;- 1 - (1 - alpha)^k\n  return(fpr)\n}\n\ndat_fpr &lt;- tibble(k = seq(from = 1, to = 10, by = 1)) |&gt;\n  mutate(fpr = map_dbl(k, fpr)) \n\ndat_fpr |&gt;\n  mutate_if(is.numeric, round_half_up, digits = 2) |&gt;\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\nk\nfpr\n\n\n\n\n1\n0.05\n\n\n2\n0.10\n\n\n3\n0.14\n\n\n4\n0.19\n\n\n5\n0.23\n\n\n6\n0.26\n\n\n7\n0.30\n\n\n8\n0.34\n\n\n9\n0.37\n\n\n10\n0.40",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>p-hacking via different forms of selecting reporting</span>"
    ]
  },
  {
    "objectID": "chapters/7_p_hacking.html#selective-reporting-of-hypotheses-harking",
    "href": "chapters/7_p_hacking.html#selective-reporting-of-hypotheses-harking",
    "title": "7  p-hacking via different forms of selecting reporting",
    "section": "7.5 Selective reporting of hypotheses / HARKing",
    "text": "7.5 Selective reporting of hypotheses / HARKing\nAka lack of familywise error correction. NB this might sometimes be seen more like selective interpretation if all results are reported, or just low test severity.\nANOVAs are, statistically speaking, somewhat dangerous because a) they by default include interactions among independent variables, but b) they do not by default apply familywise error corrections.\nThis means that if I include \\(k\\) independent variables, it produces \\(2^k - 1\\) p values for its main and interaction effects. This can rapidly increase the FPR. Until relatively recently, many people were also taught that ANOVA inherently corrects for multiple testing, when it does not - I was taught this in my Bachelors.\n\n7.5.1 Run simulation - not working\nI’ve written the generate data code in a more abstract way so that the number of independent variables can be changed easily.\n\n\nCode\n# # define generate data function ----\n# generate_data &lt;- function(n,\n#                           ivs,\n#                           mu = 0,\n#                           sd = 1) { \n#   \n#   n_iv &lt;- function(n) {\n#     strings &lt;- paste(sapply(1:n, function(i) paste0(\"x\", i, \" = c(group1 = 'Condition 1', group2 = 'Condition 2')\")), collapse = \", \") \n#     single_string &lt;- paste(strings, collapse = \", \")\n#     list_string &lt;- paste0(\"list(\", single_string, \")\")\n#     return(list_string)\n#   }\n#   \n#   data &lt;- sim_design(between = eval(parse(text = n_iv(ivs))), \n#                      n = 100, \n#                      mu = mu, \n#                      sd = sd,\n#                      plot = FALSE) |&gt;\n#     mutate(id = as.factor(id))\n#   \n#   return(data)\n# }\n# \n# # define data analysis function ----\n# analyse_data &lt;- function(data, ivs) {\n#   \n#   # generate a list of IVs\n#   generate_c_string &lt;- function(n) {\n#     sapply(1:n, function(i) paste0(\"x\", i))\n#   }\n#   \n#   # define contrasts option so it doesn't print message on every iteration\n#   options(contrasts = c(\"contr.sum\", \"contr.poly\"))\n#   \n#   fit &lt;- afex::aov_ez(id = \"id\", \n#                       dv = \"y\", \n#                       between = generate_c_string(ivs), \n#                       data = data,\n#                       anova_table = \"pes\")\n#   \n#   results &lt;- fit$anova_table |&gt;\n#     rownames_to_column(var = \"parameter\") |&gt;\n#     rename(p = `Pr(&gt;F)`,\n#            partial_eta_2 = pes,\n#            num_df = `num Df`,\n#            den_df = `den Df`)\n#   \n#   return(results)\n# }\n# \n# \n# # simulation conditions ----\n# experiment_parameters_grid &lt;- expand_grid(\n#   n = 100, \n#   ivs = 2,\n#   mu = 0,\n#   sd = 1, \n#   iteration = 1:1000\n# )\n# \n# # run simulation ----\n# set.seed(42)\n# \n# simulation &lt;- \n#   # using the experiment parameters\n#   experiment_parameters_grid |&gt;\n#   \n#   # generate data using the data generating function and the parameters relevant to data generation\n#   mutate(generated_data = future_pmap(.l = list(n = n,\n#                                                 ivs = ivs,\n#                                                 mu = mu,\n#                                                 sd = sd),\n#                                       .f = generate_data,\n#                                       .progress = TRUE,\n#                                       .options = furrr_options(seed = TRUE))) |&gt;\n#   \n#   # apply the analysis function to the generated data using the parameters relevant to analysis\n#   mutate(analysis_results = future_pmap(.l = list(generated_data = generated_data,\n#                                                   ivs = ivs),\n#                                         .f = analyse_data,\n#                                         .progress = TRUE,\n#                                         .options = furrr_options(seed = TRUE)))\n\n\n\n\n7.5.2 Results\n\n7.5.2.1 FPR by effect\n\n\nCode\n# simulation_summary &lt;- simulation |&gt;\n#   unnest(analysis_results) |&gt;\n#   # summarize\n#   group_by(ivs, parameter) |&gt;\n#   summarize(proportion_positive_results = mean(p &lt; .05))\n# \n# simulation_summary |&gt;\n#   mutate_if(is.numeric, round_half_up, digits = 2) |&gt;\n#   kable() |&gt;\n#   kable_classic(full_width = FALSE)\n\n\n\n\n7.5.2.2 FPR by ANOVA\n\n\nCode\n# simulation_summary &lt;- simulation |&gt;\n#   unnest(analysis_results) |&gt;\n#   # summarize\n#   group_by(ivs, iteration) |&gt;\n#   summarize(minimum_p = min(p)) |&gt;\n#   group_by(ivs) |&gt;\n#   summarize(proportion_positive_results = mean(minimum_p &lt; .05))\n# \n# simulation_summary |&gt;\n#   mutate_if(is.numeric, round_half_up, digits = 2) |&gt;\n#   kable() |&gt;\n#   kable_classic(full_width = FALSE)\n\n\n\n\n7.5.2.3 FPR by ANOVA with familywise error corrections\n\n\nCode\n# simulation_summary &lt;- simulation |&gt;\n#   unnest(analysis_results) |&gt;\n#   # summarize\n#   group_by(ivs, iteration) |&gt;\n#   mutate(p_adjusted = p.adjust(p, method = \"holm\")) |&gt;\n#   summarize(minimum_p_adjusted = min(p_adjusted)) |&gt;\n#   group_by(ivs) |&gt;\n#   summarize(proportion_positive_results = mean(minimum_p_adjusted &lt; .05))\n# \n# simulation_summary |&gt;\n#   mutate_if(is.numeric, round_half_up, digits = 2) |&gt;\n#   kable() |&gt;\n#   kable_classic(full_width = FALSE)\n\n\n\n\n\n7.5.3 Conclusions\nThe results are similar to the first simulation on multiple studies because the tests are independent.\n\n\n7.5.4 Exercise: extend the simulation\nIncrease the number of IVs included and observe the increase in FPR. This should follow the mathematical solution for the FPR (with some error), but simulating it allows us to see it happening rather than taking the math at face value.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>p-hacking via different forms of selecting reporting</span>"
    ]
  },
  {
    "objectID": "chapters/7_p_hacking.html#selective-reporting-of-outcomes",
    "href": "chapters/7_p_hacking.html#selective-reporting-of-outcomes",
    "title": "7  p-hacking via different forms of selecting reporting",
    "section": "7.6 Selective reporting of outcomes",
    "text": "7.6 Selective reporting of outcomes\nAka outcome switching.\nThere are of course situations where the tests are not independent, and this makes it more complicated to understand the FPR. For example, imagine I am interested in the impact of CBT on depression, but I include multiple measures of depression and report the one that ‘worked’.\n\n7.6.1 Generate correlated data\nWe haven’t simulated correlated data before so lets do that first.\n\n\nCode\nset.seed(42)\n\ndat &lt;- \n  faux::rnorm_multi(n = 1000,\n                    mu = c(0, 0),\n                    sd = c(1, 1),\n                    r = -0.7,\n                    varnames = c(\"var1\", \"var2\"))\n\ncor(dat)\n\n\n           var1       var2\nvar1  1.0000000 -0.7083632\nvar2 -0.7083632  1.0000000\n\n\nCode\nggplot(dat, aes(var1, var2)) +\n  geom_point(alpha = 0.4) +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\n\n\n7.6.2 Generate data\nCorrelated outcomes, known groups differences.\n\n\nCode\ngenerate_data &lt;- function(n_per_condition,\n                          r_between_outcomes,\n                          mu_control, # vector of length 1 or k\n                          mu_intervention, # vector of length 1 or k\n                          sigma_control = 1, # vector of length 1 or k\n                          sigma_intervention = 1) {  # vector of length 1 or k\n  \n  # generate data by condition\n  data_control &lt;- \n    rnorm_multi(n = n_per_condition,\n                mu = mu_control,\n                sd = sigma_control,\n                r = r_between_outcomes,\n                varnames = c(\"outcome1\", \"outcome2\")) |&gt;\n    mutate(condition = \"Control\")\n  \n  data_intervention &lt;- \n    rnorm_multi(n = n_per_condition,\n                mu = mu_intervention,\n                sd = sigma_intervention,\n                r = r_between_outcomes,\n                varnames = c(\"outcome1\", \"outcome2\")) |&gt;\n    mutate(condition = \"Intervention\")\n  \n  # combine\n  data_combined &lt;- \n    bind_rows(data_control, \n              data_intervention) |&gt;\n    mutate(condition = fct_relevel(condition, \"Intervention\", \"Control\"))\n  \n  return(data_combined)  \n}\n\n\nCheck the function works\n\n\nCode\n# simulate data\nset.seed(42)\n\ndat &lt;- generate_data(n_per_condition = 100000,\n                     r_between_outcomes = 0.6,\n                     mu_control = 0,\n                     mu_intervention = c(1.5, 1.3))\n\n# parameter recovery\ndat |&gt;\n  group_by(condition) |&gt;\n  summarize(mean_outcome1 = mean(outcome1),\n            mean_outcome2 = mean(outcome2),\n            correlation = cor(outcome1, outcome2)) |&gt;\n  mutate_if(is.numeric, round_half_up, digits = 2)\n\n\n# A tibble: 2 × 4\n  condition    mean_outcome1 mean_outcome2 correlation\n  &lt;fct&gt;                &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 Intervention           1.5           1.3         0.6\n2 Control                0             0           0.6\n\n\n\n\n7.6.3 Run simulation\n\n\nCode\n# define data analysis function ----\nanalyze &lt;- function(data, outcome) {\n  \n  data_renamed &lt;- data |&gt;\n    rename(score = {{outcome}})\n  \n  students_ttest &lt;- t.test(formula = score ~ condition,\n                           data = data_renamed,\n                           var.equal = TRUE,\n                           alternative = \"two.sided\")\n  \n  res &lt;- tibble(p = students_ttest$p.value)\n  \n  return(res)\n}\n\n\n# define experiment parameters ----\nexperiment_parameters &lt;- expand_grid(\n  n_per_condition = 100, \n  r_between_outcomes = c(0, .2, .4, .6, .8, .9),\n  mu_control = 0,\n  mu_intervention = 0,\n  iteration = 1:1000\n) \n\n\n# run simulation ----\nset.seed(42)\n\nsimulation &lt;- \n  # using the experiment parameters\n  experiment_parameters |&gt;\n  \n  # generate data using the data generating function and the parameters relevant to data generation\n  mutate(generated_data = future_pmap(.l = list(n_per_condition,\n                                                r_between_outcomes,\n                                                mu_control,\n                                                mu_intervention),\n                                      .f = generate_data,\n                                      .progress = TRUE,\n                                      .options = furrr_options(seed = TRUE))) |&gt;\n  \n  # apply the analysis function to the generated data using the parameters relevant to analysis\n  mutate(results_outcome1 = future_pmap(.l = list(data = generated_data,\n                                                  outcome = \"outcome1\"),\n                                        .f = analyze,\n                                        .progress = TRUE,\n                                        .options = furrr_options(seed = TRUE)),\n         results_outcome2 = future_pmap(.l = list(data = generated_data,\n                                                  outcome = \"outcome2\"),\n                                        .f = analyze,\n                                        .progress = TRUE,\n                                        .options = furrr_options(seed = TRUE)))\n\n\n# summarise simulation results over the iterations ----\nsimulation_summary &lt;- simulation |&gt;\n  # unnest and rename\n  unnest(results_outcome1) |&gt;\n  rename(outcome1_p = p) |&gt;\n  unnest(results_outcome2) |&gt;\n  rename(outcome2_p = p) |&gt;\n  # simulate flexible reporting\n  mutate(hacked_p = ifelse(outcome1_p &lt; .05, outcome1_p, outcome2_p)) |&gt;\n  # summarize\n  group_by(n_per_condition,\n           r_between_outcomes,\n           mu_control,\n           mu_intervention) |&gt;\n  summarize(prop_sig_outcome1 = mean(outcome1_p &lt; .05),\n            prop_sig_outcome2 = mean(outcome2_p &lt; .05),\n            prop_sig_hacked = mean(hacked_p &lt; .05),\n            .groups = \"drop\") \n\n\n\n\n7.6.4 Results\n\n\nCode\nggplot(simulation_summary, aes(r_between_outcomes, prop_sig_outcome1)) +\n  geom_hline(yintercept = 0.05, linetype = \"dotted\") +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = breaks_pretty(n = 9), name = \"True correlation between outcomes\") + \n  scale_y_continuous(limits = c(0, .2), breaks = breaks_pretty(n = 5), name = \"Proportion significant results\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.2, end = 0.8) \n\n\n\n\n\n\n\n\n\nCode\nggplot(simulation_summary, aes(r_between_outcomes, prop_sig_outcome2)) +\n  geom_hline(yintercept = 0.05, linetype = \"dotted\") +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = breaks_pretty(n = 9), name = \"True correlation between outcomes\") + \n  scale_y_continuous(limits = c(0, .2), breaks = breaks_pretty(n = 5), name = \"Proportion significant results\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.2, end = 0.8) \n\n\n\n\n\n\n\n\n\nCode\nggplot(simulation_summary, aes(r_between_outcomes, prop_sig_hacked)) +\n  geom_hline(yintercept = 0.05, linetype = \"dotted\") +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = breaks_pretty(n = 9), name = \"True correlation between outcomes\") + \n  scale_y_continuous(limits = c(0, .2), breaks = breaks_pretty(n = 5), name = \"Proportion significant results\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.2, end = 0.8) \n\n\n\n\n\n\n\n\n\n\n\n7.6.5 Conclusions\nHow does the false positive rate change when the outcome measures are correlated?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>p-hacking via different forms of selecting reporting</span>"
    ]
  },
  {
    "objectID": "chapters/7_p_hacking.html#check-your-learning",
    "href": "chapters/7_p_hacking.html#check-your-learning",
    "title": "7  p-hacking via different forms of selecting reporting",
    "section": "7.7 Check your learning",
    "text": "7.7 Check your learning\nWould this problem be solved/improved if the authors reported the results of all outcome variables, not just the ones that ‘worked’?\n\n7.7.1 Answer\nNo - read about conjunctive vs disjunctive inferences [REF].\n\n\n7.7.2 Exercise: extend the simulation\nExtend the simulation to include additional outcome variables.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>p-hacking via different forms of selecting reporting</span>"
    ]
  },
  {
    "objectID": "chapters/7_p_hacking.html#readings",
    "href": "chapters/7_p_hacking.html#readings",
    "title": "7  p-hacking via different forms of selecting reporting",
    "section": "7.8 Readings",
    "text": "7.8 Readings\n\nCramer, A.O.J., van Ravenzwaaij, D., Matzke, D. et al. Hidden multiplicity in exploratory multiway ANOVA: Prevalence and remedies. Psychon Bull Rev 23, 640–647 (2016). https://doi.org/10.3758/s13423-015-0913-5\nStefan, A. M. and Schönbrodt F. D. (2023) Big little lies: a compendium and simulation of p-hacking strategies. Royal Society Open Science. 10220346 http://doi.org/10.1098/rsos.220346",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>p-hacking via different forms of selecting reporting</span>"
    ]
  },
  {
    "objectID": "chapters/8_p_values_and_confidence_intervals.html",
    "href": "chapters/8_p_values_and_confidence_intervals.html",
    "title": "8  Understanding the link between p-values, Confidence Intervals, and power",
    "section": "",
    "text": "8.1 Overview of tutorial\nThis tutorial simulates a population effect size of Cohen’s d = 0.5 for different sample sizes, and examines the relationship between Cohen’s d, its 95% Confidence Interval, and the significance of the t-test’s p-value.\nBy the end of this lesson you should understand that p-values are re-expressions of the same information conveyed by Confidence Intervals, and that statistical power is a re-expression of the width of Confidence Intervals.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Understanding the link between p-values, Confidence Intervals, and power</span>"
    ]
  },
  {
    "objectID": "chapters/8_p_values_and_confidence_intervals.html#dependencies",
    "href": "chapters/8_p_values_and_confidence_intervals.html#dependencies",
    "title": "8  Understanding the link between p-values, Confidence Intervals, and power",
    "section": "8.2 Dependencies",
    "text": "8.2 Dependencies\n\n\nCode\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(purrr) \nlibrary(stringr)\nlibrary(forcats)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(janitor)\nlibrary(effsize)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Understanding the link between p-values, Confidence Intervals, and power</span>"
    ]
  },
  {
    "objectID": "chapters/8_p_values_and_confidence_intervals.html#simulation",
    "href": "chapters/8_p_values_and_confidence_intervals.html#simulation",
    "title": "8  Understanding the link between p-values, Confidence Intervals, and power",
    "section": "8.3 Simulation",
    "text": "8.3 Simulation\n\n\nCode\n# functions for simulation\ngenerate_data &lt;- function(n_per_condition,\n                          mean_control,\n                          mean_intervention,\n                          sd) {\n  \n  data_control &lt;- \n    tibble(condition = \"control\",\n           score = rnorm(n = n_per_condition, mean = mean_control, sd = sd))\n  \n  data_intervention &lt;- \n    tibble(condition = \"intervention\",\n           score = rnorm(n = n_per_condition, mean = mean_intervention, sd = sd))\n  \n  data_combined &lt;- bind_rows(data_control,\n                             data_intervention) |&gt;\n    mutate(condition = fct_relevel(condition, \"intervention\", \"control\"))\n  \n  return(data_combined)\n}\n\nanalyze &lt;- function(data) {\n\n  res_t_test &lt;- t.test(formula = score ~ condition, \n                       data = data,\n                       var.equal = TRUE,\n                       alternative = \"two.sided\")\n  \n  res_cohens_d &lt;- effsize::cohen.d(formula = score ~ condition,\n                                   data = data,\n                                   pooled = TRUE)\n  \n  res &lt;- tibble(p = res_t_test$p.value, \n                cohens_d = res_cohens_d$estimate,\n                cohens_d_ci_lower = res_cohens_d$conf.int[1],\n                cohens_d_ci_upper = res_cohens_d$conf.int[2])\n\n  return(res)\n}\n\n\n# set seed\nset.seed(42)\n\n# simulation parameters\nexperiment_parameters &lt;- expand_grid(\n  n_per_condition = seq(from = 10, to = 90, by = 10),\n  mean_control = 0,\n  mean_intervention = 0.5,\n  sd = 1,\n  iteration = 1:1000\n) \n\n# run simulation\nsimulation &lt;- experiment_parameters |&gt;\n  mutate(generated_data = pmap(list(n_per_condition, \n                                    mean_control,\n                                    mean_intervention,\n                                    sd),\n                               generate_data)) |&gt;\n  mutate(results = pmap(list(generated_data),\n                        analyze))",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Understanding the link between p-values, Confidence Intervals, and power</span>"
    ]
  },
  {
    "objectID": "chapters/8_p_values_and_confidence_intervals.html#cohens-d-by-sample-size",
    "href": "chapters/8_p_values_and_confidence_intervals.html#cohens-d-by-sample-size",
    "title": "8  Understanding the link between p-values, Confidence Intervals, and power",
    "section": "8.4 Cohen’s d by sample size",
    "text": "8.4 Cohen’s d by sample size\n\n\nCode\n# wrangle\nsimulation |&gt;\n  unnest(results) |&gt;\n  # plot\n  ggplot(aes(n_per_condition*2, cohens_d)) +\n  geom_jitter(alpha = 0.25) +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  scale_x_continuous(breaks = breaks_pretty(n = 10),\n                     name = \"Total N\") +\n  scale_y_continuous(breaks = breaks_pretty(n = 10),\n                     #limits = c(0,1),\n                     name = \"Cohen's d\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.3, end = 0.7) +\n  ggtitle(\"Population Cohen's d = 0.5\")\n\n\n\n\n\n\n\n\n\n\nEach point is a Cohen’s d from one dataset (i.e., one iteration).\nThe population Cohen’s d is 0.5. Notice how the distribution of sample Cohen’s d values vary by sample size: Cohen’s d values above 2.0 and below -0.5 are observed when sample size is very small. As sample size gets larger, they stabilize. However, individual datasets still generate Cohen’s d values that diverge substantially from the true value of 0.5.\n\n\n8.4.1 Ordered by statistical significance\nLet’s color the Cohen’s ds by their statistical significance.\n\n\nCode\n# wrangle\nsimulation |&gt;\n  unnest(results) |&gt;\n  mutate(significant = p &lt; .05) |&gt;\n  # plot\n  ggplot(aes(n_per_condition*2, cohens_d, color = significant)) +\n  geom_jitter(alpha = 0.25) +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  scale_x_continuous(breaks = breaks_pretty(n = 10),\n                     name = \"Total N\") +\n  scale_y_continuous(breaks = breaks_pretty(n = 10),\n                     #limits = c(0,1),\n                     name = \"Cohen's d\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.3, end = 0.7, direction = -1) +\n  ggtitle(\"Population Cohen's d = 0.5\")\n\n\n\n\n\n\n\n\n\n\nThere is clearly a cut-off value for each sample size that determines when a Cohen’s d value is associated with a significant vs. non-significant t-test p-value. What determines this cut-off? Do you have an intuition for it?\n\n\n\n8.4.2 Average effect size for significant effects\nQuick aside: Significant results are more likely to be published than non significant ones. If we only look at the significant results, what is the average effect size by N?\nPlot only the significant effect sizes, and add their means.\n\n\nCode\n# wrangle\nsimulation |&gt;\n  unnest(results) |&gt;\n  mutate(significant = p &lt; .05) |&gt;\n  filter(significant) |&gt;\n  # plot\n  ggplot(aes(n_per_condition*2, cohens_d, color = significant)) +\n  geom_jitter(alpha = 0.25) +\n  # add mean points \n  stat_summary(fun = \"mean\",\n               geom = \"point\",\n               size = 3,\n               color = \"#35608DFF\") +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  scale_x_continuous(breaks = breaks_pretty(n = 10),\n                     name = \"Total N\") +\n  scale_y_continuous(breaks = breaks_pretty(n = 10),\n                     #limits = c(0,1),\n                     name = \"Cohen's d\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.3, end = 0.7, direction = -1) +\n  ggtitle(\"Population Cohen's d = 0.5\")\n\n\n\n\n\n\n\n\n\n\nBack to the q: There is clearly a cut-off value for each sample size that determines when a Cohen’s d value is associated with a significant vs. non-significant t-test p-value. What determines this cut-off? Do you have an intuition for it?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Understanding the link between p-values, Confidence Intervals, and power</span>"
    ]
  },
  {
    "objectID": "chapters/8_p_values_and_confidence_intervals.html#cohens-d-and-its-95-cis",
    "href": "chapters/8_p_values_and_confidence_intervals.html#cohens-d-and-its-95-cis",
    "title": "8  Understanding the link between p-values, Confidence Intervals, and power",
    "section": "8.5 Cohen’s d and its 95% CIs",
    "text": "8.5 Cohen’s d and its 95% CIs\nTo understand it further, let’s add the 95% Confidence Intervals.\n\n8.5.1 Randomly select one dataset per sample size\nTo understand the plot, we’ll plot just one data set (iteration) for each sample size.\nRe-run the chunk to select new random values. Notice how the magnitude of the Cohen’s d values ‘dance’ around between data sets due to random sampling error, but the width of the Confidence Intervals do not. Confidence width is steady as it is a function of sample size.\n\n\nCode\n# wrangle\nsimulation |&gt;\n  unnest(results) |&gt;\n  mutate(significant = p &lt; .05) |&gt;\n  # randomly sample 1 effects size per sample size\n  group_by(n_per_condition) |&gt;\n  slice_sample(n = 1) |&gt;\n  ungroup() |&gt;\n  # plot\n  ggplot(aes(n_per_condition*2, cohens_d, color = significant)) +\n  geom_point() +\n  # 95% CIs\n  geom_linerange(aes(ymin = cohens_d_ci_lower, ymax = cohens_d_ci_upper)) +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  scale_x_continuous(breaks = breaks_pretty(n = 10),\n                     name = \"Total N\") +\n  scale_y_continuous(breaks = breaks_pretty(n = 10),\n                     #limits = c(0,1),\n                     name = \"Cohen's d\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.3, end = 0.7) +\n  ggtitle(\"Population Cohen's d = 0.5\\nOne randomly selected dataset per sample size\")\n\n\n\n\n\n\n\n\n\n\nSignificant when 95% CI excludes zero effect size\n\n\n\n8.5.2 All datasets\nNow let’s plot all the data sets (iterations).\n\n\nCode\n# wrangle\nsimulation |&gt;\n  unnest(results) |&gt;\n  mutate(significant = p &lt; .05,\n         total_n = paste(\"N =\", n_per_condition*2),\n         total_n = fct_reorder(total_n, as.numeric(str_extract(total_n, \"\\\\d+\")))) |&gt;\n  # plot\n  ggplot(aes(iteration, cohens_d, color = significant)) +\n  geom_point(alpha = 0.5) +\n  geom_linerange(aes(ymin = cohens_d_ci_lower, ymax = cohens_d_ci_upper), alpha = 0.2) +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  scale_x_continuous(breaks = breaks_pretty(n = 10),\n                     name = \"Iteration\") +\n  scale_y_continuous(breaks = breaks_pretty(n = 10),\n                     #limits = c(0,1),\n                     name = \"Cohen's d\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.3, end = 0.7) +\n  facet_wrap(~ total_n, ncol = 3, scales = \"free_y\")\n\n\n\n\n\n\n\n\n\n\n\n8.5.3 Ordered by Cohen’s d\nThe above plot is hard to understand. Let’s order the Cohen’s ds from smallest to largest, so the small ones are on the left and the large ones are on the right.\n\n\nCode\n# wrangle\nsimulation |&gt;\n  unnest(results) |&gt;\n  mutate(significant = p &lt; .05,\n         total_n = paste(\"N =\", n_per_condition*2),\n         total_n = fct_reorder(total_n, as.numeric(str_extract(total_n, \"\\\\d+\")))) |&gt;\n  arrange(n_per_condition, cohens_d) |&gt;\n  group_by(n_per_condition) |&gt;\n  mutate(rank = row_number()) |&gt;\n  ungroup() |&gt;\n  # plot\n  ggplot(aes(rank, cohens_d, color = significant)) +\n  geom_point() +\n  geom_linerange(aes(ymin = cohens_d_ci_lower, ymax = cohens_d_ci_upper), alpha = 0.2) +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  scale_x_continuous(breaks = breaks_pretty(n = 10),\n                     name = \"Ranked iteration\") +\n  scale_y_continuous(breaks = breaks_pretty(n = 10),\n                     #limits = c(0,1),\n                     name = \"Cohen's d\") +\n  theme_linedraw() +\n  scale_color_viridis_d(begin = 0.3, end = 0.7) +\n  facet_wrap(~ total_n, ncol = 3)\n\n\n\n\n\n\n\n\n\n\nNotice the relationship between 95% CI and p value significance.\nA certain percentage of Cohen’s ds are green vs. blue. What is this percentage also called? What statistical property?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Understanding the link between p-values, Confidence Intervals, and power</span>"
    ]
  },
  {
    "objectID": "chapters/8_p_values_and_confidence_intervals.html#mean-cohens-d-and-mean-interval-width-by-sample-size",
    "href": "chapters/8_p_values_and_confidence_intervals.html#mean-cohens-d-and-mean-interval-width-by-sample-size",
    "title": "8  Understanding the link between p-values, Confidence Intervals, and power",
    "section": "8.6 Mean Cohen’s d and mean interval width by sample size",
    "text": "8.6 Mean Cohen’s d and mean interval width by sample size\nNow let’s average over the Cohen’s ds in each condition to find the mean Cohen’s d and its mean 95% CIs.\n\n\nCode\n# wrangle\nsimulation_summary &lt;- simulation |&gt;\n  # unnest results\n  unnest(results) |&gt;\n  group_by(n_per_condition) |&gt;\n  summarize(proportion_significant = mean(p &lt; .05), \n            mean_cohens_d = mean(cohens_d),\n            mean_cohens_d_ci_lower = mean(cohens_d_ci_lower),\n            mean_cohens_d_ci_upper = mean(cohens_d_ci_upper)) |&gt;\n  mutate(centered_mean_cohens_d_ci_lower = mean_cohens_d_ci_lower - mean_cohens_d,\n         centered_mean_cohens_d_ci_upper = mean_cohens_d_ci_upper - mean_cohens_d)\n\n# plot results\np1 &lt;- ggplot(simulation_summary, aes(n_per_condition*2, mean_cohens_d)) +\n  geom_point() +\n  geom_linerange(aes(ymin = mean_cohens_d_ci_lower, ymax = mean_cohens_d_ci_upper)) +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  scale_x_continuous(breaks = breaks_pretty(n = 10),\n                     name = \"Total sample size\") +\n  scale_y_continuous(breaks = breaks_pretty(n = 5),\n                     #limits = c(0,1),\n                     name = \"Mean Cohen's d\\n(and mean 95% CIs)\") +\n  theme_linedraw() +\n  ggtitle(\"Population Cohen's d = 0.5\")\n\np1",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Understanding the link between p-values, Confidence Intervals, and power</span>"
    ]
  },
  {
    "objectID": "chapters/8_p_values_and_confidence_intervals.html#mean-cohens-d-and-power-by-sample-size",
    "href": "chapters/8_p_values_and_confidence_intervals.html#mean-cohens-d-and-power-by-sample-size",
    "title": "8  Understanding the link between p-values, Confidence Intervals, and power",
    "section": "8.7 Mean Cohen’s d and power by sample size",
    "text": "8.7 Mean Cohen’s d and power by sample size\n\n\nCode\np2 &lt;- ggplot(simulation_summary, aes(n_per_condition*2, proportion_significant)) +\n  geom_point() +\n  geom_hline(yintercept = 0.05, linetype = \"dotted\") +\n  geom_hline(yintercept = 0.80, linetype = \"dotted\") +\n  scale_x_continuous(breaks = breaks_pretty(n = 10),\n                     name = \"Total sample size\") +\n  scale_y_continuous(breaks = breaks_pretty(n = 5),\n                     limits = c(0,1),\n                     name = \"Proportion of significant\\np-values\") +\n  theme_linedraw() \n\np1 + p2 + plot_layout(ncol = 1)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Understanding the link between p-values, Confidence Intervals, and power</span>"
    ]
  },
  {
    "objectID": "chapters/8_p_values_and_confidence_intervals.html#exercise-power-for-equivalence-test",
    "href": "chapters/8_p_values_and_confidence_intervals.html#exercise-power-for-equivalence-test",
    "title": "8  Understanding the link between p-values, Confidence Intervals, and power",
    "section": "8.8 Exercise: Power for equivalence test",
    "text": "8.8 Exercise: Power for equivalence test\nSee Lakens et al., 2018 for a tutorial on the concepts below.\nUsing a Smallest Effect Size of Interest (SESOI) of Cohen’s d = 0.2, what is the power of a Two One-Sided Equivalence Test (TOST) for different sample sizes? What N is needed for 80% power to detect a true null effect size as equivalent to zero?\nNote: because reasons, use the 90% Confidence Interval instead of 95 (see Lakens et al., 2018).\n\n\nCode\nif(file.exists(\"materials/lakens et al 2018 figure 1.png\")){\n  knitr::include_graphics(\"materials/lakens et al 2018 figure 1.png\")\n}",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Understanding the link between p-values, Confidence Intervals, and power</span>"
    ]
  },
  {
    "objectID": "chapters/9_difference_between_signfiicant_and_nonsignificant.html",
    "href": "chapters/9_difference_between_signfiicant_and_nonsignificant.html",
    "title": "9  The differences between significant and non-significant is not itself significant",
    "section": "",
    "text": "9.1 Dependencies\nCode\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(purrr) \nlibrary(stringr)\nlibrary(forcats)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(janitor)\nlibrary(effsize)\nlibrary(metafor)\n\nformat_p_apa &lt;- function(p, threshold = 0.001) {\n  ifelse(\n    p &lt; threshold,\n    paste0(\"p &lt; \", sub(\"^0\\\\.\", \".\", format(threshold, nsmall = 3))),\n    paste0(\"p = \", sub(\"^0\\\\.\", \".\", format(round(p, 3), nsmall = 3)))\n  )\n}",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The differences between significant and non-significant is not itself significant</span>"
    ]
  },
  {
    "objectID": "chapters/9_difference_between_signfiicant_and_nonsignificant.html#simulation-functions",
    "href": "chapters/9_difference_between_signfiicant_and_nonsignificant.html#simulation-functions",
    "title": "9  The differences between significant and non-significant is not itself significant",
    "section": "9.2 Simulation functions",
    "text": "9.2 Simulation functions\n\n\nCode\n# functions for simulation\ngenerate_data &lt;- function(n_per_condition,\n                          population_cohens_d) {\n  \n  data_control &lt;- \n    tibble(condition = \"control\",\n           score = rnorm(n = n_per_condition, mean = 0, sd = 1))\n  \n  data_intervention &lt;- \n    tibble(condition = \"intervention\",\n           score = rnorm(n = n_per_condition, mean = population_cohens_d, sd = 1))\n  \n  data_combined &lt;- bind_rows(data_control,\n                             data_intervention) |&gt;\n    mutate(condition = fct_relevel(condition, \"intervention\", \"control\"))\n  \n  return(data_combined)\n}\n\nanalyze &lt;- function(data, study) {\n\n  res_t_test &lt;- t.test(formula = score ~ condition, \n                       data = data,\n                       var.equal = TRUE,\n                       alternative = \"two.sided\")\n  \n  res_cohens_d &lt;- effsize::cohen.d(formula = score ~ condition,\n                                   data = data,\n                                   pooled = TRUE)\n  \n  res &lt;- tibble(study = study,\n                p = res_t_test$p.value, \n                cohens_d = res_cohens_d$estimate,\n                cohens_d_ci_lower = res_cohens_d$conf.int[1],\n                cohens_d_ci_upper = res_cohens_d$conf.int[2],\n                cohens_d_se = sqrt(res_cohens_d$var))\n\n  return(res)\n}\n\nmeta_analyze &lt;- function(results_study_1, results_study_2) {\n  \n  dat &lt;- bind_rows(results_study_1,\n                   results_study_2)\n  \n  fit &lt;- rma(data = dat, \n             yi = cohens_d, \n             sei = cohens_d_se, \n             mods = ~ study, \n             method = \"FE\")\n\n  res &lt;- tibble(cohens_d = fit$b[2],\n                cohens_d_ci_lower = fit$ci.lb[2],\n                cohens_d_ci_upper = fit$ci.ub[2],\n                #df = fit$QMdf[1],\n                #Q = fit$QM,\n                p = fit$QMp) # p of moderation\n    #mutate(formatted = paste0(\"Q(\", df, \") = \", round_half_up(Q, 3), \", \", format_p_apa(p)))\n\n  return(res)\n}",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The differences between significant and non-significant is not itself significant</span>"
    ]
  },
  {
    "objectID": "chapters/9_difference_between_signfiicant_and_nonsignificant.html#different-population-effect-large-n",
    "href": "chapters/9_difference_between_signfiicant_and_nonsignificant.html#different-population-effect-large-n",
    "title": "9  The differences between significant and non-significant is not itself significant",
    "section": "9.3 Different population effect, large N",
    "text": "9.3 Different population effect, large N\n\n\nCode\n# set seed\nset.seed(42)\n\n# simulation parameters\nexperiment_parameters &lt;- expand_grid(\n  study_1_n_per_condition = 300,\n  study_1_population_cohens_d = 0.5,\n  study_2_n_per_condition = 300,\n  study_2_population_cohens_d = 0,\n  iteration = 1:1000\n) \n\n# run simulation\nsimulation &lt;- experiment_parameters |&gt;\n  mutate(generated_data_study_1 = pmap(list(n_per_condition = study_1_n_per_condition, \n                                            population_cohens_d = study_1_population_cohens_d),\n                                       generate_data)) |&gt;\n  mutate(generated_data_study_2 = pmap(list(n_per_condition = study_2_n_per_condition, \n                                            population_cohens_d = study_2_population_cohens_d),\n                                       generate_data)) |&gt;\n  mutate(results_study1 = pmap(list(data = generated_data_study_1),\n                                analyze,\n                                study = \"study 1\")) |&gt;\n  mutate(results_study2 = pmap(list(data = generated_data_study_2),\n                                analyze,\n                                study = \"study 2\")) |&gt;\n  mutate(results_meta = pmap(list(results_study_1 = results_study1,\n                                  results_study_2 = results_study2),\n                             meta_analyze)) |&gt;\n  # wrangle results\n  unnest(results_study1, names_sep = \"_\") |&gt;\n  unnest(results_study2, names_sep = \"_\") |&gt;\n  unnest(results_meta, names_sep = \"_\") |&gt;\n  select(-generated_data_study_1, -generated_data_study_2) |&gt;\n  # this is a complex pivot, don't worry if you don't immediately understand it\n  pivot_longer(\n    cols = c(\n      results_study1_p,\n      results_study1_cohens_d,\n      results_study1_cohens_d_ci_lower,\n      results_study1_cohens_d_ci_upper,\n      results_study1_cohens_d_se,\n      results_study2_p,\n      results_study2_cohens_d,\n      results_study2_cohens_d_se,\n      results_study2_cohens_d_ci_lower,\n      results_study2_cohens_d_ci_upper,\n      results_meta_p,\n      results_meta_cohens_d,\n      # results_meta_cohens_d_se, # not created\n      results_meta_cohens_d_ci_lower,\n      results_meta_cohens_d_ci_upper\n    ),\n    names_to = c(\"source\", \"metric\"),\n    names_pattern = \"results_(study\\\\d+|meta)_(.+)\",\n    values_to = \"value\"\n  ) |&gt;\n  pivot_wider(\n    names_from = metric,\n    values_from = value\n  )\n\nsimulation_summary &lt;- simulation |&gt;\n  group_by(study_1_n_per_condition,\n           study_1_population_cohens_d,\n           study_2_n_per_condition,\n           study_2_population_cohens_d,\n           source) |&gt;\n  summarize(proportion_significant = mean(p &lt; .05),\n            mean_cohens_d = mean(cohens_d),\n            mean_cohens_d_ci_lower = mean(cohens_d_ci_lower),\n            mean_cohens_d_ci_upper = mean(cohens_d_ci_upper),\n            .groups = \"drop\") |&gt;\n  mutate(source = str_replace(source, \"meta\", \"difference\")) |&gt;\n  mutate(source = fct_relevel(source, \"study1\", \"study2\", \"difference\"))\n\n\np_effect_size &lt;- ggplot(simulation_summary, aes(source, mean_cohens_d)) +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  geom_linerange(aes(ymin = mean_cohens_d_ci_lower, ymax = mean_cohens_d_ci_upper)) +\n  geom_point() +\n  scale_y_continuous(breaks = breaks_pretty(n = 10),\n                     name = \"Mean Cohen's d\") +\n  xlab(\"\") +\n  theme_linedraw() +\n  theme(panel.grid.minor = element_blank())\n\np_proportion_significant &lt;- ggplot(simulation_summary, aes(source, proportion_significant)) +\n  geom_hline(yintercept = 0.05, linetype = \"dashed\") +\n  geom_hline(yintercept = 0.80, linetype = \"dashed\") +\n  geom_bar(stat = \"identity\") +\n  scale_y_continuous(breaks = breaks_pretty(n = 10), limits = c(0, 1), name = \"Proportion of significant p values\") +\n  xlab(\"\") +\n  # geom_text(aes(x = \"study1\", y = 1.1, label = paste(\"Population\\nCohen's d =\", study_1_population_cohens_d))) +\n  # geom_text(aes(x = \"study2\", y = 1.1, label = paste(\"Population\\nCohen's d =\", study_2_population_cohens_d))) +\n  # geom_text(aes(x = \"difference\", y = 1.1, label = paste(\"Population\\nCohen's d =\", study_2_population_cohens_d - study_1_population_cohens_d))) +\n  theme_linedraw() +\n  theme(panel.grid.minor = element_blank())\n\np_effect_size + p_proportion_significant + plot_layout(ncol = 2)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The differences between significant and non-significant is not itself significant</span>"
    ]
  },
  {
    "objectID": "chapters/9_difference_between_signfiicant_and_nonsignificant.html#different-population-effect-small-n",
    "href": "chapters/9_difference_between_signfiicant_and_nonsignificant.html#different-population-effect-small-n",
    "title": "9  The differences between significant and non-significant is not itself significant",
    "section": "9.4 Different population effect, small N",
    "text": "9.4 Different population effect, small N\n\n\nCode\n# set seed\nset.seed(42)\n\n# simulation parameters\nexperiment_parameters &lt;- expand_grid(\n  study_1_n_per_condition = 40,\n  study_1_population_cohens_d = 0.5,\n  study_2_n_per_condition = 40,\n  study_2_population_cohens_d = 0,\n  iteration = 1:1000\n) \n\n# run simulation\nsimulation &lt;- experiment_parameters |&gt;\n  mutate(generated_data_study_1 = pmap(list(n_per_condition = study_1_n_per_condition, \n                                            population_cohens_d = study_1_population_cohens_d),\n                                       generate_data)) |&gt;\n  mutate(generated_data_study_2 = pmap(list(n_per_condition = study_2_n_per_condition, \n                                            population_cohens_d = study_2_population_cohens_d),\n                                       generate_data)) |&gt;\n  mutate(results_study1 = pmap(list(data = generated_data_study_1),\n                                analyze,\n                                study = \"study 1\")) |&gt;\n  mutate(results_study2 = pmap(list(data = generated_data_study_2),\n                                analyze,\n                                study = \"study 2\")) |&gt;\n  mutate(results_meta = pmap(list(results_study_1 = results_study1,\n                                  results_study_2 = results_study2),\n                             meta_analyze)) |&gt;\n  # wrangle results\n  unnest(results_study1, names_sep = \"_\") |&gt;\n  unnest(results_study2, names_sep = \"_\") |&gt;\n  unnest(results_meta, names_sep = \"_\") |&gt;\n  select(-generated_data_study_1, -generated_data_study_2) |&gt;\n  # this is a complex pivot, don't worry if you don't immediately understand it\n  pivot_longer(\n    cols = c(\n      results_study1_p,\n      results_study1_cohens_d,\n      results_study1_cohens_d_ci_lower,\n      results_study1_cohens_d_ci_upper,\n      results_study1_cohens_d_se,\n      results_study2_p,\n      results_study2_cohens_d,\n      results_study2_cohens_d_se,\n      results_study2_cohens_d_ci_lower,\n      results_study2_cohens_d_ci_upper,\n      results_meta_p,\n      results_meta_cohens_d,\n      # results_meta_cohens_d_se, # not created\n      results_meta_cohens_d_ci_lower,\n      results_meta_cohens_d_ci_upper\n    ),\n    names_to = c(\"source\", \"metric\"),\n    names_pattern = \"results_(study\\\\d+|meta)_(.+)\",\n    values_to = \"value\"\n  ) |&gt;\n  pivot_wider(\n    names_from = metric,\n    values_from = value\n  )\n\nsimulation_summary &lt;- simulation |&gt;\n  group_by(study_1_n_per_condition,\n           study_1_population_cohens_d,\n           study_2_n_per_condition,\n           study_2_population_cohens_d,\n           source) |&gt;\n  summarize(proportion_significant = mean(p &lt; .05),\n            mean_cohens_d = mean(cohens_d),\n            mean_cohens_d_ci_lower = mean(cohens_d_ci_lower),\n            mean_cohens_d_ci_upper = mean(cohens_d_ci_upper),\n            .groups = \"drop\") |&gt;\n  mutate(source = str_replace(source, \"meta\", \"difference\")) |&gt;\n  mutate(source = fct_relevel(source, \"study1\", \"study2\", \"difference\"))\n\n\np_effect_size &lt;- ggplot(simulation_summary, aes(source, mean_cohens_d)) +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  geom_linerange(aes(ymin = mean_cohens_d_ci_lower, ymax = mean_cohens_d_ci_upper)) +\n  geom_point() +\n  scale_y_continuous(breaks = breaks_pretty(n = 10),\n                     name = \"Mean Cohen's d\") +\n  xlab(\"\") +\n  theme_linedraw() +\n  theme(panel.grid.minor = element_blank())\n\np_proportion_significant &lt;- ggplot(simulation_summary, aes(source, proportion_significant)) +\n  geom_hline(yintercept = 0.05, linetype = \"dashed\") +\n  geom_hline(yintercept = 0.80, linetype = \"dashed\") +\n  geom_bar(stat = \"identity\") +\n  scale_y_continuous(breaks = breaks_pretty(n = 10), limits = c(0, 1), name = \"Proportion of significant p values\") +\n  xlab(\"\") +\n  # geom_text(aes(x = \"study1\", y = 1.1, label = paste(\"Population\\nCohen's d =\", study_1_population_cohens_d))) +\n  # geom_text(aes(x = \"study2\", y = 1.1, label = paste(\"Population\\nCohen's d =\", study_2_population_cohens_d))) +\n  # geom_text(aes(x = \"difference\", y = 1.1, label = paste(\"Population\\nCohen's d =\", study_2_population_cohens_d - study_1_population_cohens_d))) +\n  theme_linedraw() +\n  theme(panel.grid.minor = element_blank())\n\np_effect_size + p_proportion_significant + plot_layout(ncol = 2)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The differences between significant and non-significant is not itself significant</span>"
    ]
  },
  {
    "objectID": "chapters/9_difference_between_signfiicant_and_nonsignificant.html#different-population-effect-small-original-study-and-large-replication",
    "href": "chapters/9_difference_between_signfiicant_and_nonsignificant.html#different-population-effect-small-original-study-and-large-replication",
    "title": "9  The differences between significant and non-significant is not itself significant",
    "section": "9.5 Different population effect, small original study and large “replication”",
    "text": "9.5 Different population effect, small original study and large “replication”\nNB not really a replication as the second study has a different effect size, but its a very rough simulation assuming p hacking took place in the original study and boosted the effect size\n[needs work to be clearer]\n\n\nCode\n# set seed\nset.seed(42)\n\n# simulation parameters\nexperiment_parameters &lt;- expand_grid(\n  study_1_n_per_condition = 40,\n  study_1_population_cohens_d = 0.5,\n  study_2_n_per_condition = 200,\n  study_2_population_cohens_d = 0,\n  iteration = 1:1000\n) \n\n# run simulation\nsimulation &lt;- experiment_parameters |&gt;\n  mutate(generated_data_study_1 = pmap(list(n_per_condition = study_1_n_per_condition, \n                                            population_cohens_d = study_1_population_cohens_d),\n                                       generate_data)) |&gt;\n  mutate(generated_data_study_2 = pmap(list(n_per_condition = study_2_n_per_condition, \n                                            population_cohens_d = study_2_population_cohens_d),\n                                       generate_data)) |&gt;\n  mutate(results_study1 = pmap(list(data = generated_data_study_1),\n                                analyze,\n                                study = \"study 1\")) |&gt;\n  mutate(results_study2 = pmap(list(data = generated_data_study_2),\n                                analyze,\n                                study = \"study 2\")) |&gt;\n  mutate(results_meta = pmap(list(results_study_1 = results_study1,\n                                  results_study_2 = results_study2),\n                             meta_analyze)) |&gt;\n  # wrangle results\n  unnest(results_study1, names_sep = \"_\") |&gt;\n  unnest(results_study2, names_sep = \"_\") |&gt;\n  unnest(results_meta, names_sep = \"_\") |&gt;\n  select(-generated_data_study_1, -generated_data_study_2) |&gt;\n  # this is a complex pivot, don't worry if you don't immediately understand it\n  pivot_longer(\n    cols = c(\n      results_study1_p,\n      results_study1_cohens_d,\n      results_study1_cohens_d_ci_lower,\n      results_study1_cohens_d_ci_upper,\n      results_study1_cohens_d_se,\n      results_study2_p,\n      results_study2_cohens_d,\n      results_study2_cohens_d_se,\n      results_study2_cohens_d_ci_lower,\n      results_study2_cohens_d_ci_upper,\n      results_meta_p,\n      results_meta_cohens_d,\n      # results_meta_cohens_d_se, # not created\n      results_meta_cohens_d_ci_lower,\n      results_meta_cohens_d_ci_upper\n    ),\n    names_to = c(\"source\", \"metric\"),\n    names_pattern = \"results_(study\\\\d+|meta)_(.+)\",\n    values_to = \"value\"\n  ) |&gt;\n  pivot_wider(\n    names_from = metric,\n    values_from = value\n  )\n\nsimulation_summary &lt;- simulation |&gt;\n  group_by(study_1_n_per_condition,\n           study_1_population_cohens_d,\n           study_2_n_per_condition,\n           study_2_population_cohens_d,\n           source) |&gt;\n  summarize(proportion_significant = mean(p &lt; .05),\n            mean_cohens_d = mean(cohens_d),\n            mean_cohens_d_ci_lower = mean(cohens_d_ci_lower),\n            mean_cohens_d_ci_upper = mean(cohens_d_ci_upper),\n            .groups = \"drop\") |&gt;\n  mutate(source = str_replace(source, \"meta\", \"difference\")) |&gt;\n  mutate(source = fct_relevel(source, \"study1\", \"study2\", \"difference\"))\n\n\np_effect_size &lt;- ggplot(simulation_summary, aes(source, mean_cohens_d)) +\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\n  geom_linerange(aes(ymin = mean_cohens_d_ci_lower, ymax = mean_cohens_d_ci_upper)) +\n  geom_point() +\n  scale_y_continuous(breaks = breaks_pretty(n = 10),\n                     name = \"Mean Cohen's d\") +\n  xlab(\"\") +\n  theme_linedraw() +\n  theme(panel.grid.minor = element_blank())\n\np_proportion_significant &lt;- ggplot(simulation_summary, aes(source, proportion_significant)) +\n  geom_hline(yintercept = 0.05, linetype = \"dashed\") +\n  geom_hline(yintercept = 0.80, linetype = \"dashed\") +\n  geom_bar(stat = \"identity\") +\n  scale_y_continuous(breaks = breaks_pretty(n = 10), limits = c(0, 1), name = \"Proportion of significant p values\") +\n  xlab(\"\") +\n  # geom_text(aes(x = \"study1\", y = 1.1, label = paste(\"Population\\nCohen's d =\", study_1_population_cohens_d))) +\n  # geom_text(aes(x = \"study2\", y = 1.1, label = paste(\"Population\\nCohen's d =\", study_2_population_cohens_d))) +\n  # geom_text(aes(x = \"difference\", y = 1.1, label = paste(\"Population\\nCohen's d =\", study_2_population_cohens_d - study_1_population_cohens_d))) +\n  theme_linedraw() +\n  theme(panel.grid.minor = element_blank())\n\np_effect_size + p_proportion_significant + plot_layout(ncol = 1)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>The differences between significant and non-significant is not itself significant</span>"
    ]
  },
  {
    "objectID": "chapters/10_standardized_effect_sizes_and_range_restriction.html",
    "href": "chapters/10_standardized_effect_sizes_and_range_restriction.html",
    "title": "10  Standardized effect sizes and range restriction",
    "section": "",
    "text": "10.1 Dependencies\nCode\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(sn)\nlibrary(janitor)\nlibrary(effsize)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(faux)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Standardized effect sizes and range restriction</span>"
    ]
  },
  {
    "objectID": "chapters/10_standardized_effect_sizes_and_range_restriction.html#load-data",
    "href": "chapters/10_standardized_effect_sizes_and_range_restriction.html#load-data",
    "title": "10  Standardized effect sizes and range restriction",
    "section": "10.2 Load data",
    "text": "10.2 Load data\nReal BDI-II data is taken from Cataldo et al. (2022) Abnormal Evidence Accumulation Underlies the Positive Memory Deficit in Depression, doi: 10.1037/xge0001268.\n\n\nCode\ndata_bdi &lt;- read_csv(\"../data/bdi_data.csv\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Standardized effect sizes and range restriction</span>"
    ]
  },
  {
    "objectID": "chapters/10_standardized_effect_sizes_and_range_restriction.html#why-standardize",
    "href": "chapters/10_standardized_effect_sizes_and_range_restriction.html#why-standardize",
    "title": "10  Standardized effect sizes and range restriction",
    "section": "10.3 Why standardize?",
    "text": "10.3 Why standardize?\nThey have different possible ranges, different population means (\\(\\mu\\)), and different population SDs (\\(\\sigma\\)).\nEven if had perfect that a given therapy has a (population) efficacy of lowering BDI-II depression scores by 6 points, without knowing a lot about the relationships between the BDI-II and other scores, we know little about how many points the same therapy would affect depression scores on the MADRS or the HAM-D.\n(surprisingly, very little work is ever done to collect information on the relationship between different scores so that we could know this)\nImagine three different published RCTs, each of which studied the efficacy of the same form of cognitive behavioral therapy for depression:\n\nRCT 1 found that it lowered depression scores on the BDI-II by 6 points on average\nRCT 2 found that it lowered depression scores on the MADRS by 8 points on average\nRCT 3 found that it lowered depression scores on the HAM-D by 4 points on average\n\nWhat is the efficacy of the intervention for depression scores on the PHQ-9? This is impossible to answer without knowing a lot about the details of the different scales (e.g., their min/max scores), the distribution of each scale’s scores in the population (eg population \\(\\mu\\) and \\(\\sigma\\)), and the relationship between different depression scales in the population. A one-point-change on one scale likely has a very different meaning to a one-point-change on another scale.\nWhat is the efficacy of the intervention for depression in general? This too is impossible to answer as there is no common scale between them.\n‘Standardized’ effect sizes are useful here as they provide common units. Instead of points on the self-report scale (i.e., sum scores), which differ between scales, standardized effect sizes generally use Standard Deviations as their units. For example, Cohen’s d = 0.2 means that there are 0.2 Standard Deviations of difference between the two groups.\nIn principle, standardized effect sizes are extremely useful as they allow us to draw comparisons between studies using very different outcome measures, or indeed to synthesise results between such studies (i.e., meta-analysis).\n\n10.3.1 Visualise\nSemi-realistic depression scores on different scales.\n\n\nCode\nN &lt;- 10000\n\ngenerated_data &lt;- \n  bind_rows(\n    tibble(measure = \"BDI-II\",\n           score = rsn(n = N, \n                       xi = 2,  # location\n                       omega = 15, # scale\n                       alpha = 16),\n           max_score = 63), # skew\n    tibble(measure = \"HAM-D\",\n           score = rsn(n = N, \n                       xi = 33,  # location\n                       omega = 7, # scale\n                       alpha = -1),\n           max_score = 52), # skew\n    tibble(measure = \"MADRS\",\n           score = rsn(n = N, \n                       xi = 7,  # location\n                       omega = 7, # scale\n                       alpha = 9),\n           max_score = 60) # skew\n  ) |&gt;\n  mutate(score = case_when(score &lt; 0 ~ 0,\n                           score &gt; max_score ~ max_score,\n                           TRUE ~ score))\n\n\nggplot(generated_data, aes(score)) +\n  geom_vline(aes(xintercept = 0), linetype = \"dotted\") +\n  geom_vline(aes(xintercept = max_score), linetype = \"dotted\") +\n  geom_histogram(boundary = 0) +\n  scale_x_continuous(breaks = breaks_pretty(n = 10)) +\n  facet_wrap(~ measure, ncol = 1, scales = \"free_y\") +\n  theme_linedraw() +\n  ylab(\"Frequency\") +\n  xlab(\"Sum score\")\n\n\n\n\n\n\n\n\n\nFor the moment, let’s pretend like these scales produce continuous normal data that only differ in their population location (\\(\\mu\\)) and scale (\\(\\sigma\\)):\n\n\nCode\ngenerated_data &lt;- \n  bind_rows(\n    tibble(measure = \"BDI-II\",\n           score = rnorm(n = N, mean = 7, sd = 9),\n           max_score = 63),\n    tibble(measure = \"HAM-D\",\n           score = rnorm(n = N, mean = 12, sd = 4),\n           max_score = 52),\n    tibble(measure = \"MADRS\",\n           score = rnorm(n = N, mean = 10, sd = 8),\n           max_score = 60)\n  ) \n\nggplot(generated_data, aes(score)) +\n  geom_histogram() +\n  scale_x_continuous(breaks = breaks_pretty(n = 10)) +\n  facet_wrap(~ measure, ncol = 1) +\n  theme_linedraw() +\n  ylab(\"Frequency\") +\n  xlab(\"Sum score\")\n\n\n\n\n\n\n\n\n\nA one-point change on the BDI-II still means something very different to a one-point change on the MADRS or HAM-D.\nData for a single sample can be standardized by taking each participant’s score, deducting the mean score (the sample estimate of \\(\\mu\\)), and then dividing by the SD of scores (the sample estimate of \\(\\sigma\\)). Now, all scales have a mean of 0 and an SD of 1. A one-point change on any scale has the same interpretation: a one-standard deviation change on that scale’s scores:\n\n\nCode\ngenerated_data &lt;- \n  bind_rows(\n    tibble(measure = \"BDI-II\",\n           score = rnorm(n = N, mean = 0, sd = 1),\n           max_score = 63),\n    tibble(measure = \"HAM-D\",\n           score = rnorm(n = N, mean = 0, sd = 1),\n           max_score = 52),\n    tibble(measure = \"MADRS\",\n           score = rnorm(n = N, mean = 0, sd = 1),\n           max_score = 60)\n  ) \n\nggplot(generated_data, aes(score)) +\n  geom_histogram() +\n  scale_x_continuous(breaks = breaks_pretty(n = 10)) +\n  facet_wrap(~ measure, ncol = 1) +\n  theme_linedraw() +\n  ylab(\"Frequency\") +\n  xlab(\"Standaridized scores\\n(score - mean)/SD\")\n\n\n\n\n\n\n\n\n\nYay, now we have scores that can be compared between scales, e.g., in a meta-analysis.\nHow can this go wrong?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Standardized effect sizes and range restriction</span>"
    ]
  },
  {
    "objectID": "chapters/10_standardized_effect_sizes_and_range_restriction.html#influence-of-preselection-on-cohens-d",
    "href": "chapters/10_standardized_effect_sizes_and_range_restriction.html#influence-of-preselection-on-cohens-d",
    "title": "10  Standardized effect sizes and range restriction",
    "section": "10.4 Influence of preselection on Cohen’s d",
    "text": "10.4 Influence of preselection on Cohen’s d\nNote that in the below, only data at pre is real BDI-II data. Data at post is modified data (i.e., offset by known amounts).\n\n10.4.1 Example 1\n\n10.4.1.1 Wrangle/simulate\n\n\nCode\nset.seed(42)\n\nsubset_no_preselection &lt;- data_bdi |&gt;\n  rename(control = bdi_score) |&gt;\n  # simulate a 'intervention' score that is 5 points lower than pre\n  mutate(intervention = control - 5) |&gt;\n  # sample 100 participants from the real data \n  slice_sample(n = 100) |&gt;\n  mutate(recruitment = \"General population\") |&gt;\n  # reshape\n  pivot_longer(cols = c(control, intervention),\n               names_to = \"condition\",\n               values_to = \"bdi_score\") |&gt;\n  mutate(condition = fct_relevel(condition, \"control\", \"intervention\"))\n\n\nsubset_preselection_for_severe &lt;- data_bdi |&gt;\n  rename(control = bdi_score) |&gt;\n  # simulate recruitment into the study requiring a score of 29 or more at pre (\"severe\" depression according to the BDI-II manual)\n  filter(control &gt;= 29) |&gt;\n  # simulate a 'intervention' score that is 5 points lower than pre\n  mutate(intervention = control - 5) |&gt;\n  # sample 100 participants from the real data \n  slice_sample(n = 100) |&gt;\n  mutate(recruitment = \"'Severe' depression\") |&gt;\n  # reshape\n  pivot_longer(cols = c(control, intervention),\n               names_to = \"condition\",\n               values_to = \"bdi_score\") |&gt;\n  mutate(condition = fct_relevel(condition, \"control\", \"intervention\"))\n\n\n\n\n10.4.1.2 Plot\n\n\nCode\nbind_rows(subset_no_preselection,\n          subset_preselection_for_severe) |&gt;\n  mutate(recruitment = fct_relevel(recruitment, \"General population\", \"'Severe' depression\")) |&gt;\n  ## plot\n  ggplot(aes(bdi_score)) +\n  geom_histogram(boundary = 0, bins = 21) +\n  scale_fill_viridis_d(begin = 0.3, end = 0.7) +\n  theme_linedraw() +\n  coord_cartesian(xlim = c(-5, 63)) +\n  facet_grid(condition ~ recruitment) +\n  xlab(\"BDI-II sum score\") +\n  ylab(\"Frequency\")\n\n\n\n\n\n\n\n\n\n\n\n10.4.1.3 Analyze\nExercise:\nFor each of the two datasets, please calculate:\n\nThe unstandardized difference in means between the groups. To do this, calculate the mean BDI-II score in each condition (control vs intervention) and then the difference between the two means.\n\nThe standardized mean difference (Cohen’s d) between the two groups (e.g., using effsize::cohen.d()).\n\nDoes the intervention work? Think about the simulated population effect.\n\n\nCode\n# datasets:\nsubset_no_preselection\n\n\n# A tibble: 200 × 4\n        id recruitment        condition    bdi_score\n     &lt;dbl&gt; &lt;chr&gt;              &lt;fct&gt;            &lt;dbl&gt;\n 1 2527472 General population control              2\n 2 2527472 General population intervention        -3\n 3 2516002 General population control              9\n 4 2516002 General population intervention         4\n 5 2553222 General population control             33\n 6 2553222 General population intervention        28\n 7 2551678 General population control             26\n 8 2551678 General population intervention        21\n 9 2555281 General population control             13\n10 2555281 General population intervention         8\n# ℹ 190 more rows\n\n\nCode\nsubset_preselection_for_severe\n\n\n# A tibble: 200 × 4\n        id recruitment         condition    bdi_score\n     &lt;dbl&gt; &lt;chr&gt;               &lt;fct&gt;            &lt;dbl&gt;\n 1 2519655 'Severe' depression control             41\n 2 2519655 'Severe' depression intervention        36\n 3 2551157 'Severe' depression control             32\n 4 2551157 'Severe' depression intervention        27\n 5 2514051 'Severe' depression control             40\n 6 2514051 'Severe' depression intervention        35\n 7 2513857 'Severe' depression control             42\n 8 2513857 'Severe' depression intervention        37\n 9 2559761 'Severe' depression control             38\n10 2559761 'Severe' depression intervention        33\n# ℹ 190 more rows\n\n\nSolution\n\n\nCode\nsubset_no_preselection |&gt;\n  group_by(condition) |&gt;\n  summarize(mean_bdi_score = mean(bdi_score)) |&gt;\n  pivot_wider(names_from = condition,\n              values_from = mean_bdi_score) |&gt;\n  mutate(mean_diff = intervention - control)\n\n\n# A tibble: 1 × 3\n  control intervention mean_diff\n    &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1    15.6         10.6        -5\n\n\nCode\nsubset_preselection_for_severe |&gt;\n  group_by(condition) |&gt;\n  summarize(mean_bdi_score = mean(bdi_score)) |&gt;\n  pivot_wider(names_from = condition,\n              values_from = mean_bdi_score) |&gt;\n  mutate(mean_diff = intervention - control)\n\n\n# A tibble: 1 × 3\n  control intervention mean_diff\n    &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1    37.0         32.0     -5.00\n\n\nCode\neffsize::cohen.d(formula = bdi_score ~ condition,\n                 data = subset_no_preselection)$estimate |&gt;\n  round_half_up(2)\n\n\n[1] 0.4\n\n\nCode\neffsize::cohen.d(formula = bdi_score ~ condition,\n                 data = subset_preselection_for_severe)$estimate |&gt;\n  round_half_up(2)\n\n\n[1] 0.71\n\n\nEquivalent change in means, different change in Cohen’s d\nWe know for a fact that the true difference in means is the same in both studies, because we create the data to be this way (i.e., scores at post are exactly pre - 5). The unstandardized effect sizes (pre-post difference in means) are the same, by definition.\nDespite this, the two studies produce the different Cohen’s d values. The standardized effect sizes are the different, despite exactly the same pre-post differences between the studies.\nIf the point of standardized effect sizes is to be able to compare them between studies on a common scale, and they don’t do this, what is their point?\n\n\n\n10.4.2 Example 2\nThe only difference here is a) the true difference in means and b) the seed.\n\n10.4.2.1 Wrangle/simulate\n\n\nCode\nset.seed(46)\n\nsubset_no_preselection &lt;- data_bdi |&gt;\n  rename(control = bdi_score) |&gt;\n  # simulate a 'intervention' score that is 5 points lower than pre\n  mutate(intervention = control - 5) |&gt;\n  # sample 100 participants from the real data \n  slice_sample(n = 100) |&gt;\n  mutate(recruitment = \"General population\") |&gt;\n  # reshape\n  pivot_longer(cols = c(control, intervention),\n               names_to = \"condition\",\n               values_to = \"bdi_score\") |&gt;\n  mutate(condition = fct_relevel(condition, \"control\", \"intervention\"))\n\n\nsubset_preselection_for_severe &lt;- data_bdi |&gt;\n  rename(control = bdi_score) |&gt;\n  # simulate recruitment into the study requiring a score of 29 or more at pre (\"severe\" depression according to the BDI-II manual)\n  filter(control &gt;= 29) |&gt;\n  # simulate a 'intervention' score that is 5 points lower than pre\n  mutate(intervention = control - 3) |&gt;\n  # sample 100 participants from the real data \n  slice_sample(n = 100) |&gt;\n  mutate(recruitment = \"'Severe' depression\") |&gt;\n  # reshape\n  pivot_longer(cols = c(control, intervention),\n               names_to = \"condition\",\n               values_to = \"bdi_score\") |&gt;\n  mutate(condition = fct_relevel(condition, \"control\", \"intervention\"))\n\n\n\n\n10.4.2.2 Plot\n\n\nCode\nbind_rows(subset_no_preselection,\n          subset_preselection_for_severe) |&gt;\n  mutate(recruitment = fct_relevel(recruitment, \"General population\", \"'Severe' depression\")) |&gt;\n  ## plot\n  ggplot(aes(bdi_score)) +\n  geom_histogram(boundary = 0, bins = 21) +\n  scale_fill_viridis_d(begin = 0.3, end = 0.7) +\n  theme_linedraw() +\n  coord_cartesian(xlim = c(-5, 63)) +\n  facet_grid(condition ~ recruitment) +\n  xlab(\"BDI-II sum score\") +\n  ylab(\"Frequency\")\n\n\n\n\n\n\n\n\n\n\n\n10.4.2.3 Analyze\nExercise:\nAgain, for each of the two datasets, please calculate:\n\nThis is the unstandaridzied difference in means between the groups. To do this, calculate the mean BDI-II score in each condition (control vs intervention) and then the difference between the two means.\n\nThe standardized mean difference (Cohen’s d) between the two groups (e.g., using effsize::cohen.d()).\n\nDoes the intervention work? Think about the simulated population effect.\n\n\nCode\n# datasets:\nsubset_no_preselection\n\n\n# A tibble: 200 × 4\n        id recruitment        condition    bdi_score\n     &lt;dbl&gt; &lt;chr&gt;              &lt;fct&gt;            &lt;dbl&gt;\n 1 2506327 General population control             42\n 2 2506327 General population intervention        37\n 3 2548703 General population control             11\n 4 2548703 General population intervention         6\n 5 2551646 General population control             17\n 6 2551646 General population intervention        12\n 7 2512827 General population control              6\n 8 2512827 General population intervention         1\n 9 2544327 General population control              0\n10 2544327 General population intervention        -5\n# ℹ 190 more rows\n\n\nCode\nsubset_preselection_for_severe\n\n\n# A tibble: 200 × 4\n        id recruitment         condition    bdi_score\n     &lt;dbl&gt; &lt;chr&gt;               &lt;fct&gt;            &lt;dbl&gt;\n 1 2518232 'Severe' depression control             38\n 2 2518232 'Severe' depression intervention        35\n 3 2512966 'Severe' depression control             29\n 4 2512966 'Severe' depression intervention        26\n 5 2550823 'Severe' depression control             37\n 6 2550823 'Severe' depression intervention        34\n 7 2519655 'Severe' depression control             41\n 8 2519655 'Severe' depression intervention        38\n 9 2543945 'Severe' depression control             31\n10 2543945 'Severe' depression intervention        28\n# ℹ 190 more rows\n\n\nSolution\n\n\nCode\nsubset_no_preselection |&gt;\n  group_by(condition) |&gt;\n  summarize(mean_bdi_score = mean(bdi_score)) |&gt;\n  pivot_wider(names_from = condition,\n              values_from = mean_bdi_score) |&gt;\n  mutate(mean_diff = intervention - control)\n\n\n# A tibble: 1 × 3\n  control intervention mean_diff\n    &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1    14.2         9.18        -5\n\n\nCode\nsubset_preselection_for_severe |&gt;\n  group_by(condition) |&gt;\n  summarize(mean_bdi_score = mean(bdi_score)) |&gt;\n  pivot_wider(names_from = condition,\n              values_from = mean_bdi_score) |&gt;\n  mutate(mean_diff = intervention - control)\n\n\n# A tibble: 1 × 3\n  control intervention mean_diff\n    &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1    36.2         33.2        -3\n\n\nCode\neffsize::cohen.d(formula = bdi_score ~ condition,\n                 data = subset_no_preselection)$estimate |&gt;\n  round_half_up(2)\n\n\n[1] 0.45\n\n\nCode\neffsize::cohen.d(formula = bdi_score ~ condition,\n                 data = subset_preselection_for_severe)$estimate |&gt;\n  round_half_up(2)\n\n\n[1] 0.45\n\n\nWe know for a fact that the true difference in means is different, because we create the data to be this way (i.e., pre-post difference is -5 in the no preselection study and -3 in the severe depression preselection study). The unstandardized effect sizes (pre-post difference in means) are different, by definition.\nDespite this, the two studies produce the same Cohen’s d value. The standardized effect sizes are the same, despite genuine differences in the pre-post changes between the two studies.\nIf the same standardized effect size estimate (Cohen’s d) can represent different real changes in means, how can a Cohen’s d of .2, for example, represent “small” effects? That is, if “small” effects on standardized effect sizes can represent unstandardized effect sizes of different sizes, how are standardized effect sizes ‘standardized’ at all?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Standardized effect sizes and range restriction</span>"
    ]
  },
  {
    "objectID": "chapters/10_standardized_effect_sizes_and_range_restriction.html#explanation",
    "href": "chapters/10_standardized_effect_sizes_and_range_restriction.html#explanation",
    "title": "10  Standardized effect sizes and range restriction",
    "section": "10.5 Explanation",
    "text": "10.5 Explanation\nThe above results - where the same unstandarized effect sizes have different standardized effect sizes, or vice-versa - are due to the fact that standardized effect sizes involve dividing, in one way or another, unstandardized effect sizes by standard deviations.\nE.g., for Cohen’s \\(d\\):\n\\(d = \\frac{M_{intervention} - M_{control}}{SD_{pooled}}\\)\nMost researchers are far more interested in the numerator than the denominator.\n\nResearchers often care about how much the means differ between the intervention and control groups. Differences in the means determine whether the intervention ‘worked’ or not.\nThey usually care very little about what the SD, except perhaps if they’re assessing statistical assumptions (homogeneity of variances).\n\nDespite this, the value of the SDs heavily influences the standardized effect size.\nIn the above examples, the range restriction in the ‘severe’ depression condition produces a narrower range of scores, and therefore smaller smaller SDs. Dividing the same difference in means by a smaller value of SD produces a different Cohen’s d estimate.\nRange restrictions like these are extremely common in psychology research, where studies can differ in their inclusion/exclusion strategies. This means makes it far harder to compare ‘standardized’ effect sizes between studies than you might think.\n\n10.5.1 Standardized effect sizes require estimating multiple parameters\nCohen’s d (usually) involves having to create a sample estimate of the means in each group. Researchers are usually more interested in differences between means.\nBut it also involves having to estimate the SDs. This can be a little a little confusing the first time you encounter it: we often intuitively think of SD as the amount of noise around the signal we’re interested in (the mean). We are somewhat more used to thinking about the fact that estimated means have error round them: the standard error of the mean (SEM) is used to calculate confidence intervals around means, and the SEM is actually just the SD of the mean (as opposed to normal SD, which is SD of the data).\nWe are relatively less familiar with thinking about the fact that estimates of standard deviation also are estimated with error, e.g., the standard error of the SD, which is the SD of the SD. Confused yet?\nWe can understand this more easily with a simulation. We generate data for a single sample with a population mean (\\(\\mu\\)) = 0 and population SD (\\(\\sigma\\)) = 1.\nAcross lots of iterations, we can see that the average sample mean is close to the population mean (\\(\\mu\\)), and the average sample SD is close to the population (\\(\\sigma\\)):\n\n\nCode\n# set the seed ----\n# for the pseudo random number generator to make results reproducible\nset.seed(123)\n\n\n# define data generating function ----\ngenerate_data &lt;- function(n,\n                          mean,\n                          sd) {\n  \n  data &lt;- tibble(score = rnorm(n = n, mean = mean, sd = sd))\n  \n  return(data)\n}\n\n\n# define data analysis function ----\nanalyse_data &lt;- function(data) {\n  \n  res &lt;- data |&gt;\n    summarize(sample_mean = mean(score),\n              sample_sd = sd(score))\n  \n  return(res)\n}\n\n\n# define experiment parameters ----\nexperiment_parameters_grid &lt;- expand_grid(\n  n = c(50, 100, 150),\n  mean = 0,\n  sd = 1,\n  iteration = 1:1000\n)\n\n\n# run simulation ----\nsimulation &lt;- \n  # using the experiment parameters\n  experiment_parameters_grid |&gt;\n  \n  # generate data using the data generating function and the parameters relevant to data generation\n  mutate(generated_data = pmap(list(n,\n                                    mean,\n                                    sd),\n                               generate_data)) |&gt;\n  \n  # apply the analysis function to the generated data using the parameters relevant to analysis\n  mutate(analysis_results = pmap(list(generated_data),\n                                 analyse_data))\n  \n\n# summarise simulation results over the iterations ----\nsimulation_summary &lt;- simulation |&gt;\n  unnest(analysis_results) \n\nsimulation_summary |&gt;\n  group_by(n) |&gt;\n  summarize(average_sample_means = mean(sample_mean),\n            average_sample_sds = mean(sample_sd)) |&gt;\n  mutate_if(is.numeric, janitor::round_half_up, digits = 2) |&gt;\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\nn\naverage_sample_means\naverage_sample_sds\n\n\n\n\n50\n0\n1\n\n\n100\n0\n1\n\n\n150\n0\n1\n\n\n\n\n\nBut the estimated means in individual samples (i.e., individual iterations) vary around this true value (\\(\\mu\\) = 0). The smaller the sample size, the more deviation there is from the population value:\n\n\nCode\nsimulation_summary |&gt;\n  mutate(n_string = paste(\"N =\", n),\n         n_string = fct_relevel(n_string, \"N = 50\", \"N = 100\", \"N = 150\")) |&gt;\n  ggplot(aes(sample_mean)) +\n  geom_histogram(boundary = 0) +\n  theme_linedraw() +\n  ylab(\"Frequency\") +\n  xlab(\"Means found in different samples\\n(where population mu = 0)\") +\n  facet_wrap(~ n_string)\n\n\n\n\n\n\n\n\n\nThe same applies to the estimated SDs in individual samples (i.e., individual iterations), which also vary around this true value (\\(\\sigma\\) = 1). The smaller the sample size, the more deviation there is from the population value:\n\n\nCode\nsimulation_summary |&gt;\n  mutate(n_string = paste(\"N =\", n),\n         n_string = fct_relevel(n_string, \"N = 50\", \"N = 100\", \"N = 150\")) |&gt;\n  ggplot(aes(sample_sd)) +\n  geom_histogram(boundary = 0) +\n  theme_linedraw() +\n  ylab(\"Frequency\") +\n  xlab(\"SDs found in different samples\\n(where population sigma = 1)\") +\n  facet_wrap(~ n_string)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Standardized effect sizes and range restriction</span>"
    ]
  },
  {
    "objectID": "chapters/10_standardized_effect_sizes_and_range_restriction.html#solutions-to-this-problem",
    "href": "chapters/10_standardized_effect_sizes_and_range_restriction.html#solutions-to-this-problem",
    "title": "10  Standardized effect sizes and range restriction",
    "section": "10.6 Solutions to this problem",
    "text": "10.6 Solutions to this problem\nThere are solutions to this, to make “standardized” effect sizes actually standard between studies. But almost no one does them.\n\nThe when calculating standardized effect sizes, use a well established population norm estimate of the measure’s SD rather than the sample SD. E.g., always set the BDI’s SD to 12 (or whatever your best estimate is). Note that no implementations of Cohen’s d in commonly used R packages recommend this, and only a few can directly handle it (e.g., {esci}).\nUse math/R packages to correct your standardised effect size estimate for range restriction (see Wiernik & Dahlke, 2020, doi: 10.1177/2515245919885611).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Standardized effect sizes and range restriction</span>"
    ]
  },
  {
    "objectID": "chapters/10_standardized_effect_sizes_and_range_restriction.html#is-this-issue-limited-to-cohens-d",
    "href": "chapters/10_standardized_effect_sizes_and_range_restriction.html#is-this-issue-limited-to-cohens-d",
    "title": "10  Standardized effect sizes and range restriction",
    "section": "10.7 Is this issue limited to Cohen’s d?",
    "text": "10.7 Is this issue limited to Cohen’s d?\nNo, it affects other forms of standardized effect sizes too, including correlations.\nE.g., there is a perennial debate in the US about whether standardized university entrance tests like the SAT are useful or not, or indeed are biased or not (e.g., between gender and race/ethnicity), because straightforward analyses suggest that SAT scores (used to get a place at university) are poorly predictive of grades at university.\nHowever, this poor predictive validity may be due in part to range restriction: because the SAT scores are used to determine who goes to university, data on university grades is only obtained from those individuals who already scored highly on the SAT. That is, there is a fairly narrow range of SAT scores among university students. Correlations, like Cohen’s d, include SD in their denominator (i.e., \\(r = covariance_{xy}/(SD_x*SD_y)\\)), and therefore range restriction also distorts correlations.\nIt is therefore possible - indeed, likely - that SAT scores are usefully predictive of grades at university. The below short simulation demonstrates attentuation in correlations due to range constraint.\n\n\nCode\n# Set seed for reproducibility\nset.seed(42)\n\n# Parameters\nn &lt;- 10000  # number of observations\nrho &lt;- 0.6  # correlation between x and y\n\n# Generate correlated data using the faux package\nsimulated_data &lt;- rnorm_multi(n = n, \n                              mu = c(0, 0), \n                              sd = c(1, 1), \n                              r = matrix(c(1, rho, \n                                           rho, 1), nrow = 2),\n                              varnames = c(\"x\", \"y\"))\n\n# Calculate correlation in full data\nfull_correlation &lt;- cor(simulated_data$x, simulated_data$y)\ncat(\"Correlation in full data:\", janitor::round_half_up(full_correlation, digits = 2), \"\\n\")\n\n\nCorrelation in full data: 0.6 \n\n\nCode\n# Introduce range restriction (e.g., keep only x &gt; -0.5 and x &lt; 0.5)\nsimulated_data_range_restricted &lt;- simulated_data |&gt;\n  filter(x &gt; qnorm(0.75)) # top 25% of a normal population corresponds to SD &gt; qnorm(0.75), ie 0.6744898\n\n# Calculate correlation in restricted data\nrestricted_correlation &lt;- cor(simulated_data_range_restricted$x, simulated_data_range_restricted$y)\ncat(\"Correlation in restricted data:\", janitor::round_half_up(restricted_correlation, digits = 2), \"\\n\")\n\n\nCorrelation in restricted data: 0.35 \n\n\nCode\n# Plot full data with correlation annotation\nggplot(simulated_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.4) +\n  #geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  ggtitle(\"Correlation in Full Data\") +\n  theme_linedraw() +\n  annotate(\"text\", x = -2, y = 2, label = paste(\"r =\", round(full_correlation, 2)), \n           hjust = 0.5, vjust = 0.5, size = 6, color = \"blue\") +\n  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3))\n\n\n\n\n\n\n\n\n\nCode\n# Plot restricted data with correlation annotation\nggplot(simulated_data_range_restricted, aes(x = x, y = y)) +\n  geom_point(alpha = 0.4) +\n  #geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  ggtitle(\"Correlation in Range Restricted Data\") +\n  theme_linedraw() +\n  annotate(\"text\", x = -2, y = 2, label = paste(\"r =\", round(restricted_correlation, 2)), \n           hjust = 0.5, vjust = 0.5, size = 6, color = \"red\") +\n  coord_cartesian(xlim = c(-3, 3), ylim = c(-3, 3))\n\n\n\n\n\n\n\n\n\nNote that the observed correlations which have been distorted due to range restriction can be ‘de-attentuated’ or corrected if normative data is available to know what the unrestricted range looks like. However, this is very rarely done in studies and meta-analyses.\n\n\nCode\n# Calculate the variance ratios as an estimate of the range restriction factor\nvariance_ratio &lt;- var(simulated_data_range_restricted$x) / var(simulated_data$x)\n\n# Deattenuate the observed correlation\ncorrected_correlation &lt;- restricted_correlation / sqrt(variance_ratio)\n\n# Output results\ncat(\"Observed Correlation (Restricted):\", janitor::round_half_up(restricted_correlation, 2), \"\\n\")\n\n\nObserved Correlation (Restricted): 0.35 \n\n\nCode\ncat(\"Variance Ratio (Range Restriction Factor):\", janitor::round_half_up(variance_ratio, 2), \"\\n\")\n\n\nVariance Ratio (Range Restriction Factor): 0.25 \n\n\nCode\ncat(\"Corrected Correlation (Deattenuated):\", janitor::round_half_up(corrected_correlation, 2), \"\\n\")\n\n\nCorrected Correlation (Deattenuated): 0.69 \n\n\nNote that the corrected correlation is much closer to the original one.\n\n\nCode\nsessionInfo()\n\n\nR version 4.5.0 (2025-04-11)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.5\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Zurich\ntzcode source: internal\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] faux_1.2.2       kableExtra_1.4.0 knitr_1.50       effsize_0.8.1   \n [5] janitor_2.2.1    sn_2.1.1         scales_1.4.0     lubridate_1.9.4 \n [9] forcats_1.0.0    stringr_1.5.1    dplyr_1.1.4      purrr_1.1.0     \n[13] readr_2.1.5      tidyr_1.3.1      tibble_3.3.0     ggplot2_3.5.2   \n[17] tidyverse_2.0.0 \n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.6          generics_0.1.4      xml2_1.3.8         \n [4] stringi_1.8.7       hms_1.1.3           digest_0.6.37      \n [7] magrittr_2.0.3      evaluate_1.0.3      grid_4.5.0         \n[10] timechange_0.3.0    RColorBrewer_1.1-3  fastmap_1.2.0      \n[13] jsonlite_2.0.0      viridisLite_0.4.2   numDeriv_2016.8-1.1\n[16] textshaping_1.0.1   mnormt_2.1.1        cli_3.6.5          \n[19] crayon_1.5.3        rlang_1.1.6         bit64_4.6.0-1      \n[22] withr_3.0.2         parallel_4.5.0      tools_4.5.0        \n[25] tzdb_0.5.0          vctrs_0.6.5         R6_2.6.1           \n[28] lifecycle_1.0.4     snakecase_0.11.1    bit_4.6.0          \n[31] htmlwidgets_1.6.4   vroom_1.6.5         pkgconfig_2.0.3    \n[34] pillar_1.11.0       gtable_0.3.6        glue_1.8.0         \n[37] systemfonts_1.2.3   xfun_0.52           tidyselect_1.2.1   \n[40] rstudioapi_0.17.1   farver_2.1.2        htmltools_0.5.8.1  \n[43] labeling_0.4.3      rmarkdown_2.29      svglite_2.2.1      \n[46] compiler_4.5.0",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Standardized effect sizes and range restriction</span>"
    ]
  },
  {
    "objectID": "chapters/11_impact_of_lazy_responding_assignment.html",
    "href": "chapters/11_impact_of_lazy_responding_assignment.html",
    "title": "11  The impact of one form of lazy/careless responding on the power and false positive rate of a Student’s t-test",
    "section": "",
    "text": "11.1 Assignment\nBackground/Rationale of the exercise:\nMany surveys that use a Likert scale or a slider have a default response. E.g., when you load the page all answers already have a default answer of “0” on a -3 to +3 scale. Careless or lazy responding is common. Some participants simply leave the default answers and click “next” in the survey. Many researchers don’t use attention checks in their surveys, or don’t use good ones, and these careless or lazy responses are not excluded. This stimulation seeks to quantify the impact of this type of responding on the results. Please note there any many other forms of careless responding - this is just one example and doesn’t provide a full answer to this question.\nExercise:\nWrite R code from scratch, but using our established workflow, that does the following:",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The impact of one form of lazy/careless responding on the power and false positive rate of a Student's t-test</span>"
    ]
  },
  {
    "objectID": "chapters/11_impact_of_lazy_responding_assignment.html#assignment",
    "href": "chapters/11_impact_of_lazy_responding_assignment.html#assignment",
    "title": "11  The impact of one form of lazy/careless responding on the power and false positive rate of a Student’s t-test",
    "section": "",
    "text": "Data generation function\n\nSimulate two independent groups, control and intervention, drawn from a normal distribution. The mean and SD of both conditions should be variables.\n\nCorrupt data function\n\nYou can use the corrupt data function I provide you with below. This replaces a proportion of the whole dataset’s ‘score’ column with a default value (in this case zero). You should use the usual mutate() and pmap() workflow to create a new column, corrupted_data, from an existing column named generated_data.\n\nAnalyze data function\n\nFit a Student’s t-test and extract the p value in a tidy tibble.\n\nAn expand grid call using:\n\nn per condition = 100\nmean = 0 for the control group\nmean = 0 or 0.50 for the intervention group (two scenarios, population effect exists or does not)\nSD = 1\nproportion of straight line responders = 0 or 0.1\n1000 iterations\nusing set.seed(42)\n\nSummarize across iterations\n\nSummarise the proportion of significant p values in all simulated conditions in a table or plot\nProvide a description and interpretation of the results: How does this form of straight line responding affect the false positive rate? How does it affect power? (briefly, in two or three sentences)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The impact of one form of lazy/careless responding on the power and false positive rate of a Student's t-test</span>"
    ]
  },
  {
    "objectID": "chapters/11_impact_of_lazy_responding_assignment.html#dependencies",
    "href": "chapters/11_impact_of_lazy_responding_assignment.html#dependencies",
    "title": "11  The impact of one form of lazy/careless responding on the power and false positive rate of a Student’s t-test",
    "section": "11.2 Dependencies",
    "text": "11.2 Dependencies\n\n\nCode\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(sn)\nlibrary(janitor)\nlibrary(effsize)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(faux)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The impact of one form of lazy/careless responding on the power and false positive rate of a Student's t-test</span>"
    ]
  },
  {
    "objectID": "chapters/11_impact_of_lazy_responding_assignment.html#generate-data-function",
    "href": "chapters/11_impact_of_lazy_responding_assignment.html#generate-data-function",
    "title": "11  The impact of one form of lazy/careless responding on the power and false positive rate of a Student’s t-test",
    "section": "11.3 Generate data function",
    "text": "11.3 Generate data function",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The impact of one form of lazy/careless responding on the power and false positive rate of a Student's t-test</span>"
    ]
  },
  {
    "objectID": "chapters/11_impact_of_lazy_responding_assignment.html#contaminate-data-function",
    "href": "chapters/11_impact_of_lazy_responding_assignment.html#contaminate-data-function",
    "title": "11  The impact of one form of lazy/careless responding on the power and false positive rate of a Student’s t-test",
    "section": "11.4 Contaminate data function",
    "text": "11.4 Contaminate data function\n\n\nCode\ncontaminate_data &lt;- function(data, proportion_straightline_responder, value = 0) {\n  data %&gt;% \n    mutate(is_straightline_responder = runif(n()) &lt; proportion_straightline_responder,    # Bernoulli(proportion)\n           score       = if_else(is_straightline_responder, value, score)) %&gt;% \n    ungroup()\n}",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The impact of one form of lazy/careless responding on the power and false positive rate of a Student's t-test</span>"
    ]
  },
  {
    "objectID": "chapters/11_impact_of_lazy_responding_assignment.html#analyze-data-function",
    "href": "chapters/11_impact_of_lazy_responding_assignment.html#analyze-data-function",
    "title": "11  The impact of one form of lazy/careless responding on the power and false positive rate of a Student’s t-test",
    "section": "11.5 Analyze data function",
    "text": "11.5 Analyze data function",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The impact of one form of lazy/careless responding on the power and false positive rate of a Student's t-test</span>"
    ]
  },
  {
    "objectID": "chapters/11_impact_of_lazy_responding_assignment.html#simulation-parameters",
    "href": "chapters/11_impact_of_lazy_responding_assignment.html#simulation-parameters",
    "title": "11  The impact of one form of lazy/careless responding on the power and false positive rate of a Student’s t-test",
    "section": "11.6 Simulation parameters",
    "text": "11.6 Simulation parameters",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The impact of one form of lazy/careless responding on the power and false positive rate of a Student's t-test</span>"
    ]
  },
  {
    "objectID": "chapters/11_impact_of_lazy_responding_assignment.html#run-simulation",
    "href": "chapters/11_impact_of_lazy_responding_assignment.html#run-simulation",
    "title": "11  The impact of one form of lazy/careless responding on the power and false positive rate of a Student’s t-test",
    "section": "11.7 Run simulation",
    "text": "11.7 Run simulation\n\n\nCode\nset.seed(42)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The impact of one form of lazy/careless responding on the power and false positive rate of a Student's t-test</span>"
    ]
  },
  {
    "objectID": "chapters/11_impact_of_lazy_responding_assignment.html#summarize-results-across-iterations",
    "href": "chapters/11_impact_of_lazy_responding_assignment.html#summarize-results-across-iterations",
    "title": "11  The impact of one form of lazy/careless responding on the power and false positive rate of a Student’s t-test",
    "section": "11.8 Summarize results across iterations",
    "text": "11.8 Summarize results across iterations\n[written description and interpretation of results here]\nRemember that this is just one narrow simulation of the impact of one type of lazy/careless responding on one type of analysis - other forms and other analyses can be affected very differently.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The impact of one form of lazy/careless responding on the power and false positive rate of a Student's t-test</span>"
    ]
  },
  {
    "objectID": "chapters/12_meta_analysis_and_publication_bias.html",
    "href": "chapters/12_meta_analysis_and_publication_bias.html",
    "title": "12  Meta-analysis and publication bias",
    "section": "",
    "text": "12.1 Dependencies\nCode\n# dependencies ----\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(readr)\nlibrary(purrr)\nlibrary(furrr)\nlibrary(ggplot2)\nlibrary(effsize)\nlibrary(janitor)\nlibrary(tibble)\n#library(sn)\nlibrary(metafor)\nlibrary(parameters)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(pwr)\nlibrary(ggstance)\n\n# set the seed ----\n# for the pseudo random number generator to make results reproducible\nset.seed(123)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/12_meta_analysis_and_publication_bias.html#what-is-a-meta-analysis",
    "href": "chapters/12_meta_analysis_and_publication_bias.html#what-is-a-meta-analysis",
    "title": "12  Meta-analysis and publication bias",
    "section": "12.2 What is a meta-analysis?",
    "text": "12.2 What is a meta-analysis?\nLet’s start with some imagined summary statistics, and convert them to Cohen’s d effect sizes.\nyi refers to the effect size, and vi refers to its variance (note that Standard Error = sqrt(variance)).\n\n\nCode\nmean_intervention &lt;- c(0.68, 0.97, 0.40, 0.48, 0.56, 0.10, -0.10, 0.03)\nmean_control      &lt;- c(   0,    0,    0,    0,    0,    0,     0,    0)\nsd_intervention   &lt;- c(   1,    1,    1,    1,    1,    1,     1,    1)\nsd_control        &lt;- c(   1,    1,    1,    1,    1,    1,     1,    1)\nn_intervention    &lt;- c(  20,   10,  100,   37,   50,  450,    50, 1000)\nn_control         &lt;- c(  20,   10,  100,   37,   50,  450,    50, 1000)\n\nes &lt;- escalc(measure = \"SMD\", \n             m1i  = mean_intervention, \n             m2i  = mean_control, \n             sd1i = sd_intervention,\n             sd2i = sd_control,\n             n1i  = n_intervention,\n             n2i  = n_control)\n\nes |&gt;\n  as_tibble() |&gt;\n  mutate_all(round_half_up, digits = 2) |&gt;\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\nyi\nvi\n\n\n\n\n0.67\n0.11\n\n\n0.93\n0.22\n\n\n0.40\n0.02\n\n\n0.47\n0.06\n\n\n0.56\n0.04\n\n\n0.10\n0.00\n\n\n-0.10\n0.04\n\n\n0.03\n0.00\n\n\n\n\n\n\n12.2.1 Fixed-effects meta-analysis\nAka Common-Effects or Equal-Effects.\nThe simplest form of meta-analysis - although it would not be acceptable to use anywhere these days - is simply the mean effect size.\n\n\nCode\nes |&gt;\n  summarize(mean_effect_size = round_half_up(mean(yi), digits = 2)) |&gt;\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\nmean_effect_size\n\n\n\n\n0.38\n\n\n\n\n\nNote that calculating the mean is equivalent to fitting an intercept only fixed-effects model (ie linear regression) with the effect sizes as the DV. The estimate of the intercept is equivalent to the mean effect size.\n\n\nCode\nlm(yi ~ 1,\n   data = es) |&gt;\n  model_parameters() \n\n\nParameter   | Coefficient |   SE |       95% CI | t(7) |     p\n--------------------------------------------------------------\n(Intercept) |        0.38 | 0.12 | [0.09, 0.67] | 3.09 | 0.018\n\n\nWhy is this not acceptable? Because it ignores the error associated with each effect size. A very simple meta-analysis might use weighted-mean effect sizes and weight them by the total sample size of each study:\n\n\nCode\nweighted.mean(x = es$yi, w = n_intervention + n_control) |&gt;\n  round_half_up(digits = 2) \n\n\n[1] 0.1\n\n\nNote that the weighted mean effect size is equivalent to an intercept only fixed-effects model (linear regression) with the effect sizes as the DV and the sample sizes as weights. The estimate of the intercept is equivalent to the weighted mean effect size.\n\n\nCode\nlm(yi ~ 1,\n   weights = n_intervention + n_control,\n   data = es) |&gt;\n  model_parameters()\n\n\nParameter   | Coefficient |   SE |        95% CI | t(7) |     p\n---------------------------------------------------------------\n(Intercept) |        0.10 | 0.06 | [-0.04, 0.25] | 1.70 | 0.133\n\n\nQuite early in the development of meta-analysis methods, people started to weight not by N but by inverse variance of the effect size, on the basis that things other than N can affect the precision of estimation of the effect size. This can be implemented as follows:\n\n\nCode\nlm(yi ~ 1,\n   weights = 1/vi,\n   data = es) |&gt;\n  model_parameters()\n\n\nParameter   | Coefficient |   SE |        95% CI | t(7) |     p\n---------------------------------------------------------------\n(Intercept) |        0.10 | 0.06 | [-0.04, 0.24] | 1.70 | 0.133\n\n\nThe above linear regression produces comparable results as when you fit a ‘proper’ fixed-effects meta-analysis using the {metafor} package. There are some small differences in the effect size and its 95% CIs that aren’t important to understand here. The ‘Overall’ row reports the meta-analysis results.\n\n\nCode\nrma(yi = yi, \n    vi = vi, \n    method = \"FE\", # fixed effect model\n    data = es) |&gt;\n  model_parameters()\n\n\nMeta-analysis using 'metafor'\n\nParameter | Coefficient |   SE |        95% CI |     z |     p | Weight\n-----------------------------------------------------------------------\nStudy 1   |        0.67 | 0.32 | [ 0.03, 1.30] |  2.05 | 0.040 |   9.47\nStudy 2   |        0.93 | 0.47 | [ 0.01, 1.85] |  1.97 | 0.048 |   4.51\nStudy 3   |        0.40 | 0.14 | [ 0.12, 0.68] |  2.79 | 0.005 |  49.03\nStudy 4   |        0.47 | 0.24 | [ 0.01, 0.94] |  2.01 | 0.044 |  17.99\nStudy 5   |        0.56 | 0.20 | [ 0.16, 0.96] |  2.73 | 0.006 |  24.07\nStudy 6   |        0.10 | 0.07 | [-0.03, 0.23] |  1.50 | 0.134 | 224.72\nStudy 7   |       -0.10 | 0.20 | [-0.49, 0.29] | -0.50 | 0.620 |  24.97\nStudy 8   |        0.03 | 0.04 | [-0.06, 0.12] |  0.67 | 0.503 | 499.94\nOverall   |        0.10 | 0.03 | [ 0.03, 0.17] |  2.97 | 0.003 |       \n\n\n\n\n12.2.2 Random-effects meta-analysis\nThere is a debate about whether Fixed-Effects vs. Random-Effects models should be employed in meta-analysis. Most recommendations come down on the side of Random-Effects, sometimes people recommend reporting the results of both. Briefly: FE models have less plausible assumptions about the differences between studies, but RE models suffer from putting less weight on the sample sizes of individual large studies.\nWithout getting into Random-Effects models conceptually, its useful to know that Random-Effects meta-analyses can also easily be fitted in {metafor}, and indeed are the default.\n\n\nCode\nfit &lt;- \n  rma(yi = yi, \n      vi = vi, \n      method = \"REML\", # default random effects model\n      data = es)\n\nmodel_parameters(fit)\n\n\nMeta-analysis using 'metafor'\n\nParameter | Coefficient |   SE |        95% CI |     z |     p | Weight\n-----------------------------------------------------------------------\nStudy 1   |        0.67 | 0.32 | [ 0.03, 1.30] |  2.05 | 0.040 |   9.47\nStudy 2   |        0.93 | 0.47 | [ 0.01, 1.85] |  1.97 | 0.048 |   4.51\nStudy 3   |        0.40 | 0.14 | [ 0.12, 0.68] |  2.79 | 0.005 |  49.03\nStudy 4   |        0.47 | 0.24 | [ 0.01, 0.94] |  2.01 | 0.044 |  17.99\nStudy 5   |        0.56 | 0.20 | [ 0.16, 0.96] |  2.73 | 0.006 |  24.07\nStudy 6   |        0.10 | 0.07 | [-0.03, 0.23] |  1.50 | 0.134 | 224.72\nStudy 7   |       -0.10 | 0.20 | [-0.49, 0.29] | -0.50 | 0.620 |  24.97\nStudy 8   |        0.03 | 0.04 | [-0.06, 0.12] |  0.67 | 0.503 | 499.94\nOverall   |        0.27 | 0.10 | [ 0.07, 0.47] |  2.64 | 0.008 |       \n\n\nMany meta-analyses also report forest plots, which list the studies, plot the individual and meta-analysis effect sizes and their 95% CIs, and report them numerically too for precision.\n\n\nCode\nforest(fit, header = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/12_meta_analysis_and_publication_bias.html#why-you-cant-just-count-the-proportion-of-significant-p-values",
    "href": "chapters/12_meta_analysis_and_publication_bias.html#why-you-cant-just-count-the-proportion-of-significant-p-values",
    "title": "12  Meta-analysis and publication bias",
    "section": "12.3 Why you can’t just count the proportion of significant p values",
    "text": "12.3 Why you can’t just count the proportion of significant p values\nStudies have different power, so each tests the hypothesis with a different probability of detecting the effect assuming its true. Mixed results can therefore be found even when all have studied the same true hypothesis, even if they happened to all find exactly the same effect size.\n\nCounting p values: Only 2/8 studies found significant results [reject H1]\nObserved effect sizes: All studies found Cohen’s = 0.2 [accept H1]\nMeta-analysis: Cohen’s d = 0.20, 95% CI [0.13, 0.27] [accept H1]\n\n\n\nCode\nes &lt;- escalc(measure = \"SMD\", \n             m1i  = c( 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,  0.2), \n             m2i  = c(   0,   0,   0,   0,   0,   0,   0,    0), \n             sd1i = c(   1,   1,   1,   1,   1,   1,   1,    1),\n             sd2i = c(   1,   1,   1,   1,   1,   1,   1,    1),\n             n1i  = c(  20,  10, 100,  37,  50, 450,  50, 1000),\n             n2i  = c(  20,  10, 100,  37,  50, 450,  50, 1000))\n\nrma(yi     = yi, \n    vi     = vi, \n    data   = es,\n    method = \"REML\") |&gt;\n  forest(header = TRUE)\n\n\n\n\n\n\n\n\n\nEqually, the population effect might be zero, but some studies might still detect effects. Why might this happen?\n\n\nCode\nes &lt;- escalc(measure = \"SMD\", \n             m1i  = c( 0.18, 0.43, -0.30, -0.08, 0.06, 0.10, -0.10, 0.03), \n             m2i  = c(    0,    0,     0,     0,    0,    0,     0,    0), \n             sd1i = c(    1,    1,     1,     1,    1,    1,     1,    1),\n             sd2i = c(    1,    1,     1,     1,    1,    1,     1,    1),\n             n1i  = c(   70,   50,   100,    37,   50,  350,    50,  400),\n             n2i  = c(   70,   50,   100,    37,   50,  350,    50,  400))\n\nrma(yi     = yi, \n    vi     = vi, \n    data   = es,\n    method = \"REML\") |&gt;\n  forest(header = TRUE)\n\n\n\n\n\n\n\n\n\nOf course, the majority of studies might produce significant results and the meta-analysis also produce a significant effect, and we might still have doubts about whether the effect really exists or not. Why might this be?\n\n\nCode\nes &lt;- escalc(measure = \"SMD\", \n             m1i  = c( 0.68, 0.97, 0.40, 0.48, 0.56, 0.10, -0.10, 0.03), \n             m2i  = c(    0,    0,    0,    0,    0,    0,     0,    0), \n             sd1i = c(    1,    1,    1,    1,    1,    1,     1,    1),\n             sd2i = c(    1,    1,    1,    1,    1,    1,     1,    1),\n             n1i  = c(   20,   10,  100,   37,   50,  450,    50, 1000),\n             n2i  = c(   20,   10,  100,   37,   50,  450,    50, 1000))\n\nres &lt;- \n  rma(yi     = yi, \n      vi     = vi, \n      data   = es,\n      method = \"REML\") \n\nforest(res, header = TRUE)\n\n\n\n\n\n\n\n\n\nCode\nfunnel(res)\n\n\n\n\n\n\n\n\n\nCode\n# funnel(res, level = c(90, 95, 99), refline = 0, legend = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/12_meta_analysis_and_publication_bias.html#why-we-cant-have-nice-things",
    "href": "chapters/12_meta_analysis_and_publication_bias.html#why-we-cant-have-nice-things",
    "title": "12  Meta-analysis and publication bias",
    "section": "12.4 Why we can’t have nice things",
    "text": "12.4 Why we can’t have nice things\n\n12.4.1 Confusing SD and SE when extracting summary statistics\nEven articles published in the most prestigious journals and on topics that will impact patient care are highly susceptible to this, e.g., Metaxa & Clarke (2024) “Efficacy of psilocybin for treating symptoms of depression: systematic review and meta-analysis” was highlighted as doing this. It’s not even down to unclear labelling in the orignal study: even when they state “numerical data show means (SEM)”, as in this case, its often extracted as the SD.\nBecause SEs are MUCH smaller than SDs, incorrectly using SE causes Cohen’s d to be inflated - usually by a lot.\nThe below demonstrates this. The same summary statistics are used as in previous examples above. Effect sizes, their 95% CIs, and their SEM are then calculated. For two of the studies, the SDs are replaced with the SEs. The meta-analysis then shows this distortion on the individual effect sizes and the meta-analytic effect size.\n\n\nCode\n# summary stats\nmean_intervention &lt;- c(0.68, 0.97, 0.40, 0.48, 0.56, 0.10, -0.10, 0.03)\nmean_control      &lt;- c(   0,    0,    0,    0,    0,    0,     0,    0)\nsd_intervention   &lt;- c(   1,    1,    1,    1,    1,    1,     1,    1)\nsd_control        &lt;- c(   1,    1,    1,    1,    1,    1,     1,    1)\nn_intervention    &lt;- c(  20,   10,  100,   37,   50,  450,    50, 1000)\nn_control         &lt;- c(  20,   10,  100,   37,   50,  450,    50, 1000)\n\ndat &lt;- \n  tibble(m1i  = mean_intervention, \n         m2i  = mean_control, \n         sd1i = sd_intervention,\n         sd2i = sd_control,\n         n1i  = n_intervention,\n         n2i  = n_control) |&gt;\n  rownames_to_column(var = \"study\") |&gt;\n  # calculate SEs\n  mutate(se1i = sd1i/sqrt(n1i),\n         se2i = sd2i/sqrt(n2i)) |&gt;\n  # replace SDs with SEs for two studies, studies 4 and 6\n  mutate(sd1i_error = ifelse(study %in% c(\"4\", \"6\"), se1i, sd1i),\n         sd2i_error = ifelse(study %in% c(\"4\", \"6\"), se2i, sd2i))\n\n# calculate effect sizes properly\nes_without_errors &lt;- \n  escalc(measure = \"SMD\", \n         m1i  = dat$m1i,\n         m2i  = dat$m2i,\n         sd1i = dat$sd1i,\n         sd2i = dat$sd2i,\n         n1i  = dat$n1i,\n         n2i  = dat$n2i) \n\n# calculate effect sizes with SE/SD errors\nes_with_errors &lt;- \n  escalc(measure = \"SMD\", \n         m1i  = dat$m1i,\n         m2i  = dat$m2i,\n         sd1i = dat$sd1i_error,\n         sd2i = dat$sd2i_error,\n         n1i  = dat$n1i,\n         n2i  = dat$n2i) \n\n# meta-analyze the correctly calculated effect sizes\nfit_correct &lt;- \n  rma(yi = yi, \n      vi = vi, \n      method = \"REML\",\n      data = es_without_errors)\n\nforest(fit_correct, \n       header = \"Correctly calculated effect sizes\")\n\n\n\n\n\n\n\n\n\nCode\n# meta-analyze the erroneously calculated effect sizes\nfit_errors &lt;- \n  rma(yi = yi, \n      vi = vi, \n      method = \"REML\",\n      data = es_with_errors) \n\nforest(fit_errors,\n       header = \"Erroneous effect sizes: SE used as SD for two studies\")\n\n\n\n\n\n\n\n\n\nMaking this error for 2 of 8 studies here inflates the effect sizes for those studies to be extremely and implausibly large - Cohen’s d &gt; 2. This also greatly increases the meta-analysis effect size.\n\nNote that this could be simulated more extensively as an end-of-course assignment. I.e., assuming different true effect sizes and prevalences of misinterpreting SE as SD, what is the proportionate distortion of of meta-effect sizes in the literature? For a given true effect size, what is the probability of observing a true effect size of X in a component study relative to it being a coding error? (e.g., if true effect size is 0.2, what proportion of observed effect sizes of 1.5 are erroneous?)\n\n\n\n12.4.2 Publication bias\nWhat proportion of studies that are conducted are actually published?\nGiven what we know about publication bias, perhaps we should instead ask: what proportion of studies with significant results are published? And what proportion with non-significant results are published?\nIt is very hard to know how to interpret and synthesize the published literature without knowing this, because we don’t know what is hidden from us.\nSeveral estimates of the prevalence of significant vs non-significant results in the literature exist.\n\nSterling (1959) found that 97% of psychology articles reported support for their hypothesis.\nSterling et al. (1989) later found that this result was nearly unchanged 30 years later (95%).\nAnother 20 years later, Fanelli (2010) found it was around 90% (albeit using different journals).\n\nHowever, all of these estimate estimate the opposite conditional probability: the probability of significance given being published: P(significant | published).\nWe actually need to know the opposite, the probability of being published given significance: P(published | significant), and the probability of being published given non-significance: P(published | nonsignificant).\nWorryingly, there is very little research on this. I only know of two studies that provide estimates of this (Franco et al. (2014; 2016):\n\nP(published | significant) = 57/93 = 0.61\nP(published | nonsignificant) = 11/49 = 0.22\n\nHowever, registered databases of approved studies are not typical in psychology, so these values are likely to be representative of psychology as a whole.\nWe can also look to other fields such as medical trials. Both the EU Clinical Trials Register (EUCTR) and the US Food and Drug Administration’s (FDA) ClinicalTrials.gov registries make it a legal requirement to publish clinical trials within 12 months of their completion. So, perhaps at least in some areas that really matter, and where there is a legal requirement to do so, null results don’t sit unpublished? Unfortunately:\n\nGoldacre et al. (2018) found that only 50% of 7274 EU trials were published within that time frame.\nDeVito, Bacon, & Goldacre (2020) found that only 41% of 4209 US trials were published within that time frame and 64% were published ever.\n\nMore extreme values for these conditional probabilities might therefore be more realistic for psychology. In my anecdotal experience, they are more like: P(published | significant) = 0.70 and P(published | nonsignificant) = 0.05.\nLet’s simulate the impact of this on rate of bias for a given literature. In this simulation, each iteration is a given study in the literature, so the number of iterations (25) is much smaller than a typical simulation.\n\n\nCode\n# remove all objects from environment ----\n#rm(list = ls())\n\n\n# dependencies ----\n# repeated here for the sake of completeness \n\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(forcats)\nlibrary(purrr) \nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(janitor)\nlibrary(metafor)\n\n\n# set the seed ----\n# for the pseudo random number generator to make results reproducible\nset.seed(46)\n\n\n# define data generating function ----\ngenerate_data &lt;- function(n_minimum,\n                          n_max,\n                          mean_control,\n                          mean_intervention,\n                          sd_control,\n                          sd_intervention) {\n  require(tibble)\n  require(dplyr)\n  require(forcats)\n  \n  n_per_condition &lt;- runif(n = 1, min = n_minimum, max = n_max)\n  \n  data_control &lt;- \n    tibble(condition = \"control\",\n           score = rnorm(n = n_per_condition, mean = mean_control, sd = sd_control))\n  \n  data_intervention &lt;- \n    tibble(condition = \"intervention\",\n           score = rnorm(n = n_per_condition, mean = mean_intervention, sd = sd_intervention))\n  \n  data &lt;- bind_rows(data_control,\n                    data_intervention) |&gt;\n    # control's factor levels must be ordered so that intervention is the first level and control is the second\n    # this ensures that positive cohen's d values refer to intervention &gt; control and not the other way around.\n    mutate(condition = fct_relevel(condition, \"intervention\", \"control\"))\n  \n  return(data)\n}\n\n\n# define data analysis function ----\nanalyse_data &lt;- function(data, probability_sig_published, probability_nonsig_published) {\n  require(effsize)\n  require(tibble)\n  \n  res_n &lt;- data |&gt;\n    count()\n  \n  res_t_test &lt;- t.test(formula = score ~ condition, \n                       data = data,\n                       var.equal = FALSE,\n                       alternative = \"two.sided\")\n  \n  res_cohens_d &lt;- effsize::cohen.d(formula = score ~ condition,  # new addition: also fit cohen's d\n                                   within = FALSE,\n                                   data = data)\n  \n  res &lt;- tibble(total_n = res_n$n,\n                p = res_t_test$p.value, \n                cohens_d_estimate = res_cohens_d$estimate,  # new addition: save cohen's d and its 95% CIs to the results tibble\n                cohens_d_ci_lower = res_cohens_d$conf.int[\"lower\"],\n                cohens_d_ci_upper = res_cohens_d$conf.int[\"upper\"]) |&gt;\n    mutate(cohens_d_se = (cohens_d_ci_upper - cohens_d_ci_lower)/(1.96*2),\n           cohens_d_variance = cohens_d_se^2) |&gt; # variance of effect size = its standard error squared\n    mutate(\n      # define result as (non)significant\n      significant = p &lt; .05,\n      # generate a random luck probability between 0 and 1\n      luck = runif(n = 1, min = 0, max = 1),\n      # decide if the result is published or not based on whether:\n      # (a) the result was significant and the luck variable is higher than the probability of significant results being published, or\n      # (b) the result was nonsignificant and the luck variable is higher than the probability of nonsignificant results being published\n      published = ifelse((significant & luck &gt;= (1 - probability_sig_published)) |\n                           (!significant & luck &gt;= (1 - probability_nonsig_published)), TRUE, FALSE)\n    )\n  \n  return(res)\n}\n\n\n# define experiment parameters ----\nexperiment_parameters_grid &lt;- expand_grid(\n  n_minimum = 10,\n  n_maximum = 100,\n  mean_control = 0,\n  mean_intervention = 0.25,  \n  sd_control = 1,\n  sd_intervention = 1,\n  probability_sig_published = 0.70, # 0.61 from Franco et al 2014, 2016\n  probability_nonsig_published = 0.05, # 0.22 from Franco et al 2014, 2016\n  iteration = 1:25 # here iterations are studies, so the number is small relative to a normal simulation\n)\n\n\n# run simulation ----\nsimulation &lt;- \n  # using the experiment parameters\n  experiment_parameters_grid |&gt;\n  \n  # generate data using the data generating function and the parameters relevant to data generation\n  mutate(generated_data = pmap(list(n_minimum,\n                                    n_maximum,\n                                    mean_control,\n                                    mean_intervention,\n                                    sd_control,\n                                    sd_intervention),\n                               generate_data)) |&gt;\n  \n  # apply the analysis function to the generated data using the parameters relevant to analysis\n  mutate(analysis_results = pmap(list(generated_data,\n                                      probability_sig_published,\n                                      probability_nonsig_published),\n                                 analyse_data))\n\n\n# summarise simulation results over the iterations ----\nsimulation_unnested &lt;- simulation |&gt;\n  unnest(analysis_results)\n\n\n\n\nCode\n# meta analysis and forest plot\nfit_all &lt;- \n  rma(yi     = cohens_d_estimate, \n      vi     = cohens_d_variance, \n      data   = simulation_unnested,\n      method = \"REML\")\n\nforest(fit_all, header = c(\"All studies conducted (unknowable)\", \"SMD [95% CI]\"), xlab = \"Standardized Mean Difference\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nfit_published &lt;- \n  rma(yi     = cohens_d_estimate, \n      vi     = cohens_d_variance, \n      data   = simulation_unnested |&gt; filter(published == TRUE),\n      method = \"REML\")\n\nforest(fit_published, header = c(\"Published studies\", \"SMD [95% CI]\"), xlab = \"Standardized Mean Difference\")\n\n\n\n\n\n\n\n\n\nNote that the non-overlap between the confidence intervals between the two meta-analyses imply that the published literature has a significantly higher effect size than the actual studies run.\nRemember, however, that (a) our values for the prior probability that (non)significant results are published are not based on any good evidence, and (b) that this only simulates a single literature. These results try to illustrate a point, they don’t comprehensively simulate the potential impact of publication bias across a range of conditions.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/12_meta_analysis_and_publication_bias.html#power-of-eggers-test-of-publication-bias",
    "href": "chapters/12_meta_analysis_and_publication_bias.html#power-of-eggers-test-of-publication-bias",
    "title": "12  Meta-analysis and publication bias",
    "section": "12.5 Power of Egger’s Test of publication bias",
    "text": "12.5 Power of Egger’s Test of publication bias\nHow would we simulate at two levels, studies and also meta-analyses of them?\nHow could we assess the power of a method like Egger’s test for funnel plot asymmetry, as implemented by regtest(res, model = \"lm\")?\n[needs work]\n\n\nCode\n# # remove all objects from environment ----\n# #rm(list = ls())\n# \n# \n# # run furrr:::future_map in parallel\n# plan(multisession)\n# \n# # dependencies ----\n# # repeated here for the sake of completeness \n# \n# library(tidyr)\n# library(dplyr)\n# library(tibble)\n# library(forcats)\n# library(purrr) \n# library(ggplot2)\n# library(knitr)\n# library(kableExtra)\n# library(janitor)\n# library(metafor)\n# \n# # define data generating function ----\n# generate_data &lt;- function(n_min,\n#                           n_max,\n#                           mean_control,\n#                           mean_intervention,\n#                           sd_control,\n#                           sd_intervention) {\n#   \n#   \n#   n_per_condition &lt;- runif(n = 1, min = n_min, max = n_max)\n#   \n#   data_control &lt;- \n#     tibble(condition = \"control\",\n#            score = rnorm(n = n_per_condition, mean = mean_control, sd = sd_control))\n#   \n#   data_intervention &lt;- \n#     tibble(condition = \"intervention\",\n#            score = rnorm(n = n_per_condition, mean = mean_intervention, sd = sd_intervention))\n#   \n#   data &lt;- bind_rows(data_control,\n#                     data_intervention) |&gt;\n#     # control's factor levels must be ordered so that intervention is the first level and control is the second\n#     # this ensures that positive cohen's d values refer to intervention &gt; control and not the other way around.\n#     mutate(condition = fct_relevel(condition, \"intervention\", \"control\"))\n#   \n#   return(data)\n# }\n# \n# \n# # define data analysis function ----\n# analyse_study &lt;- function(data, probability_sig_published, probability_nonsig_published) {\n#   \n#   res_n &lt;- data |&gt;\n#     count()\n#   \n#   res_t_test &lt;- t.test(formula = score ~ condition, \n#                        data = data,\n#                        var.equal = FALSE,\n#                        alternative = \"two.sided\")\n#   \n#   res_cohens_d &lt;- effsize::cohen.d(formula = score ~ condition,  # new addition: also fit cohen's d\n#                                    within = FALSE,\n#                                    data = data)\n#   \n#   res &lt;- tibble(total_n = res_n$n,\n#                 p = res_t_test$p.value, \n#                 cohens_d_estimate = res_cohens_d$estimate,  # new addition: save cohen's d and its 95% CIs to the results tibble\n#                 cohens_d_ci_lower = res_cohens_d$conf.int[\"lower\"],\n#                 cohens_d_ci_upper = res_cohens_d$conf.int[\"upper\"]) |&gt;\n#     mutate(cohens_d_se = (cohens_d_ci_upper - cohens_d_ci_lower)/(1.96*2),\n#            cohens_d_variance = cohens_d_se^2) |&gt; # variance of effect size = its standard error squared\n#     mutate(\n#       # define result as (non)significant\n#       significant = p &lt; .05,\n#       # generate a random luck probability between 0 and 1\n#       luck = runif(n = 1, min = 0, max = 1),\n#       # decide if the result is published or not based on whether:\n#       # (a) the result was significant and the luck variable is higher than the probability of significant results being published, or\n#       # (b) the result was nonsignificant and the luck variable is higher than the probability of nonsignificant results being published\n#       published = ifelse((significant & luck &gt;= (1 - probability_sig_published)) |\n#                            (!significant & luck &gt;= (1 - probability_nonsig_published)), TRUE, FALSE)\n#     )\n#   \n#   return(res)\n# }\n# \n# analyse_meta &lt;- function(data){\n#   \n#   data_for_meta &lt;- data |&gt; \n#     filter(published == TRUE)\n#   \n#   if(nrow(data_for_meta) &gt; 0){\n#     fit_meta &lt;- \n#       rma(yi     = cohens_d_estimate, \n#           vi     = cohens_d_variance, \n#           data   = data_for_meta,\n#           method = \"REML\")\n#   } else {\n#     fit_meta &lt;- NULL\n#   }\n#   \n#   return(fit_meta)\n# }\n# \n# results_meta &lt;- function(fit_meta){\n#   results &lt;- tibble(meta_es = as.numeric(fit_meta$b[,1]))\n#   return(results)\n# }\n# \n# analyse_meta_publication_bias &lt;- function(fit_meta) {\n#   # sometimes 0 studies are published, so we need ways to handle this absence of data which would cause an error\n#   safe_regtest &lt;- possibly(\n#     function(fit) {\n#       fit_egger &lt;- regtest(fit, model = \"lm\")\n#       tibble(\n#         egger_p = ifelse(!is.nan(fit_egger$pval), fit_egger$pval, NA_real_),\n#         egger_corrected_es = ifelse(!is.nan(fit_egger$est), fit_egger$est, NA_real_)\n#       )\n#     },\n#     otherwise = tibble(egger_p = NA_real_, egger_corrected_es = NA_real_)\n#   )\n#   \n#   if (is.null(fit_meta)) {\n#     return(tibble(egger_p = NA_real_, egger_corrected_es = NA_real_))\n#   }\n#   \n#   safe_regtest(fit_meta)\n# }\n# \n# \n# # define experiment parameters ----\n# experiment_parameters_grid &lt;- bind_rows(\n#   expand_grid(\n#     n_min = 10,\n#     n_max = 100,\n#     mean_control = 0,\n#     mean_intervention = c(0, 0.2, 0.5, 0.8),  \n#     sd_control = 1,\n#     sd_intervention = 1,\n#     probability_sig_published = 0.70, # 0.61 from Franco et al 2014, 2016\n#     probability_nonsig_published = 0.05, # 0.22 from Franco et al 2014, 2016\n#     iteration_meta = 1:1000, # here iterations are meta analyses\n#     k_studies = 10,  # for reference later, number must match max iterations below\n#     iteration_study = 1:10 # here iterations are studies\n#   ),\n#   expand_grid(\n#     n_min = 10,\n#     n_max = 100,\n#     mean_control = 0,\n#     mean_intervention = c(0, 0.2, 0.5, 0.8),  \n#     sd_control = 1,\n#     sd_intervention = 1,\n#     probability_sig_published = 0.70, # 0.61 from Franco et al 2014, 2016\n#     probability_nonsig_published = 0.05, # 0.22 from Franco et al 2014, 2016\n#     iteration_meta = 1:1000, # here iterations are meta analyses\n#     k_studies = 20,  # for reference later, number must match max iterations below\n#     iteration_study = 1:20 # here iterations are studies\n#   ),\n#   expand_grid(\n#     n_min = 10,\n#     n_max = 100,\n#     mean_control = 0,\n#     mean_intervention = c(0, 0.2, 0.5, 0.8),  \n#     sd_control = 1,\n#     sd_intervention = 1,\n#     probability_sig_published = 0.70, # 0.61 from Franco et al 2014, 2016\n#     probability_nonsig_published = 0.05, # 0.22 from Franco et al 2014, 2016\n#     iteration_meta = 1:1000, # here iterations are meta analyses\n#     k_studies = 30,  # for reference later, number must match max iterations below\n#     iteration_study = 1:30 # here iterations are studies\n#   )\n# )\n# \n# if(file.exists(\"simulation_summary.rds\")) {\n#   simulation_summary &lt;- read_rds(\"../data/simulation_summary_12.rds\")\n# } else {\n#   \n#   # set the seed ----\n#   # for the pseudo random number generator to make results reproducible\n#   set.seed(46)\n#   \n#   # run study level simulation ----\n#   simulation_studies &lt;- \n#     # using the experiment parameters\n#     experiment_parameters_grid |&gt;\n#     \n#     # generate data using the data generating function and the parameters relevant to data generation\n#     mutate(generated_data = future_pmap(list(n_min = n_min,\n#                                              n_max = n_max,\n#                                              mean_control = mean_control,\n#                                              mean_intervention = mean_intervention,\n#                                              sd_control = sd_control,\n#                                              sd_intervention = sd_intervention),\n#                                         generate_data,\n#                                         .progress = TRUE,\n#                                         .options = furrr_options(seed = TRUE))) |&gt;\n#     \n#     # apply the analysis function to the generated data using the parameters relevant to analysis\n#     mutate(analysis_results_study = future_pmap(list(data = generated_data,\n#                                                      probability_sig_published = probability_sig_published,\n#                                                      probability_nonsig_published = probability_nonsig_published),\n#                                                 analyse_study,\n#                                                 .progress = TRUE,\n#                                                 .options = furrr_options(seed = TRUE)))\n#   \n#   \n#   # run meta-analysis simulation ----\n#   simulation_meta &lt;- simulation_studies |&gt;\n#     unnest(analysis_results_study) |&gt;\n#     select(-generated_data) |&gt;\n#     group_by(n_min, n_max, mean_control, mean_intervention, sd_control, sd_intervention, probability_sig_published, probability_nonsig_published, iteration_meta, k_studies) |&gt;\n#     nest(.key = \"meta_data\") |&gt;\n#     ungroup() |&gt;\n#     # at this point we have the equivalent of the usual steps up to and including generate data. next analyze that data\n#     mutate(analysis_meta = future_pmap(list(data = meta_data),\n#                                        analyse_meta,\n#                                        .progress = TRUE,\n#                                        .options = furrr_options(seed = TRUE))) |&gt;\n#     # extract meta ES\n#     mutate(meta_es = future_pmap(list(fit_meta = analysis_meta),\n#                                  results_meta,\n#                                  .progress = TRUE,\n#                                  .options = furrr_options(seed = TRUE)),\n#            .progress = TRUE,\n#            seed = TRUE) |&gt;\n#     unnest(meta_es) |&gt;\n#     # run eggers test\n#     mutate(analysis_meta_publication_bias = future_pmap(list(fit_meta = analysis_meta),\n#                                                         analyse_meta_publication_bias,\n#                                                         .progress = TRUE,\n#                                                         .options = furrr_options(seed = TRUE)))\n#   \n#   # summarize across meta iterations\n#   simulation_summary &lt;- simulation_meta |&gt;\n#     unnest(analysis_meta_publication_bias) |&gt;\n#     rename(population_es = mean_intervention) |&gt;\n#     group_by(population_es, probability_sig_published, probability_nonsig_published, k_studies) |&gt;\n#     summarize(mean_meta_es = mean(meta_es), \n#               egger_proportion_significant = mean(egger_p &lt; .05, na.rm = TRUE),\n#               mean_egger_corrected_es = mean(egger_corrected_es, na.rm = TRUE))\n#   \n#   write_rds(simulation_summary, \"../data/simulation_summary_12.rds\")\n# }\n# \n# # print table\n# simulation_summary |&gt;\n#   mutate_if(is.numeric, round_half_up, digits = 2) |&gt;\n#   kable() |&gt;\n#   kable_classic(full_width = FALSE)\n\n# Error in `mutate()`:\n# ℹ In argument: `analysis_meta = future_pmap(...)`.\n# Caused by error:\n# ℹ In index: 384.\n# Caused by error in `rma()`:\n# ! Fisher scoring algorithm did not converge. See 'help(rma)' for possible remedies.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/13_causality.html",
    "href": "chapters/13_causality.html",
    "title": "13  Regression assumes causality",
    "section": "",
    "text": "13.1 Dependencies\nCode\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(purrr) \nlibrary(parameters)\nlibrary(lavaan)\nlibrary(semPlot)\nlibrary(knitr)\nlibrary(kableExtra)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regression assumes causality</span>"
    ]
  },
  {
    "objectID": "chapters/13_causality.html#functions",
    "href": "chapters/13_causality.html#functions",
    "title": "13  Regression assumes causality",
    "section": "13.2 Functions",
    "text": "13.2 Functions\n\n\nCode\ngenerate_data &lt;- function(n, population_model) {\n  \n  data &lt;- lavaan::simulateData(model = population_model, sample.nobs = n) \n  \n  return(data)\n}\n\nanalyse &lt;- function(data, model) {\n  \n  # specify and fit model\n  fit &lt;- sem(model = model, data = data)\n  #fit &lt;- lm(formula = mode, data = data)\n  \n  # extract regression beta estimates \n  results &lt;- parameters::model_parameters(fit, standardize = FALSE) |&gt;\n    filter(To == \"Y\" & From == \"X\") |&gt;  # this corresponds to the Y ~ X effect\n    select(beta = Coefficient,\n           ci_lower = CI_low,\n           ci_upper = CI_high,\n           p) \n  \n  return(results)\n}\n\n plots_causal &lt;- function(model){\n  generate_data(n = 300, population_model = model) %&gt;%\n    sem(model = model, data = .) |&gt;\n    semPaths(whatLabels = \"diagram\", \n             layout = layout_matrix, \n             residuals = FALSE,\n             edge.label.cex = 1.2, \n             sizeMan = 10)\n}",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regression assumes causality</span>"
    ]
  },
  {
    "objectID": "chapters/13_causality.html#does-regression-assume-causality",
    "href": "chapters/13_causality.html#does-regression-assume-causality",
    "title": "13  Regression assumes causality",
    "section": "13.3 Does regression assume causality?",
    "text": "13.3 Does regression assume causality?\n\n13.3.1 Plots\nSimple regression: X causes Y\n\n\nCode\n# simple regression\nlayout_matrix &lt;- matrix(c( 1,  0,\n                           -1,  0), \n                        ncol = 2, \n                        byrow = TRUE)\n\nplots_causal(\"Y ~ 0.5*X\")\n\n\n\n\n\n\n\n\n\nSimple regression: Y causes X\n\n\nCode\n# simple regression\nlayout_matrix &lt;- matrix(c(-1,  0,\n                           1,  0), \n                        ncol = 2, \n                        byrow = TRUE)\n\nplots_causal(\"X ~ 0.5*Y\")\n\n\n\n\n\n\n\n\n\n\n\n13.3.2 Run simulation\n\n\nCode\nexperiment_parameters_grid &lt;- expand_grid(\n  n = 200,\n  population_model = c(\"Y ~ 0.5*X\",\n                       \"X ~ 0.5*Y\"),\n  analyse_model = \"Y ~ X\",\n  iteration = 1:1000\n) \n\nset.seed(42)\n\nsimulation &lt;- \n  # using the experiment parameters...\n  experiment_parameters_grid |&gt;\n  \n  # ...generate data... \n  mutate(generated_data = pmap(list(n = n,\n                                    population_model = population_model),\n                               generate_data)) |&gt;\n  # ...analyze data \n  mutate(results = pmap(list(data = generated_data,\n                             model = analyse_model),\n                        analyse))\n\n\n\n\n13.3.3 Summarize results\n\n\nCode\nsimulation_summary &lt;- simulation |&gt;\n  unnest(results) |&gt;\n  group_by(n,\n           population_model,\n           analyse_model) |&gt;\n  summarize(mean_beta = mean(beta),\n            proportion_signficiant = mean(p &lt; .05))\n\nsimulation_summary |&gt;\n  mutate_if(is.numeric, janitor::round_half_up, digits = 2) |&gt;\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\nn\npopulation_model\nanalyse_model\nmean_beta\nproportion_signficiant\n\n\n\n\n200\nX ~ 0.5*Y\nY ~ X\n0.4\n1\n\n\n200\nY ~ 0.5*X\nY ~ X\n0.5\n1",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regression assumes causality</span>"
    ]
  },
  {
    "objectID": "chapters/13_causality.html#collider",
    "href": "chapters/13_causality.html#collider",
    "title": "13  Regression assumes causality",
    "section": "13.4 Collider",
    "text": "13.4 Collider\n\n13.4.1 Plots\nCovariate is a confounder:\n\n\nCode\nlayout_matrix &lt;- matrix(c( 1,  0,\n                           -1,  0,\n                           0,  1), \n                        ncol = 2, \n                        byrow = TRUE)\n\nplots_causal(\"Y ~ 0.0*X + 0.5*C; X ~ 0.5*C\")\n\n\n\n\n\n\n\n\n\nConfounder is a collider:\n\n\nCode\nlayout_matrix &lt;- matrix(c( 0,  1,\n                           1,  0,\n                           -1,  0), \n                        ncol = 2, \n                        byrow = TRUE)\n\nplots_causal(\"C ~ 0.5*X + 0.5*Y; Y ~ 0.0*X\")\n\n\n\n\n\n\n\n\n\n\n\n13.4.2 Run simulation\n\n\nCode\nexperiment_parameters_grid &lt;- expand_grid(\n  n = 200,\n  population_model = c(\"Y ~ 0.5*X + 0.0*C\",\n                       \"C ~ 0.5*X + 0.5*Y; Y ~~ 0.0*X\"),\n  analyse_model = \"Y ~ X + C\",\n  iteration = 1:1000\n)\n\nset.seed(42)\n\nsimulation &lt;- \n  # using the experiment parameters...\n  experiment_parameters_grid |&gt;\n  \n  # ...generate data...\n  mutate(generated_data = pmap(list(n = n,\n                                    population_model = population_model),\n                               generate_data)) |&gt;\n  # ...analyze data\n  mutate(results = pmap(list(data = generated_data,\n                             model = analyse_model),\n                        analyse))\n\n\n\n\n13.4.3 Summarize results\n\n\nCode\nsimulation_summary &lt;- simulation |&gt;\n  unnest(results) |&gt;\n  group_by(n,\n           population_model,\n           analyse_model) |&gt;\n  summarize(mean_beta = mean(beta),\n            proportion_signficiant = mean(p &lt; .05))\n\nsimulation_summary |&gt;\n  mutate_if(is.numeric, janitor::round_half_up, digits = 2) |&gt;\n  kable() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\n\nn\npopulation_model\nanalyse_model\nmean_beta\nproportion_signficiant\n\n\n\n\n200\nC ~ 0.5*X + 0.5*Y; Y ~~ 0.0*X\nY ~ X + C\n-0.2\n0.82\n\n\n200\nY ~ 0.5*X + 0.0*C\nY ~ X + C\n0.5\n1.00",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Regression assumes causality</span>"
    ]
  },
  {
    "objectID": "chapters/license.html",
    "href": "chapters/license.html",
    "title": "14  License and citation",
    "section": "",
    "text": "© Ian Hussey (2025)\nText and figures are licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) license.\nCode is licensed under the MIT License.\nYou are free to copy, share, adapt, and reuse the contents of this book — text, figures, and code — for any purpose, including commercial use, provided you cite it.\nCitation:\nHussey, I. (2025) Improving your statistical inferences using Monte Carlo simulation studies in tidyverse. github.com/ianhussey/improving-your-statistical-inferences-through-monte-carlo-simulation-studies",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>License and citation</span>"
    ]
  }
]