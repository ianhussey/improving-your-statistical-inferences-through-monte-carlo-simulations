# Writing functions

## Overview

Two of the key steps in a simulation study (generate data and analyze data) require us to know how to write functions. This R Markdown lesson practices this.

## To add

function extraction resources

https://www.appsilon.com/post/r-studio-shortcuts-and-tips-part-2

## What is a function?

A function is a piece of code that receives an input, does something to it, and returns an output. That's it! Of course, that's also a very vague definition. But let's look at the most simple example:

```{r}

output <- 1 + 1

print(output)

```

The `+` operator is a function. It takes some input (one number either side of it), does something to this input (adds the two numbers together), and returns an output (the value of the two numbers added together). We use the `<-` operator to assign this to an object called `output`. 

When we write code in R, however, we don't write functions like this. Instead, we write them in the form: `function_name(argument1, argument2, ...argumentN)`. 

The equivalent to `+` in this style is the `sum()` function. Let's see what happens when we run `sum(1, 1)`:

```{r}

output <- sum(1, 1)

print(output)

```

We get the same result! Sometimes, we might want to use many functions together. For example:

```{r}

1 + 1 + 2 + 3

```

In this case, we used three functions: `+` three times. In R, we could actually still write this more easily using `sum()`:

```{r}

sum(c(1, 1, 2, 3))

```

But let's imagine a situation where we want to create a first output using one function, then run a second (or third, or fourth) function on that output, like:

```{r}

output1 <- sqrt(c(1, 4, 9, 16, 25, 30)) # get the square root of these numbers

output2 <- mean(output1) # get the mean of those square roots

output3 <- round(output2, digits = 2) # round this to two decimal placess

print(output3)

```

Doing this is fine, but we are creating a lot of objects that we don't care about (`output1`, `output2`) in order to finally get the one we eventually care about (`output3`).

Ideally, we would minimise the number of unnecessary objects we create. We could do this by nesting the functions together, like this:

```{r}

output <- round(
  mean(
    sqrt(
      c(1, 4, 9, 16, 25, 30)
    )
  ),
  digits = 2
)

print(output)

```

Another way to do this is using the pipe operator, `|>`. You can think of this operator as a way of saying "...and then do this". For the example above, this would look like:

```{r}

output <- sqrt(c(1, 4, 9, 16, 25, 30)) |> 
  mean() |>
  round(digits = 2)

print(output)

```

This is also a lot easier to read!

## Writing functions 

Previously, you saw a few functions: `sum()`, `round()`, `mean()`, and `sqrt()`. These are all in-built functions in R, meaning that R "knows" them by default. One other way that R can "know" a function is if we write it ourselves! 

The way to write a function in R is simple. It takes the following format:

`function_name <- function(argument1, argument2, ... argumentN) { [function goes in here] }`

Let's look at a simple example. We'll create a new function called `sum2()`, which is identical to `sum()`, except it also rounds the output to two decimal places at the end. We'll also compare the outputs of `sum()` and `sum2()`:

```{r}

sum2 <- function(first_number, second_number) {
  
  round(first_number + second_number, digits = 2)
  
}

sum(2.22222, 3.33333)
sum2(2.22222, 3.33333)

```

Creating functions is very, very important for Monte Carlo simulations. Recall our five steps:

1. Construct an experiment
2. Generate data
3. Analyze the data
4. Do things many times
5. Summarise the results

We will use functions in all 5 of these steps. And we will basically always write new functions for steps 2 and 3 (and often, step 5).

## Why functions are useful: the false positive rate of AI detection tools

Imagine a test has has a false positive rate of 5%, like the standard alpha value for a p-value. 

If you apply this test many times to independent cases without applying familywise error corrections, what is the resulting false positive rate?

Let's put this in meaningful terms. Many professors and universities now run essays and assignments through AI detection tools. But students submit many essays and assignments throughout their degree. What is the probability that, assuming you never violate the AI policy, one of your assignments is falsely flagged as using AI in a way that wasn't allowed? 

### Math

The familywise error rate ($\alpha_{\text{total}}$) for independent tests is the probability of observing at least one false positive ($P(V \geq 1)$), which can be reexpressed as the probability of not observing true positives ($1 - P(V = 0)$). This is the product of the individual probabilities of false positives ($1 - \prod_{i=1}^{n} (1 - \alpha_i)$), which simplifies to:

$$
\begin{align}
\alpha_{\text{total}} &= P(V \geq 1) \\
&= 1 - P(V = 0) \\
&= 1 - \prod_{i=1}^{n} (1 - \alpha_i) \\
&= 1 - (1 - \alpha)^n
\end{align}
$$

Horrible, disgusting math which most of you want to avoid. But the code to do it is quite simple.

### Hard coded

Assuming individual false positives of 5% and 10 total tests.

```{r}

1 - (1 - 0.05)^10

```

### Using variables

What about other values of alpha and n? We could make them variables at the top of the chunk that are easier to change.

```{r}

# variables
alpha <- 0.05
n <- 10

# code
1 - (1 - alpha)^n

```

### Written as a function

Defining our own custom function involves putting the variables as arguments inside the `function()` call, and putting the code inside its `{}`.

We can then call the function using its name.

```{r}

# define function
calc_aggregate_fpr <- function(n, alpha = 0.05) {
  1 - (1 - alpha)^n
}

# usage
calc_aggregate_fpr(n = 10)

```

This also lets us call it an arbitrary number of times, eg for different values of n

```{r}

calc_aggregate_fpr(n =  5)
calc_aggregate_fpr(n = 10)
calc_aggregate_fpr(n = 15)
calc_aggregate_fpr(n = 20)
calc_aggregate_fpr(n = 25)

```

### Use one of {purrr}'s `map()` functions to apply the custom function to many inputs

This previews some skills we'll learn in a future chapter: calling a function multiple times within a tidy workflow by mapping it on to a set of input columns.

```{r}

library(tibble)
library(dplyr)
library(purrr)

experiment <- 
  # define many values of n as different rows in a data frame or tibble
  tibble(n = 1:20) 

# view 
experiment

results <- experiment |>  
  # use mutate to create a new column, fpr, by calling the custom function for each row, using n as the input
  # use a _dbl map function as the output is a numeric (double precision floating point) variable
  mutate(fpr = map_dbl(.x = n, 
                       .f = calc_aggregate_fpr))

results

```

Because we've done this in a tidy format in a tibble, we can easily plot the results too.

```{r}

library(ggplot2)

ggplot(results, aes(n, fpr)) +
  geom_line() +
  geom_point() +
  theme_linedraw()

```

What if we want to vary not only n but also alpha, and see all combinations of them? This previews some skills we'll learn in a later chapter: defining crossed experiment conditions using `expand_grid()`.

```{r}

library(tidyr)

# define many values of n and alpha, and then have them 'crossed' to create all permutations of them, using expand_grid()
experiment <- 
  expand_grid(n = 1:20,
              alpha = c(0.01, 0.05, 0.10)) 

# view the combinations used in the experiment
experiment

results <- experiment |>  
  # use mutate to create a new column, fpr, by calling the custom function for each row
  # use a map2_ function as there are two inputs, n and alpha
  # these are passed by location, .x to the first argument, .y to the second
  mutate(fpr = map2_dbl(.x = n, 
                        .y = alpha,
                        .f = calc_aggregate_fpr))

results

```

```{r}

results |>
  mutate(alpha = as.factor(alpha)) |>
  ggplot(aes(n, fpr, color = alpha, group = alpha)) +
  geom_line() +
  geom_point() +
  theme_linedraw() 

```



## Primer on functions

Most code we use are functions, e.g., `mean()`, `setwd()` and `library()`.

These functions were written by others, but we can write our own.

"It's functions all the down": you will use existing functions to write new ones. For example:

```{r}

values <- c(4, 2, 6, 2, NA, 4, 3, 1, NA, 7, 5)

mean(values) # returns NA 
mean(values, na.rm = TRUE) # returns the mean after dropping NA

# tired of writing 'na.rm = TRUE' repeatedly? write your own function to do it automatically
mean_na_rm <- function(x){
  mean(x, na.rm = TRUE)
}

mean_na_rm(values) # returns the mean after dropping NA

```

What if we usually want to `round()` to two decimal places, and we're tired of writing `digits = 2` every time?

```{r}

mean_of_values <- mean_na_rm(values)

round(mean_of_values, digits = 2)

# write a function to always round to two decimal places
round_2 <- function(x){
  round(x, digits = 2)
}

round_2(mean_of_values)

```

### Mini lesson: round() probably doesn't do what you think it does

round() uses "banker's rounding" rather than the round-half-up method we're used to

```{r}

round(0.5)
round(1.5)
round(2.5)
round(3.5)
round(4.5)
round(5.5)

```

```{r}

library(roundwork)

roundwork::round_up(0.5)
roundwork::round_up(1.5)
roundwork::round_up(2.5)
roundwork::round_up(3.5)
roundwork::round_up(4.5)
roundwork::round_up(5.5)

```

### General structure of a function

Functions (usually) have 'inputs', they have code that they run ('do stuff'), and they (almost always) return 'outputs'. The often specify their requirements and include checks that their inputs are correctly formatted.

Note that this is pseudo-code only: chunk is set not to run (`eval=FALSE`).

```{r eval=FALSE}

# define function
function_name <- function(argument_1, # first argument is often the data, if the function takes a data frame as an argument
                          argument_2 = "default", # arguments can have defaults
                          argument_3) {
  # required packages
  require(dplyr)
  
  # checks
  # well written functions contain checks. 
  # e.g., if the function assumes that argument_1 is a data frame, check that this is the case.
  # note that it is more useful to write the function first and add checks later.
  if(!is.data.frame(argument_1)){
    stop("argument_1 must be a data frame")
  }
  
  # code that does things
  object_to_be_returned <- input_data_frame |>
    # do things
    mutate(value = value + 1)
  
  # object to be returned
  return(object_to_be_returned)
}

```

### Example function: *t*-test *p*-value

```{r}

# data to be analyzed using the analysis function
data_simulated_intervention <- 
  tibble(condition = "intervention", 
         score = rnorm(n = 50, mean = 0, sd = 1))

data_simulated_control <- 
  tibble(condition = "control", 
         score = rnorm(n = 50, mean = 0, sd = 1))

data_simulated <- 
  bind_rows(data_simulated_intervention,
            data_simulated_control)

```

```{r}

# define function
t_test_p_value <- function(data) {

  res <- t.test(formula = score ~ condition, 
                data = data)
  
  return(res$p.value)
}

# call function
t_test_p_value(data_simulated)

```

How would I build this from scratch? What's the first thing I would type?

```{r}



```

### General things to remember when writing functions

-   If you can't immediately write the code, write pseudo-code first!
-   Build the 'do stuff' part outside of a function first!
-   Wrap the 'do stuff' with input and output after you have 'do stuff' working. Why: so you don't have to fight variable scoping.
-   The function must be present in your environment to be usable, and must be called to be used
-   Check that your function actually works as you expect, not just that it runs. Give it lots of different input values that should and should not work, and check you get the correct outputs.
-   Don't try to abstract more than you need. One function should do one thing. Elaborate the function only as needed.

## In class and at home exercise: Practice writing functions

Write functions below that can be applied to the following data sets, i.e., use these data sets to guide how you write the functions and test that they work.

Try and use AI as little as possible.

If your existing knowledge of data processing/wrangling with {tidyverse}/{dplyr}/{tidyr} isn't good, start with

```{r}

library(forcats)
library(faux)

set.seed(42)

# data for t tests
data_intervention <- 
  tibble(condition = "intervention", 
         score = rnorm(n = 50, mean = 0, sd = 1))

data_control <- 
  tibble(condition = "control", 
         score = rnorm(n = 50, mean = 0, sd = 1))

data_combined_ttest <- 
  bind_rows(data_intervention,
            data_control) |>
  # control's factor levels must be ordered so that intervention is the first level and control is the second
  # this ensures that positive Cohen's d values refer to intervention > control and not the other way around.
  mutate(condition = fct_relevel(condition, "intervention", "control"))


# data for correlations
data_correlation <- rnorm_multi(n = 100, 
                                vars = 2, 
                                mu = 0, 
                                sd = 1, 
                                r = 0.5, 
                                varnames = c("X", "Y"))

```

### Calculate mean

-   Use `dplyr::summarize()`.
-   Use the data_intervention data set.
-   Return results in a data frame.

```{r}



```

### Calculate *SD*

-   Use `dplyr::summarize()`.
-   Use the data_intervention data set.
-   Return results in a data frame.

```{r}



```

### Calculate mean for each condition

-   Use `dplyr::summarize()` and `group_by()`.
-   Use the data_combined_ttest data set.
-   Return results in a data frame.

```{r}



```

### Calculate mean and *SD* for each condition

-   Use `dplyr::summarize()` and `group_by()`.
-   Use the data_combined_ttest data set.
-   Return results in a data frame.

```{r}



```

### Cohen's *d* and its 95% Confidence Intervals

-   Calculate Cohen's *d* using `effsize::cohen.d()` and extract the Cohen's *d* estimate.
-   Use the data_combined_ttest data set.
-   Return results in a data frame.

```{r}



```

### *t*-test's *p*-value, Cohen's *d* and its 95% Confidence Intervals

-   Calculate Cohen's *d* using `effsize::cohen.d()` and extract the estimate.
-   Also fit a Student's *t*-test and extract its *p* value.
-   Use the data_combined_ttest data set.
-   Return results (*d* and *p*) in a data frame.

```{r}



```

### Pearson's *r* from correlation test

-   Fit a correlation test using `cor.test()` and extract the correlation estimate.
-   Use the data_correlation data set.
-   Return results in a data frame.

```{r}



```

### *p*-value from correlation test

-   Fit a correlation test using `cor.test()` and extract the *p* value.
-   Use the data_correlation data set.
-   Return results in a data frame.

```{r}



```

### Pearson's *r* and its *p*-value from `cor.test()`

-   Fit a correlation test using `cor.test()` and extract the *p* value and correlation.
-   Use the data_correlation data set.
-   Return results in a data frame.

```{r}



```

### Generate data for a between groups design

Rather than writing a data analysis function, this time write a data generation function. In the previous chunks we've used this code to generate a single data set with intervention and control conditions and simulated normally distributed data. Rewrite this as a function so that we can generate such a data set with one line of code using the new function `generate_data()`. Unlike your previous functions, this one has no inputs, i.e., you can write `function() <-`.

```{r}

# # code to convert into a function
# data_simulated_intervention <- tibble(condition = "intervention", 
#                                       score = rnorm(n = 50, mean = 0, sd = 1))
# 
# data_simulated_control <- tibble(condition = "control", 
#                                  score = rnorm(n = 50, mean = 0, sd = 1))
# 
# data_simulated <- 
#   bind_rows(data_simulated_intervention,
#             data_simulated_control) 

```

## Further reading

Although we have practiced writing custom functions to extract statistical results / model parameters, it is worth knowing that the {easystats} family of packages includes [{parameters}](https://easystats.github.io/parameters/) package, which does a very good job of extracting model parameters from a very wide range of models including base R functions, {lavaan}, {psych}, and other packages. If you want to extract values from a model, consider using {parameters} to do a lot of the work for you when writing your function.

Separately, the [{report}](https://easystats.github.io/report/) package will fully report the results of many common analyses for you. e.g.:

```{r}

library(report)

data_simulated_intervention <- 
  tibble(condition = "intervention", 
         score = rnorm(n = 50, mean = 0, sd = 1))

data_simulated_control <- 
  tibble(condition = "control", 
         score = rnorm(n = 50, mean = 0, sd = 1))

data_simulated <- 
  bind_rows(data_simulated_intervention,
            data_simulated_control)

t.test(score ~ condition, data = data_simulated) |>
  report::report()

```

This lesson does not cover documenting your functions well, organizing them into an R package to make them easy to load and include help menus, or writing unit tests them. These are all very worth doing. Look into the {roxygen} package.
